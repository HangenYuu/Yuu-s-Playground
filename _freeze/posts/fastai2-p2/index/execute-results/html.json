{
  "hash": "871cb11ca96cb2810ee6dbea72b6e456",
  "result": {
    "markdown": "---\ntitle: Deep Learning from the ground up - From tensor to multi-layer perceptron (1)\nsubtitle: Lesson 9 - 14 of fast.ai course part 2\nauthor: Pham Nguyen Hung\ndraft: true\ndate: last-modified\ncategories:\n  - code\n  - From scratch\nformat:\n  html:\n    toc: true\n    code-fold: false\n---\n\nIn the [first post](https://hangenyuu.github.io/h-notes/posts/fastai2-p1/), I finished backpropagation and the simple maths behind it. Now let's talk about the design of PyTorch...\n\n> Actually I am not including it in my post. You can read about it [here](https://pytorch.org/docs/stable/community/design.html). To feel the need for PyTorch or Keras (and then TensorFlow 2.x), check out this [script](https://github.com/jsyoon0823/TimeGAN/blob/master/timegan.py). **Warning:** the author defined four models as *functions* and *updated them together* in TensorFlow 1.x.\n\n... which leads to our need to refactor the layers into *objects*, instead of *functions*. On defining them as class, we make them reusable and reduce the amount of codes to be written.\n\n# Setup\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Follow the previous post\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom torch import tensor,nn\nimport torch.nn.functional as F\nfrom fastcore.test import test_close\n# ---\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n# ---\npath_data = Path('data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\nbatch_size, feature = x_train.shape\nclasses = y_train.max() + 1\nnum_hidden = 50\n\nw1 = torch.randn(feature,num_hidden)\nb1 = torch.zeros(num_hidden)\nw2 = torch.randn(num_hidden,1)\nb2 = torch.zeros(1)\n\ndef lin(x, w, b): return x@w + b\n\ndef relu(x): return x.clamp_min(0.)\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    b.g = out.g.sum(0)\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = lin(inp, w1, b1)\n    l2 = relu(l1)\n    out = lin(l2, w2, b2)\n    diff = out[:,0]-targ\n    loss = diff.pow(2).mean()\n    \n    # backward pass:\n    out.g = 2.*diff[:,None] / inp.shape[0]\n    lin_grad(l2, out, w2, b2)\n    l1.g = (l1>0).float() * l2.g\n    lin_grad(inp, l1, w1, b1)\n\nforward_and_backward(x_train, y_train)\n\n# Save for testing against later\ndef get_grad(x): return x.g.clone()\nchks = w1,w2,b1,b2,x_train\ngrads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n```\n:::\n\n\n# Refactor model\n\n## First attempt\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n```\n:::\n\n\nHere, I learnt about the dunder methods. They define the intrinsic appearance of a class. `__init__()` defines the parameters is expect you to pass into when you define a new instance of that class. `__call__` defines what happens when you use an instance as a function. Here, `__call__()` is used to define the forward pass of the class. For the general `Model()` class, Jeremy suggested that the loss is returned. As this is an image classification problem, I think that a metric such as accuracy can be calculated and returned here as well.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef test_grad(inp, targ):\n    model = Model(w1, b1, w2, b2)\n    loss = model(inp, targ)\n    model.backward()\n\n    try:\n        test_close(w2g, w2.g, eps=0.01)\n        test_close(b2g, b2.g, eps=0.01)\n        test_close(w1g, w1.g, eps=0.01)\n        test_close(b1g, b1.g, eps=0.01)\n        test_close(ig, x_train.g, eps=0.01)\n        print(\"Test passed\")\n    except AssertionError as e:\n        print(f\"Test failed on line {e.__traceback__.tb_lineno}\")\n        print(f\"{e.args[0]} is different from {e.args[1]}. Please check the implementation associated with the particular weight again.\")\n        raise Exception\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntest_grad(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest passed\n```\n:::\n:::\n\n\nPerfect - no error! This is means that the gradient is calculated correctly.\n\nHowever, in practice, if I am a programmer, I would rather you do not modify dunder methods if necessary. In PyTorch this is avoided by creating a base class (maybe called `Module` or `nn.Module`) having `__call__()` calling a `forward()` method instead. In subsequent inherited class, whether the official `nn.Linear` or a newly designed layer such as `nn.MultiHeadAttention`, the user only needs to modify the public method `forward()`.\n\n## `Module.forward()`\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n    def bwd(self): raise Exception('not implemented')\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nclass Relu(Module):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nclass Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n    def forward(self, inp): return inp@self.w + self.b\n    def bwd(self, out, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nclass Mse(Module):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntest_grad(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest passed\n```\n:::\n:::\n\n\n## PyTorch-ic `Model`\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nclass Model(nn.Module):\n    def __init__(self, n_in, n_hidden, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_out))\n    \n    def forward(self, x, target):\n        x = self.layers(x)\n        return F.mse_loss(x, target[:, None])\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nmodel = Model(feature, num_hidden, 1)\nloss = model(x_train, y_train.float())\nloss.backward()\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nl0 = model.layers[0]\nl0.bias.grad\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\ntensor([-0.11,  0.72,  0.51,  0.25, -0.49,  0.30, -0.32,  0.32, -0.27, -0.80, -0.13, -0.54,  0.01,  0.05, -0.16,  0.64, -0.07,  0.96, -0.04,\n        -1.04, -0.63, -0.91,  0.74,  0.43,  0.14,  0.11, -0.07, -0.17,  0.20, -0.23, -0.68, -0.16,  0.16, -0.23,  0.01,  0.03, -0.04,  0.79,\n         0.74, -0.24,  0.13,  0.44, -0.01,  0.93, -0.36,  0.53, -0.22,  0.66, -0.21,  0.07])\n```\n:::\n:::\n\n\n# Cross-entropy loss\n\nAt the moment, the output is directly passed into the loss function as it is just a single output. However, this is a poor choice to optimize the parameters.\n\n> The simplest explanation is the current loss measures the relative distance between class. For example, if the true class is 5 and our model outputs 6, it is worse than 7 but only as good as 4. The model may still be able to learn correctly, but there is better option.  \n\nThe output should be passed into a *softmax function*. The loss function should be *cross-entropy loss* instead of MSE loss. Let's unpack these. First, softmax functions:\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\nor more concisely:\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$\n\nSecond, cross-entropy loss:\n$$ -\\sum x\\, \\log p(x) $$\nwhere $x$ is the true class and $p(x)$ is the predicted probability of the true class. For $x$ one-hot encoded, the formula is reduced to $-\\log(p_{i})$ with $i$ the index of the desired target.\n\nThe softmax function is used to convert the output into a probability. You can see that it is just a way to calculate the proportion of something in a whole, though more sophisticated than simply $ \\frac{x_{i}}{x_{0} + x_{1} + \\cdots + x_{n-1}} $. The reason for the exponential is to make sure that the output is positive plus amplify the difference between classes.\n\nThe cross entropy loss effectively measures the distance between the true probability distribution and the predicted probability distribution. The true probability distribution is one-hot encoded, while the predicted one has one value for each class at each example, altogether summed to 1. The loss is the negative log of the predicted probability of the true class. If the model predicts a low probability i.e. close to 0, the loss will be very large, towards infinity (but not beyond). If the model appropriately predicts a high probability i.e. close to 1, the loss will be very small, towards 0. Just, finding the parameters to minimize the loss will have the effect of maximizing the probability of the true class.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nclass Model(nn.Module):\n    def __init__(self, n_in, n_hidden, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_out))\n    \n    def forward(self, x):\n        return self.layers(x)\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nmodel = Model(feature, num_hidden, 10)\npred = model(x_train)\npred.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\ntorch.Size([50000, 10])\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nlog_softmax(pred)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\ntensor([[-2.35, -2.23, -2.25,  ..., -2.38, -2.44, -2.27],\n        [-2.31, -2.22, -2.27,  ..., -2.36, -2.48, -2.39],\n        [-2.24, -2.27, -2.19,  ..., -2.41, -2.42, -2.27],\n        ...,\n        [-2.28, -2.27, -2.19,  ..., -2.42, -2.42, -2.31],\n        [-2.29, -2.30, -2.27,  ..., -2.29, -2.42, -2.33],\n        [-2.34, -2.25, -2.17,  ..., -2.26, -2.33, -2.37]], grad_fn=<LogBackward0>)\n```\n:::\n:::\n\n\nThe log of softmax can be more reliably calculated with the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp)\n\n$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\nwhere a is the maximum of the $x_{j}$. (This is already implemented in PyTorch.)\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n```\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ntest_close(logsumexp(pred), pred.logsumexp(-1))\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nsm_pred = log_softmax(pred)\nsm_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntensor([[-2.35, -2.23, -2.25,  ..., -2.38, -2.44, -2.27],\n        [-2.31, -2.22, -2.27,  ..., -2.36, -2.48, -2.39],\n        [-2.24, -2.27, -2.19,  ..., -2.41, -2.42, -2.27],\n        ...,\n        [-2.28, -2.27, -2.19,  ..., -2.42, -2.42, -2.31],\n        [-2.29, -2.30, -2.27,  ..., -2.29, -2.42, -2.33],\n        [-2.34, -2.25, -2.17,  ..., -2.26, -2.33, -2.37]], grad_fn=<SubBackward0>)\n```\n:::\n:::\n\n\nFrom the simplified formula above, to calculate the loss, only the softmax activation at that index is required. The value can be accessed with [integer array indexing](https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing), which is the same as in NumPy or PyTorch.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprint(\"This is the first 3 true classes: \\n\", y_train[:3])\nprint(\"This is the first 3 log softmax predictions for all classes: \\n\", sm_pred[:3,:])\nprint(\"This is the WRONG first 3 log softmax predictions for just the first 3 true classes: \\n\", sm_pred[:3,y_train[:3]])\nprint(\"This is the CORRECT first 3 log softmax predictions for just the first 3 true classes: \\n\", sm_pred[[0, 1, 2],y_train[:3]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis is the first 3 true classes: \n tensor([5, 0, 4])\nThis is the first 3 log softmax predictions for all classes: \n tensor([[-2.35, -2.23, -2.25, -2.15, -2.33, -2.34, -2.33, -2.38, -2.44, -2.27],\n        [-2.31, -2.22, -2.27, -2.18, -2.31, -2.33, -2.22, -2.36, -2.48, -2.39],\n        [-2.24, -2.27, -2.19, -2.28, -2.29, -2.38, -2.32, -2.41, -2.42, -2.27]], grad_fn=<SliceBackward0>)\nThis is the WRONG first 3 log softmax predictions for just the first 3 true classes: \n tensor([[-2.34, -2.35, -2.33],\n        [-2.33, -2.31, -2.31],\n        [-2.38, -2.24, -2.29]], grad_fn=<IndexBackward0>)\nThis is the CORRECT first 3 log softmax predictions for just the first 3 true classes: \n tensor([-2.34, -2.31, -2.29], grad_fn=<IndexBackward0>)\n```\n:::\n:::\n\n\nNote that integer array indexing requires an array (Python list, NumPy/PyTorch array) passed in. Using normal slicing will broadcast the result instead.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\nloss = nll(sm_pred, y_train)\nloss\n# Test against PyTorch implementation\ntest_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)\n```\n:::\n\n\nIn PyTorch, the two steps `F.log_softmax` and `F.nll_loss` are combined into `F.cross_entropy` so the logits from the model is enough. (This has something to do with better precision, which matters more as dataset size grows.)\n\n# Training loop\n\nThe basic training loop is contained within [a nursery rhyme](https://youtu.be/Nutpusq_AFw)\n\n\n{{< tweet mrdbourke 1450977868406673410 >}}\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}