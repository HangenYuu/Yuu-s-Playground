{
  "hash": "7836def1bcb70bdab1622853d1862a4a",
  "result": {
    "markdown": "---\ntitle: Deep Learning from the ground up - From tensor to multi-layer perceptron (1)\nsubtitle: Lesson 9 - 14 of fast.ai course part 2\nauthor: Pham Nguyen Hung\ndraft: false\ndate: last-modified\ncategories:\n  - code\n  - From scratch\nformat:\n  html:\n    toc: true\n    code-fold: false\n---\n\nIn the [first post](https://hangenyuu.github.io/h-notes/posts/fastai2-p1/), I finished backpropagation and the simple maths behind it. Now let's talk about the design of PyTorch...\n\n> Actually I am not including it in my post. You can read about it [here](https://pytorch.org/docs/stable/community/design.html). To feel the need for PyTorch or Keras (and then TensorFlow 2.x), check out this [script](https://github.com/jsyoon0823/TimeGAN/blob/master/timegan.py). **Warning:** the author defined four models as *functions* and *updated them together* in TensorFlow 1.x.\n\n... which leads to our need to refactor the layers into *objects*, instead of *functions*. On defining them as class, we make them reusable and reduce the amount of codes to be written.\n\n# Setup\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Follow the previous post\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom torch import tensor,nn\nimport torch.nn.functional as F\nfrom fastcore.test import test_close\n# ---\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n# ---\npath_data = Path('data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\nnum_data, feature = x_train.shape\nclasses = y_train.max() + 1\nnum_hidden = 50\n\nw1 = torch.randn(feature,num_hidden)\nb1 = torch.zeros(num_hidden)\nw2 = torch.randn(num_hidden,1)\nb2 = torch.zeros(1)\n\ndef lin(x, w, b): return x@w + b\n\ndef relu(x): return x.clamp_min(0.)\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t()\n    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    b.g = out.g.sum(0)\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = lin(inp, w1, b1)\n    l2 = relu(l1)\n    out = lin(l2, w2, b2)\n    diff = out[:,0]-targ\n    loss = diff.pow(2).mean()\n    \n    # backward pass:\n    out.g = 2.*diff[:,None] / inp.shape[0]\n    lin_grad(l2, out, w2, b2)\n    l1.g = (l1>0).float() * l2.g\n    lin_grad(inp, l1, w1, b1)\n\nforward_and_backward(x_train, y_train)\n\n# Save for testing against later\ndef get_grad(x): return x.g.clone()\nchks = w1,w2,b1,b2,x_train\ngrads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n```\n:::\n\n\n# Refactor model\n\n## First attempt\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n    \n    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nclass Lin():\n    def __init__(self, w, b): self.w,self.b = w,b\n\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n        \n    def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n```\n:::\n\n\nHere, I learnt about the dunder methods. They define the intrinsic appearance of a class. `__init__()` defines the parameters is expect you to pass into when you define a new instance of that class. `__call__` defines what happens when you use an instance as a function. Here, `__call__()` is used to define the forward pass of the class. For the general `Model()` class, Jeremy suggested that the loss is returned. As this is an image classification problem, I think that a metric such as accuracy can be calculated and returned here as well.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef test_grad(inp, targ):\n    model = Model(w1, b1, w2, b2)\n    loss = model(inp, targ)\n    model.backward()\n\n    try:\n        test_close(w2g, w2.g, eps=0.01)\n        test_close(b2g, b2.g, eps=0.01)\n        test_close(w1g, w1.g, eps=0.01)\n        test_close(b1g, b1.g, eps=0.01)\n        test_close(ig, x_train.g, eps=0.01)\n        print(\"Test passed\")\n    except AssertionError as e:\n        print(f\"Test failed on line {e.__traceback__.tb_lineno}\")\n        print(f\"{e.args[0]} is different from {e.args[1]}. Please check the implementation associated with the particular weight again.\")\n        raise Exception\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntest_grad(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest passed\n```\n:::\n:::\n\n\nPerfect - no error! This is means that the gradient is calculated correctly.\n\nHowever, in practice, if I am a programmer, I would rather you do not modify dunder methods if necessary. In PyTorch this is avoided by creating a base class (maybe called `Module` or `nn.Module`) having `__call__()` calling a `forward()` method instead. In subsequent inherited class, whether the official `nn.Linear` or a newly designed layer such as `nn.MultiHeadAttention`, the user only needs to modify the public method `forward()`.\n\n## `Module.forward()`\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclass Module():\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n\n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n    def bwd(self): raise Exception('not implemented')\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nclass Relu(Module):\n    def forward(self, inp): return inp.clamp_min(0.)\n    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nclass Lin(Module):\n    def __init__(self, w, b): self.w,self.b = w,b\n    def forward(self, inp): return inp@self.w + self.b\n    def bwd(self, out, inp):\n        inp.g = self.out.g @ self.w.t()\n        self.w.g = inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nclass Mse(Module):\n    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntest_grad(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest passed\n```\n:::\n:::\n\n\n## PyTorch-ic `Model`\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nclass Model(nn.Module):\n    def __init__(self, n_in, n_hidden, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_out))\n    \n    def forward(self, x, target):\n        x = self.layers(x)\n        return F.mse_loss(x, target[:, None])\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nmodel = Model(feature, num_hidden, 1)\nloss = model(x_train, y_train.float())\nloss.backward()\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nl0 = model.layers[0]\nl0.bias.grad\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\ntensor([-0.11,  0.72,  0.51,  0.25, -0.49,  0.30, -0.32,  0.32, -0.27, -0.80, -0.13, -0.54,  0.01,  0.05, -0.16,  0.64, -0.07,  0.96, -0.04,\n        -1.04, -0.63, -0.91,  0.74,  0.43,  0.14,  0.11, -0.07, -0.17,  0.20, -0.23, -0.68, -0.16,  0.16, -0.23,  0.01,  0.03, -0.04,  0.79,\n         0.74, -0.24,  0.13,  0.44, -0.01,  0.93, -0.36,  0.53, -0.22,  0.66, -0.21,  0.07])\n```\n:::\n:::\n\n\n# Cross-entropy loss\n\nAt the moment, the output is directly passed into the loss function as it is just a single output. However, this is a poor choice to optimize the parameters.\n\n> The simplest explanation is the current loss measures the relative distance between class. For example, if the true class is 5 and our model outputs 6, it is worse than 7 but only as good as 4. The model may still be able to learn correctly, but there is better option.  \n\nThe output should be passed into a *softmax function*. The loss function should be *cross-entropy loss* instead of MSE loss. Let's unpack these. First, softmax functions:\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\nor more concisely:\n$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$\n\nSecond, cross-entropy loss:\n$$ -\\sum x\\, \\log p(x) $$\nwhere $x$ is the true class and $p(x)$ is the predicted probability of the true class. For $x$ one-hot encoded, the formula is reduced to $-\\log(p_{i})$ with $i$ the index of the desired target.\n\nThe softmax function is used to convert the output into a probability. You can see that it is just a way to calculate the proportion of something in a whole, though more sophisticated than simply $ \\frac{x_{i}}{x_{0} + x_{1} + \\cdots + x_{n-1}} $. The reason for the exponential is to make sure that the output is positive plus amplify the difference between classes.\n\nThe cross entropy loss effectively measures the distance between the true probability distribution and the predicted probability distribution. The true probability distribution is one-hot encoded, while the predicted one has one value for each class at each example, altogether summed to 1. The loss is the negative log of the predicted probability of the true class. If the model predicts a low probability i.e. close to 0, the loss will be very large, towards infinity (but not beyond). If the model appropriately predicts a high probability i.e. close to 1, the loss will be very small, towards 0. Just, finding the parameters to minimize the loss will have the effect of maximizing the probability of the true class.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nclass Model(nn.Module):\n    def __init__(self, n_in, n_hidden, n_out):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_in, n_hidden),\n            nn.ReLU(),\n            nn.Linear(n_hidden, n_out))\n    \n    def forward(self, x):\n        return self.layers(x)\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nmodel = Model(feature, num_hidden, 10)\npred = model(x_train)\npred.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\ntorch.Size([50000, 10])\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nlog_softmax(pred)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\ntensor([[-2.35, -2.23, -2.25,  ..., -2.38, -2.44, -2.27],\n        [-2.31, -2.22, -2.27,  ..., -2.36, -2.48, -2.39],\n        [-2.24, -2.27, -2.19,  ..., -2.41, -2.42, -2.27],\n        ...,\n        [-2.28, -2.27, -2.19,  ..., -2.42, -2.42, -2.31],\n        [-2.29, -2.30, -2.27,  ..., -2.29, -2.42, -2.33],\n        [-2.34, -2.25, -2.17,  ..., -2.26, -2.33, -2.37]], grad_fn=<LogBackward0>)\n```\n:::\n:::\n\n\nThe log of softmax can be more reliably calculated with the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp)\n\n$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\nwhere a is the maximum of the $x_{j}$. (This is already implemented in PyTorch.)\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ndef logsumexp(x):\n    m = x.max(-1)[0]\n    return m + (x-m[:,None]).exp().sum(-1).log()\n\ndef log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n```\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ntest_close(logsumexp(pred), pred.logsumexp(-1))\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nsm_pred = log_softmax(pred)\nsm_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntensor([[-2.35, -2.23, -2.25,  ..., -2.38, -2.44, -2.27],\n        [-2.31, -2.22, -2.27,  ..., -2.36, -2.48, -2.39],\n        [-2.24, -2.27, -2.19,  ..., -2.41, -2.42, -2.27],\n        ...,\n        [-2.28, -2.27, -2.19,  ..., -2.42, -2.42, -2.31],\n        [-2.29, -2.30, -2.27,  ..., -2.29, -2.42, -2.33],\n        [-2.34, -2.25, -2.17,  ..., -2.26, -2.33, -2.37]], grad_fn=<SubBackward0>)\n```\n:::\n:::\n\n\nFrom the simplified formula above, to calculate the loss, only the softmax activation at that index is required. The value can be accessed with [integer array indexing](https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing), which is the same as in NumPy or PyTorch.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprint(\"This is the first 3 true classes: \\n\", y_train[:3])\nprint(\"This is the first 3 log softmax predictions for all classes: \\n\", sm_pred[:3,:])\nprint(\"This is the WRONG first 3 log softmax predictions for just the first 3 true classes: \\n\", sm_pred[:3,y_train[:3]])\nprint(\"This is the CORRECT first 3 log softmax predictions for just the first 3 true classes: \\n\", sm_pred[[0, 1, 2],y_train[:3]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis is the first 3 true classes: \n tensor([5, 0, 4])\nThis is the first 3 log softmax predictions for all classes: \n tensor([[-2.35, -2.23, -2.25, -2.15, -2.33, -2.34, -2.33, -2.38, -2.44, -2.27],\n        [-2.31, -2.22, -2.27, -2.18, -2.31, -2.33, -2.22, -2.36, -2.48, -2.39],\n        [-2.24, -2.27, -2.19, -2.28, -2.29, -2.38, -2.32, -2.41, -2.42, -2.27]], grad_fn=<SliceBackward0>)\nThis is the WRONG first 3 log softmax predictions for just the first 3 true classes: \n tensor([[-2.34, -2.35, -2.33],\n        [-2.33, -2.31, -2.31],\n        [-2.38, -2.24, -2.29]], grad_fn=<IndexBackward0>)\nThis is the CORRECT first 3 log softmax predictions for just the first 3 true classes: \n tensor([-2.34, -2.31, -2.29], grad_fn=<IndexBackward0>)\n```\n:::\n:::\n\n\nNote that integer array indexing requires an array (Python list, NumPy/PyTorch array) passed in. Using normal slicing will broadcast the result instead.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ndef nll(input, target): return -input[range(target.shape[0]), target].mean()\n\nloss = nll(sm_pred, y_train)\nloss\n# Test against PyTorch implementation\ntest_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)\n```\n:::\n\n\nIn PyTorch, the two steps `F.log_softmax` and `F.nll_loss` are combined into `F.cross_entropy` so the logits from the model is enough. (This has something to do with better precision, which matters more as dataset size grows.)\n\n# Training loop\n\nThe basic training loop is contained within [a nursery rhyme](https://youtu.be/Nutpusq_AFw)\n\n\n{{< tweet mrdbourke 1450977868406673410 >}}\n\n\n\nBasically, there are 4 steps in each batch:\n\n- Get model's predictions on a batch of inputs.\n- Compute the loss of the current batch.\n- Calculate the gradients of the loss with respect to every parameter of the model\n- Update parameters with those gradients, one small step (or large, as in [Super-Convergence](https://arxiv.org/abs/1708.07120) large) at a time to improve predictions.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# Hyperparameters\nbatch_size = 50\nlr = 0.5   # learning rate\nepochs = 3\nloss_func = F.cross_entropy\n\n# Helper functions\ndef accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()\ndef report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')\n```\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nfor epoch in range(epochs):\n    for i in range(0, num_data, batch_size):\n        s = slice(i, min(num_data,i+batch_size))\n        xb,yb = x_train[s], y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        with torch.no_grad():\n            for l in model.layers:\n                if hasattr(l, 'weight'):\n                    l.weight -= l.weight.grad * lr\n                    l.bias   -= l.bias.grad   * lr\n                    l.weight.grad.zero_()\n                    l.bias  .grad.zero_()\n    report(loss, preds, yb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.14, 0.96\n0.15, 0.94\n0.10, 0.94\n```\n:::\n:::\n\n\n# Training loop with `parameters`, `optim`, `Dataset`, and `Dataloader`\n\n## Parameters\n\n> Refactoring is about writing less code to do the same work - Jeremy Howard\n\nWhich essentially means designing with *reusability* in mind.\n\nBecause the training loop is central to deep learning, PyTorch designers have defined two important pieces: `model.parameters()` and `optim` submodule.\n\nIn a PyTorch module, the layers are stored inside `named_children()` attribute and the parameters are stored inside `parameters()`, which give you a generator object when you call it.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nclass MLP(nn.Module):\n    def __init__(self, n_in, n_hidden, n_out):\n        super().__init__()\n        self.l1 = nn.Linear(n_in,n_hidden)\n        self.l2 = nn.Linear(n_hidden,n_out)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x): return self.l2(self.relu(self.l1(x)))\n```\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nmodel = MLP(feature, num_hidden, 10)\nmodel\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\nMLP(\n  (l1): Linear(in_features=784, out_features=50, bias=True)\n  (l2): Linear(in_features=50, out_features=10, bias=True)\n  (relu): ReLU()\n)\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nprint('Generator object: ', model.named_children())\nfor name, layer in model.named_children():\n    print(f\"{name}: {layer}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerator object:  <generator object Module.named_children at 0x0000028D98B70E50>\nl1: Linear(in_features=784, out_features=50, bias=True)\nl2: Linear(in_features=50, out_features=10, bias=True)\nrelu: ReLU()\n```\n:::\n:::\n\n\nSo the information prints out when I called `model` is stored inside `model.named_children()`. As it is a Generator, I have to iterate through it to see the information.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# Similarly for parameters\nprint(model.parameters())\nlist(model.parameters())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<generator object Module.parameters at 0x0000028D98811E00>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n[Parameter containing:\n tensor([[-0.03, -0.03, -0.01,  ..., -0.02, -0.02, -0.00],\n         [-0.02, -0.01,  0.00,  ...,  0.03, -0.00,  0.03],\n         [ 0.01, -0.02, -0.00,  ...,  0.00, -0.02,  0.01],\n         ...,\n         [ 0.02,  0.03,  0.01,  ...,  0.00,  0.01, -0.02],\n         [-0.03,  0.01,  0.02,  ...,  0.02,  0.03,  0.01],\n         [ 0.00, -0.03,  0.01,  ...,  0.01,  0.01, -0.01]], requires_grad=True),\n Parameter containing:\n tensor([-0.00, -0.01,  0.03,  0.02,  0.03,  0.02,  0.01, -0.03, -0.01, -0.01,  0.02, -0.02, -0.04, -0.01,  0.03, -0.02,  0.01, -0.03, -0.01,\n          0.01,  0.03, -0.03, -0.02, -0.01,  0.01,  0.01,  0.00, -0.01,  0.02,  0.01,  0.03, -0.02, -0.00, -0.03,  0.01, -0.00,  0.01,  0.01,\n          0.03,  0.02,  0.03,  0.03,  0.02, -0.04,  0.03,  0.01,  0.01, -0.01, -0.03,  0.03], requires_grad=True),\n Parameter containing:\n tensor([[    -0.10,     -0.08,      0.13,     -0.11,      0.05,     -0.03,     -0.11,     -0.14,     -0.11,      0.11,     -0.09,      0.10,\n              -0.02,     -0.10,     -0.09,     -0.10,     -0.12,     -0.13,     -0.04,      0.10,      0.03,      0.09,     -0.12,     -0.07,\n               0.05,      0.06,      0.10,     -0.04,     -0.07,     -0.11,     -0.09,     -0.09,     -0.05,      0.12,      0.00,     -0.02,\n               0.05,      0.13,     -0.04,      0.02,     -0.02,      0.06,     -0.11,     -0.04,      0.13,     -0.06,     -0.02,     -0.03,\n               0.04,      0.06],\n         [    -0.10,     -0.03,     -0.04,      0.03,     -0.03,      0.02,     -0.03,     -0.01,     -0.03,     -0.12,      0.02,      0.03,\n              -0.07,     -0.04,      0.02,      0.11,     -0.12,      0.13,      0.08,      0.11,      0.09,     -0.11,      0.00,     -0.12,\n              -0.07,     -0.13,     -0.10,      0.00,     -0.10,      0.06,     -0.13,     -0.02,      0.05,     -0.06,      0.11,     -0.06,\n               0.09,      0.07,     -0.08,      0.09,     -0.00,      0.13,     -0.02,      0.13,      0.08,     -0.07,      0.10,      0.01,\n               0.13,      0.03],\n         [    -0.12,      0.03,     -0.05,      0.05,      0.09,     -0.03,      0.03,      0.01,     -0.03,     -0.01,     -0.09,      0.07,\n               0.02,     -0.02,     -0.08,      0.03,     -0.13,     -0.02,      0.01,      0.02,      0.02,      0.04,      0.12,     -0.01,\n              -0.04,     -0.05,     -0.09,      0.06,      0.11,      0.10,      0.10,      0.04,     -0.03,      0.02,     -0.07,      0.05,\n               0.06,     -0.04,     -0.01,      0.02,      0.07,     -0.10,     -0.01,      0.02,      0.08,      0.03,      0.01,      0.01,\n               0.11,     -0.08],\n         [    -0.06,      0.06,     -0.14,      0.04,      0.09,     -0.08,     -0.01,     -0.01,      0.06,      0.14,      0.12,     -0.08,\n              -0.06,      0.13,     -0.11,     -0.11,     -0.04,      0.02,      0.08,      0.04,      0.10,      0.00,      0.01,     -0.08,\n              -0.12,      0.10,     -0.10,     -0.01,      0.02,      0.04,      0.06,     -0.13,     -0.11,     -0.05,      0.05,     -0.10,\n               0.09,      0.09,     -0.12,     -0.08,     -0.05,     -0.07,     -0.09,      0.12,      0.14,      0.08,      0.08,      0.12,\n               0.08,      0.09],\n         [     0.03,     -0.10,     -0.09,      0.13,      0.02,     -0.11,      0.05,      0.08,     -0.06,      0.07,     -0.09,      0.03,\n               0.00,     -0.00,      0.06,     -0.01,     -0.11,      0.02,     -0.14,      0.07,      0.12,      0.04,      0.03,      0.03,\n              -0.01,     -0.08,     -0.14,      0.13,      0.12,      0.10,      0.03,      0.09,     -0.07,      0.07,     -0.02,      0.08,\n              -0.10,      0.01,      0.11,      0.12,     -0.02,      0.05,      0.09,     -0.13,     -0.02,     -0.03,     -0.10,      0.13,\n               0.05,     -0.07],\n         [    -0.08,      0.02,      0.07,      0.14,      0.08,      0.04,     -0.12,     -0.14,      0.01,     -0.08,     -0.12,      0.14,\n              -0.05,     -0.09,     -0.06,      0.07,     -0.01,      0.11,      0.13,      0.14,     -0.06,      0.07,     -0.07,      0.12,\n              -0.00,     -0.07,      0.02,      0.11,     -0.02,     -0.04,      0.03,      0.09,     -0.11,      0.06,      0.10,     -0.06,\n               0.12,     -0.03,     -0.00,     -0.13,     -0.04,      0.11,     -0.01,      0.08,      0.03,      0.02,      0.04,      0.05,\n               0.04,     -0.09],\n         [    -0.09,      0.02,     -0.09,      0.13,      0.05,      0.12,      0.09,      0.02,     -0.07,     -0.10,     -0.01,      0.13,\n              -0.07,     -0.07,      0.11,      0.05,      0.13,     -0.06,     -0.11,     -0.04,      0.05,      0.13,      0.08,     -0.11,\n               0.01,     -0.07,     -0.10,      0.10,     -0.04,      0.02,      0.07,     -0.10,     -0.05,      0.09,     -0.03,     -0.12,\n              -0.11,      0.12,     -0.05,      0.10,     -0.06,     -0.10,     -0.01,     -0.04,      0.07,      0.06,     -0.12,     -0.11,\n               0.13,      0.04],\n         [    -0.06,     -0.09,     -0.09,     -0.04,      0.00,      0.10,     -0.13,     -0.13,     -0.08,      0.08,      0.02,      0.05,\n               0.06,      0.07,      0.03,     -0.14,      0.07,      0.00,     -0.08,      0.07,     -0.01,      0.11,     -0.01,      0.12,\n              -0.11,     -0.11,      0.03,     -0.09,      0.10,     -0.06,      0.04,     -0.11,     -0.14,     -0.02,     -0.06,     -0.04,\n               0.09,     -0.02,     -0.11,      0.05,      0.09,     -0.10,      0.12,     -0.07,     -0.05,     -0.03,      0.10,     -0.12,\n               0.06,      0.00],\n         [    -0.05,     -0.14,     -0.12,     -0.00,     -0.11,      0.03,      0.06,      0.07,      0.10,     -0.02,      0.01,      0.09,\n               0.06,      0.02,      0.14,      0.03,     -0.09,     -0.02,     -0.07,      0.13,      0.04,     -0.00,     -0.02,     -0.12,\n              -0.11,     -0.06,      0.00,      0.02,     -0.11,     -0.13,     -0.10,     -0.12,     -0.09,      0.09,     -0.09,      0.08,\n              -0.12,      0.07,      0.10,      0.13,     -0.05,      0.01,     -0.04,      0.00,      0.03,      0.12,     -0.11,     -0.07,\n               0.10,     -0.00],\n         [    -0.11,     -0.10,     -0.00,     -0.09,     -0.08,     -0.10,      0.09,      0.01,      0.01,      0.04,      0.10,     -0.04,\n              -0.14,     -0.04,      0.00,     -0.07,     -0.04,      0.12,     -0.06,     -0.10,     -0.04,     -0.12,      0.01,     -0.06,\n               0.12,      0.00,      0.04,      0.03,     -0.06,      0.07,      0.08,     -0.07,      0.12,      0.13,      0.01,     -0.03,\n               0.07,      0.10,     -0.14,     -0.12,      0.06,      0.09,     -0.10,     -0.05,     -0.01,      0.11,     -0.00,     -0.08,\n               0.13,      0.03]], requires_grad=True),\n Parameter containing:\n tensor([-0.13,  0.11, -0.08, -0.09, -0.12, -0.02,  0.04,  0.01,  0.13, -0.00], requires_grad=True)]\n```\n:::\n:::\n\n\nNow let's define a function for training the model using the `.parameters()` method.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\ndef fit():\n    for epoch in range(epochs):\n        for i in range(0, num_data, batch_size):\n            s = slice(i, min(num_data,i+batch_size))\n            xb,yb = x_train[s],y_train[s]\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters(): p -= p.grad * lr\n                model.zero_grad()\n        report(loss, preds, yb)\n```\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nfit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.10, 0.94\n0.08, 0.98\n0.11, 0.96\n```\n:::\n:::\n\n\nBehind the scene, PyTorch overrides the `__setattr__()` method in `nn.Module` to properly register the parameters. The code is equivalent to\n```{.python}\n# Taken straight from Jeremy Howard\n\nclass MyModule:\n    def __init__(self, n_in, nh, n_out):\n        self._modules = {}\n        self.l1 = nn.Linear(n_in,nh)\n        self.l2 = nn.Linear(nh,n_out)\n\n    def __setattr__(self,k,v):\n        if not k.startswith(\"_\"): self._modules[k] = v\n        super().__setattr__(k,v)\n\n    def __repr__(self): return f'{self._modules}'\n    \n    def parameters(self):\n        for l in self._modules.values(): yield from l.parameters()\n```\nThe layers are registered as attributes of the new instance. A *private* (with leading underscore) dictionary is initialized and used to keep track of the *public* attributes of the instance. `__setattr__()` is called when a new instance is initialized. All public attributes are written into the dictionary, and `super().__setattr__(k,v)` is calling the `__setattr__()` of the parent `object` class to properly register the attributes in the new instance. To output the nice summary when the instance is called, `__repr__()` must be defined. Lastly, to mimic the effect of calling `.parameters()`, a `.parameters()` method is defined, but instead of `return`, it `yield` the parameters of the layers, one parameter at a time, one layer at a time.\n\nNext, you might want to pass in a list of layers instead of specifying the hyperparameters. In PyTorch, there is `nn.ModuleList`, which is equivalent to calling `nn.Module.add_module()` method iteratively on a list. One step forward and there is `nn.Sequential()`, which I excuse myself using earlier. I will skip this here, but will dwell on an interesting piece: the use of `functools.reduce()`.\n\nIn forward, if I use `nn.Sequential()`, I can just call `self.layers(x)`. But under the hood, something similar to a loop\n```python\ndef forward(self, x):\n    for l in self.layers: x = l(x)\n    return x\n```\nIn Python, this can be reduced (pun not intended) to\n```python\ndef forward(self, x):\n    return reduce(lambda val, layer: layer(val), self.layers, x)\n```\nThis was implemented in Python \"by some Lisp hacker\". The idea is to iterative call a function (the `lambda` one) that takes in two arguments, one or both can be from an iterable (`self.layers`) and one from an optional initial value (`x`) and produce an intermediate value, which is then used to call the function together with the next value of the iterable, until there is only one answer left. That's too hand-wavy even for me, so let's consider this example demonstrating how to calculate the sum of an array\n```python\ndef sum(array):\n    return reduce(lambda a, b: a + b, array)\n```\nAnywho, having dealt with the parameters, the update loop is still verbose. The whole circle of updating the parameters and then zeroing the grads is implemented within `optim` for reusability. Let's implement that.\n\n## Optimizer\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nclass Optimizer():\n    def __init__(self, params, lr=0.5):\n        self.params = list(params)\n        self.lr     = lr\n\n    def step(self):\n        with torch.no_grad():\n            for p in self.params: p -= p.grad * self.lr\n\n    def zero_grad(self):\n        for p in self.params: p.grad.data.zero_()\n```\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nmodel = nn.Sequential(nn.Linear(feature, num_hidden), nn.ReLU(), nn.Linear(num_hidden,10))\nopt = Optimizer(model.parameters())\n```\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nfor epoch in range(epochs):\n    for i in range(0, num_data, batch_size):\n        s = slice(i, min(num_data,i+batch_size))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.11, 0.96\n0.07, 0.96\n0.09, 0.98\n```\n:::\n:::\n\n\nNow `torch.optim` is unlocked!\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nfrom torch import optim\ndef get_model():\n    model = nn.Sequential(nn.Linear(feature, num_hidden), nn.ReLU(), nn.Linear(num_hidden,10))\n    return model, optim.SGD(model.parameters(), lr=lr)\nmodel,opt = get_model()\n```\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nfor epoch in range(epochs):\n    for i in range(0, num_data, batch_size):\n        s = slice(i, min(num_data,i+batch_size))\n        xb,yb = x_train[s],y_train[s]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.11, 0.98\n0.09, 0.98\n0.06, 0.98\n```\n:::\n:::\n\n\n> Some may think that passing the generator `params` to `self.params` would work. It doesn't. The parameters need to be unpacked into a list storing inside the Optimizer. It is also the case in the [implementation](https://github.com/pytorch/pytorch/blob/main/torch/optim/optimizer.py#LL178C5-L206C57) of PyTorch.\n\n## Dataset and Dataloader\n\nFor organization, it is better to have the data processing codes grouped in one place and decoupled from the training loop. Thus, PyTorch comes prepared with `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` to achieve this goal.\n\nThe `Dataset` class should store the data with its label. A minimal implementation will be\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nclass Dataset():\n    def __init__(self, x, y): self.x,self.y = x,y\n    def __len__(self): return len(self.x)\n    def __getitem__(self, i): return self.x[i], self.y[i]\n```\n:::\n\n\nThe dunder method `__getitem__()` needs implementing to support square bracket indexing.\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\ndef test_Dataset(x_train, y_train, x_valid, y_valid):\n    try:\n        train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n        assert len(train_ds)==len(x_train), \"train_ds length does not match x_train length\"\n        assert len(valid_ds)==len(x_valid), \"valid_ds length does not match x_valid length\"\n        xb,yb = train_ds[0:5]\n        assert xb.shape==(5,28*28), \"xb shape is incorrect\"\n        assert yb.shape==(5,), \"yb shape is incorrect\"\n        print('All tests passed')\n    except AssertionError as e:\n        print(\"AssertionError:\", e)\n```\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\ntest_Dataset(x_train, y_train, x_valid, y_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAll tests passed\n```\n:::\n:::\n\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\ntrain_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\nmodel, opt = get_model()\nfor epoch in range(epochs):\n    for i in range(0, num_data, batch_size):\n        xb,yb = train_ds[i:min(num_data, i+batch_size)]\n        preds = model(xb)\n        loss = loss_func(preds, yb)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    report(loss, preds, yb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.12, 0.96\n0.07, 0.98\n0.05, 1.00\n```\n:::\n:::\n\n\n`DataLoader` is designed as an iterator wrapper for `Dataset`. It can be minimally constructed as\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nclass DataLoader():\n    def __init__(self, dataset, batch_size): self.dataset,self.batch_size = dataset,batch_size\n    def __iter__(self):\n        for i in range(0, len(self.dataset), self.batch_size): yield self.dataset[i:i+self.batch_size]\n```\n:::\n\n\nTo give the object iterator's functionalities, the dunder method `__iter__()` needs implementing.\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\ntrain_dl = DataLoader(train_ds, batch_size)\nvalid_dl = DataLoader(valid_ds, batch_size)\n```\n:::\n\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\nxb,yb = next(iter(valid_dl))\nprint(xb.shape)\nplt.imshow(xb[0].view(28,28))\nprint(yb[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([50, 784])\ntensor(3)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-45-output-2.png){width=415 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nmodel,opt = get_model()\n```\n:::\n\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\ndef fit():\n    for epoch in range(epochs):\n        for xb,yb in train_dl:\n            preds = model(xb)\n            loss = loss_func(preds, yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n        report(loss, preds, yb)\nfit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.09, 0.96\n0.05, 1.00\n0.02, 1.00\n```\n:::\n:::\n\n\n## Random Sampler\n\nIt is usually the case that training data is randomized for each batch. For that, I need a `Sampler`, which can be constructed from Python `random` module.\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\nimport random\nimport fastcore.all as fc\n\nclass Sampler():\n    def __init__(self, dataset, shuffle=False): self.n,self.shuffle = len(dataset),shuffle\n    def __iter__(self):\n        res = list(range(self.n))\n        if self.shuffle: random.shuffle(res)\n        return iter(res)\n\nclass BatchSampler():\n    def __init__(self, sampler, batch_size, drop_last=False): fc.store_attr()\n    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.batch_size, drop_last=self.drop_last)\n```\n:::\n\n\nLet's unpack the codes. In `Sampler()`, the logic of `__iter__()` means it will return the indices of the data, randomly shuffled if `shuffle = True`. In `BatchSampler()`, it takes the indices (can be from `Sampler()` or not) and then chunk them into an iterator that returns only 1 batch at a time. These randomized, batchified indices are then passed into the DataLoader to pull the data one batch at a time.\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\ndef collate(b):\n    xs,ys = zip(*b)\n    return torch.stack(xs),torch.stack(ys)\n\nclass DataLoader():\n    def __init__(self, dataset, batchs, collate_fn=collate): fc.store_attr()\n    def __iter__(self): yield from (self.collate_fn(self.dataset[i] for i in b) for b in self.batchs)\n```\n:::\n\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\ntrain_samp = BatchSampler(Sampler(train_ds, shuffle=True ), batch_size)\nvalid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), batch_size)\ntrain_dl = DataLoader(train_ds, batchs=train_samp)\nvalid_dl = DataLoader(valid_ds, batchs=valid_samp)\nxb,yb = next(iter(valid_dl))\nplt.imshow(xb[0].view(28,28))\nyb[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\ntensor(3)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-50-output-2.png){width=415 height=411}\n:::\n:::\n\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\nmodel,opt = get_model()\nfit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.25, 0.90\n0.33, 0.92\n0.08, 0.94\n```\n:::\n:::\n\n\nIn PyTorch, these are all built-in - one just need to specify `shuffle` option inside `DataLoader`.\n\n# Final loop\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\ndef get_dls(train_ds, valid_ds, batch_size, **kwargs):\n    return (DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs),\n            DataLoader(valid_ds, batch_size=batch_size*2, **kwargs))\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb,yb in train_dl:\n            loss = loss_func(model(xb), yb)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        model.eval()\n        with torch.no_grad():\n            tot_loss,tot_acc,count = 0.,0.,0\n            for xb,yb in valid_dl:\n                pred = model(xb)\n                n = len(xb)\n                count += n\n                tot_loss += loss_func(pred,yb).item()*n\n                tot_acc  += accuracy (pred,yb).item()*n\n        print(epoch, tot_loss/count, tot_acc/count)\n    return tot_loss/count, tot_acc/count\n```\n:::\n\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, batch_size)\nmodel,opt = get_model()\n%time loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0 0.14974518071860074 0.9573000037670135\n1 0.1304094205936417 0.9616000056266785\n2 0.15038241338916122 0.9566000056266785\n3 0.10842797339893878 0.9713000059127808\n4 0.1303363377088681 0.9622000086307526\nCPU times: total: 14.5 s\nWall time: 4.68 s\n```\n:::\n:::\n\n\n# Conclusion\nThat concludes everything. Next time, let's talk about using `nbdev` to export the most useful codes from notebooks into a module or a Python script.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}