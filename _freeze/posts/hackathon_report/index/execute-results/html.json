{
  "hash": "fa6155b3dc30e4b329b0e8c97f8b7f68",
  "result": {
    "markdown": "---\ntitle: A micro AI tool\nsubtitle: A writtent demo (and some reflections now and there) for the project for the Intuition hackathon by the NTU branch of IEEE\nauthor: Pham Nguyen Hung\ndraft: false\ndate: '2023-02-28'\ncategories:\n  - code\n  - engineering_hacking\nformat:\n  html:\n    toc: true\n    code-fold: false\n---\n\nOn the weekends of 25-26/02 I had the pleasure(?) of attending the [Intuition](https://intuition.ieeentu.com/) hackathon hosted by the NTU branch of IEEE with [Phan Nhat Hoang](https://www.linkedin.com/in/hoang-phan-nhat-8a3892191/) a.k.a John Phan. We did not win any prize this time (yes, there was a last time that we won, which deserved a post of it all, but not today). Consider this post the debrief for the two days.\n\nFirst, here is the [link](https://intuition-v9-0.devpost.com/project-gallery) to the gallery of the hackathon. Take some time to browse through it and you will notice that at least half of them mentioned GPT-3. Our project, [SumMed](https://devpost.com/software/summed-is-all-you-need), did, too. And we were not alone. After OpenAI released the APIs for their GPT 3.5 (`davinci`) and DALLÂ·E 2 model, there swiftly spawned a generation of pico AI start-ups that made use of the platform to build products that bring in good income. This was mentioned in Data Machina's [Newsletter 190](https://datamachina.substack.com/p/data-machina-190), together with a bag of tools termed \"*Modern AI Stack*\" by Carlos.\n\n![*Here is the full list for those who wonder. [Source](https://datamachina.substack.com/p/data-machina-190)*](Data Machina 190.png)\n\nIt was amazing how quickly people in tech caught on to something interesting. Or perhaps it was the ability to turn almost everything into interesting stuff. Anyway, I want to mention the newsletter first because it was our first mistake. We were not up with the news. Had only we known more about the trend in the field, we could have utilized more tools to save the work. As we were about to see, the biggest regret would be the front-end, which Hoang spent most of his time to write with React.js, while [another team](https://devpost.com/software/archmed) accomplished nearly the same thing and some more with [Streamlit](https://streamlit.io/). And it was also worth mentioning that neither of us know how to use Streamlit - Hoang fell into React.js out of habit. And we just straight up focused on OpenAI technology instead of considering others, with two worth mentioning being [HuggingFace](https://huggingface.co/) and [Colossal-AI](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt). There was no time, and we were not knowledgeable enough to utilize the tools.\n\nBefore moving on, it is worth mentioning that the \"mistake\" I wrote above needs reading as \"mistake in the context of a hackathon\". When you are in such a rush (<24 hours) and you are not a master learner who can acquire tools and integrate in the project at will (yet), you will need to prepare everything way before the event. I did not do that, because these skills were not the highest in my long-term priority yet (guess so for Hoang). A hackathon seemed big and important on the resume (especially when you are deep into it and do not have any sleep for the past 24 hours), but the long-term vision is always more important and always comes first.\n\nNow that is enough rambling. Onto the actual stuff.\n\n# Before SumMed\n\nThe hackathon was divided into 3 [tracks](https://intuition.ieeentu.com/#tracks): Healthcare track, FinTech track, and an Open track that also cover the two. We chose Healthcare track, with the lengthier problem statement.\n\n![](iNTUition v9.0.png)\n\nThe requirement was clear: zoom onto \"an AI tool that can automatically convert research articles into multimodal formats such as PowerPoint, blogs, and infographic posters.\" Anybody who caught wind of GPT-3 would think about calling an API together with the paper content to retrieve various summaries for the parts of the paper and create stuff (slides, infographic, or blog post) from them. Well, such was the majority of the submissions. For some reason, we got tunnel vision, did not realize this, and got stuck with a project that resembled everybody else. Eventually, the selection for winner became the selection for the prototype that was closer to the requirements ([here](https://github.com/jiawen3131/Hacknwhack)).\n\nBack to our product, it all started some time ago when Hoang introduced me to the concept of DocumentQA. This started with the discovery of the model to reason *in-context*. This is something that is unique to large language models (LLMs). Simply put, if we feed the model a context i.e. background information *that it has never seen before* together with a format of conversation that we desire, the model can immediately adapt to the format we want, and use the background information as the main source of knowledge to answer our prompt.\n\n![*An example from the [GPT-3 research paper](https://arxiv.org/pdf/2005.14165.pdf)*](GPT 3 SQuADv2.png)\n\nThis means that we can take a pre-trained LLM, which will act as a *document reader* and augment it with a *document retriever* to form a DocumentQA pipeline. You ask, the retriever performs preliminary search and takes out the relevant one to feed into the reader together with your question, and the reader answers after reading the document. A most notable example is [DrQA](https://github.com/facebookresearch/DrQA). For the particular case of OpenAI [GPT-3.5](https://platform.openai.com/docs/model-index-for-researchers) (`text-davinci-003`), there exists two applications available as retriever for the model: [LangChain](https://langchain.readthedocs.io/en/latest/) and [LlamaIndex (GPT Index)](https://gpt-index.readthedocs.io/en/latest/). We started simply with a Discord chatbot that used LlamaIndex to read an attachment (PDF, HTML, etc.) and answer a question that you send. I have not created a GitHub repo for it, but here is the [Repl](https://replit.com/@HangenYuu/PoliteWavyReciprocal).\n\nBecause of this toy project, we got tunnel vision into creating a chatbot for QA over a research paper, which was far from the point. We shifted gear after a Dr. from MSD set me straight about the project, and came up with SumMed.\n\n# Enter SumMed\n\nSumMed supported 3 features:\n\n1. Extract and display key information about a research paper.\n2. Extract and display all tables, figures, and charts from a research paper.\n3. Of course, a chatbot for QA over a researcher paper.\n\nThe diagram of the application is simple\n\n![](SumMed diagram.png)\n\nHoang was in charge of the intricate detail of the front-end and Flask app, which could be viewed in client folder of repo (again, [here](https://github.com/JohnToro-CZAF/MedSum/tree/main/client)). I was in charge of the model part of the back-end, and I was not a React.js pro, would not try to show you what Hoang had done. Instead, I will walk you through the back-end models: GPT-3.5 and Detectron2.\n\n> Note: Apparently, catching wind of this blog post, Hoang has refactored the codes. The codes in the post are `DocLayout.py`, `DocReader.py`, `DocSummarizer.py` in the `server` folder.\n\n## `DocLayout.py`\n\nFirst, the whole file:\n```python\nimport pdf2image\nimport numpy as np\nimport layoutparser as lp\nfrom collections import defaultdict\n\nclass DocLayout(object):\n    def __init__(self) -> None:\n        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n                                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n                                    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n        self.ocr_agent = lp.TesseractAgent(languages='eng')\n\n    def extract_pdf(self, file_name: str):\n        \"\"\" From a local file pdf file, extract the title, text, tables and figures\n        Args:\n            file_name (str): path to the pdf file\n        Returns:\n            title (str): title of the paper\n            Paper (str): text of the paper\n            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array\n            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array\n        \"\"\"\n        list_of_pages = pdf2image.convert_from_path(file_name)\n        images = [np.asarray(page) for page in list_of_pages]\n        image_width = len(images[0][0])\n\n        header_blocks, text_blocks, table_blocks, figure_blocks = self._detect_element(images)\n\n        title = self._extract_title(image_width, images, header_blocks)\n        Paper = self._extract_text_info(image_width, images, text_blocks)\n        table_by_page, figure_by_page = self._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)\n        # Currently we dont care about the order of the figures or tables returned\n        tables = self._general_by_table_to_list(table_by_page)\n        figures = self._general_by_table_to_list(figure_by_page)\n        return title, Paper, tables, figures\n    \n    def _general_by_table_to_list(self, general_by_page: dict):\n        return [general for i in general_by_page.keys() for general in general_by_page[i]]\n    \n    def _detect_element(self, images):\n        types = ['Title', 'Text', 'Table', 'Figure']\n        type_blocks = {\n            t: defaultdict(list) for t in types\n        }\n        for i in range(len(images)):\n            layout_result = self.model.detect(images[i])\n            for t in types:\n                type_block = lp.Layout([b for b in layout_result if b.type==t])\n                if len(type_block) != 0:\n                    type_blocks[t][i] = type_block\n        return type_blocks.values()\n    \n    \n    def _extract_title(self, image_width, images, header_blocks):\n        \"\"\"\n        Extract the title of the article from several headers\n        \"\"\"\n        first_page = min(header_blocks.keys())\n        segment_title = self._extract_page(first_page, image_width, images, header_blocks)[0]\n        title = self.ocr_agent.detect(segment_title)\n        return title\n    \n    def _extract_text_info(self, image_width, images, text_blocks):\n        \"\"\"\n        Returns all the text in the article\n        \"\"\"\n        Paper = \"\"\n        for page_id in text_blocks:\n            text_block_images = self._extract_page(page_id, image_width, images, text_blocks)\n            for block in text_block_images:\n                text = self.ocr_agent.detect(block).strip()\n                Paper += text + \" \"\n        return Paper\n\n    def _extract_table_n_figure(self, image_width, images, table_blocks, figure_blocks):\n        \"\"\"Extract 3D numpy array of tables and figures from deteced layout\n        Args:\n            image_width (int): width of image\n            images (_type_): _description_\n            table_blocks (_type_): _description_\n            figure_blocks (_type_): _description_\n        Returns:\n            table_by_page, figure_by_page (dict(list)): 3D numpy array of tables and figures by page\n        \"\"\"\n        \n        table_by_page, figure_by_page = defaultdict(list), defaultdict(list)\n        for page_id in table_blocks:\n            results = self._extract_page(page_id, image_width, images, table_blocks )\n            table_by_page[page_id] = results\n        \n        for page_id in figure_blocks:\n            results = self._extract_page(page_id, image_width, images, figure_blocks)\n            figure_by_page[page_id] = results\n        \n        return table_by_page, figure_by_page\n\n    def _extract_page(self, page_id, image_width, images, general_blocks):\n        \"\"\" \n        Get a list of 3D array numpy image of tables and figures, or text from a page\n        \"\"\"\n        results = []\n        left_interval = lp.Interval(0, image_width/2, axis='x').put_on_canvas(images[page_id])\n        left_blocks = general_blocks[page_id].filter_by(left_interval, center=True)._blocks\n        left_blocks.sort(key = lambda b: b.coordinates[1])\n\n        # Sort element ID of the right column based on y1 coordinate\n        right_blocks = [b for b in general_blocks[page_id] if b not in left_blocks]\n        right_blocks.sort(key = lambda b: b.coordinates[1])\n\n        # Sort the overall element ID starts from left column\n        general_block = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n\n        # Crop image around the detected layout\n        for block in general_block:\n            segment_image = (block\n                                .pad(left=15, right=15, top=5, bottom=5)\n                                .crop_image(images[page_id]))\n            results.append(segment_image)\n\n        return results\n```\nLet's dissect the codes.\n```python\nimport pdf2image\nimport numpy as np\nimport layoutparser as lp\nfrom collections import defaultdict\n\nclass DocLayout(object):\n    def __init__(self) -> None:\n        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n                                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n                                    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n        self.ocr_agent = lp.TesseractAgent(languages='eng')\n```\nThe star of the show is [LayoutParser](https://layout-parser.readthedocs.io/en/latest/) module, which employs a host of models from the [Detectron2 platform](https://github.com/facebookresearch/detectron2) for the task of document layout parsing. We used the best configuration suggested by the [docs](https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html) of [Mark RCNN](https://arxiv.org/pdf/1703.06870.pdf) trained on the [PubLayNet](https://arxiv.org/pdf/1908.07836.pdf) dataset of document layout analysis. As you can see, the model in this case can detect 5 elements in the dictionary `{0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}`. `pdf2image` and an `ocr_agent` needs importing and creating respectively because the model works on images, so we need to convert the PDF file to image(s) i.e. NumPy array(s) before doing anything.\n```python\nclass DocLayout(object):\n    def extract_pdf(self, file_name: str):\n        \"\"\" From a local file pdf file, extract the title, text, tables and figures\n        Args:\n            file_name (str): path to the pdf file\n        Returns:\n            title (str): title of the paper\n            Paper (str): text of the paper\n            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array\n            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array\n        \"\"\"\n        list_of_pages = pdf2image.convert_from_path(file_name)\n        images = [np.asarray(page) for page in list_of_pages]\n        image_width = len(images[0][0])\n\n        header_blocks, text_blocks, table_blocks, figure_blocks = self._detect_element(images)\n\n        title = self._extract_title(image_width, images, header_blocks)\n        Paper = self._extract_text_info(image_width, images, text_blocks)\n        table_by_page, figure_by_page = self._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)\n        # Currently we dont care about the order of the figures or tables returned\n        tables = self._general_by_table_to_list(table_by_page)\n        figures = self._general_by_table_to_list(figure_by_page)\n        return title, Paper, tables, figures\n\n    def _detect_element(self, images):\n        types = ['Title', 'Text', 'Table', 'Figure']\n        type_blocks = {\n            t: defaultdict(list) for t in types\n        }\n        for i in range(len(images)):\n            layout_result = self.model.detect(images[i])\n            for t in types:\n                type_block = lp.Layout([b for b in layout_result if b.type==t])\n                if len(type_block) != 0:\n                    type_blocks[t][i] = type_block\n        return type_blocks.values()\n```\n`pdf2image.convert_from_path()` returns a list of Pillow image, which needs converting to a list of NumPy arrays before work. Afterwards, in `_detect_element()` method, call `sel.model.detect()` to return a list of bounding boxes (represent by the top-left and right-bottom coordinates with respect to the particular page) with element type. The list of blocks returned will be processed accordingly.\n\n## `DocReader.py`\n```python\n# There are minor differences (by the time of post) from the file in the repo\nfrom llama_index  import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\nfrom langchain import OpenAI\n\nclass DocReader(object):\n    def __init__(self, directory_path, index_path):\n        self.index_path = index_path\n        self.directory_path = directory_path\n        self.max_input_size = 4096\n        self.num_outputs = 256\n        self.max_chunk_overlap = 20\n        self.chunk_size_limit = 600\n        self.llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.75, model_name=\"text-davinci-003\", max_tokens=self.num_outputs))\n        self.prompt_helper = PromptHelper(self.max_input_size, self.num_outputs, self.max_chunk_overlap, chunk_size_limit=self.chunk_size_limit)\n\n    def construct_index(self):\n        \"\"\"\n        Reconstruct the index, and save it to the database\n        \"\"\"\n        documents = SimpleDirectoryReader(self.directory_path).load_data()        \n        index = GPTSimpleVectorIndex(\n            documents, llm_predictor=self.llm_predictor, prompt_helper=self.prompt_helper\n        )\n        index.save_to_disk(self.index_path + '/index.json')\n\n    def predict(self, query):\n        index = GPTSimpleVectorIndex.load_from_disk(self.index_path + '/index.json')\n        response = index.query(query, response_mode=\"default\")\n        return response.response\n```\nA LlamaIndex workflow consists of 4 steps:\n\n1. Initialize an `LLMPredictor()` instance (based on LangChain [`LLM` and `LLMChain`](https://langchain.readthedocs.io/en/latest/modules/llms.html), which supports many other model hubs besides OpenAI). `LLMPredictor()` is a wrapper outside the model we use.\n2. Initialize a [`PromptHelper`](https://gpt-index.readthedocs.io/en/latest/reference/prompt_helper.html) instance that helps to define various parameters for the prompt.\n3. Index the document. There are many ways to achieve this, but the most simple way is calling `SimpleDirectoryReader()` to get the documents and `GPTSimpleVectorIndex()` to get the index that can be saved as a .json file.\n4. Query over the index. There are different, pre-defined response mode in LlamaIndex. Explore the docs for more.\n\nAnd that's it! Short and simple, yes powerful.\n\n## `DocSummarizer.py`\n```python\nimport numpy as np\nfrom PIL import Image\nimport os\nimport json\nfrom DocLayout import DocLayout\nfrom collections import defaultdict\n\nclass DocSummarizer(object):\n    def __init__(self, documents_path: str, resources_path: str):\n        self.documents_path = documents_path\n        self.resources_path = resources_path\n        self.prompt_tail = {\n            'authors': 'Who are the authors of this paper',\n            'summary':\"\\n\\nSummarize the above text, focus on key insights\",\n            'keyresults':'''\\n\\nGive me three key results in the format of \"Key results:\n                1.  Key result 1\n                2. Key result 2\n                3. Key result 3\"''',\n            'keyword':'\\n\\nGive me keywords in the format of \"Keywords:  Keyword 1, Keyword 2, Keyword 3\"',\n            'limitations':'\\n\\nGive me 3 sentences describing the limitations of the text above.'\n        }\n        self.layout_model = DocLayout()\n        \n    def get_summary(self, file_name: str, reader):\n        \"\"\"\n        Returns a summary of the document, this document is a pdf file that has been uploaded to the server.\n        And save the summary to the database/resources.\n        \"\"\"\n        title, Paper, tables, figures = self.layout_model.extract_pdf(self.documents_path + '/' + file_name)\n        authors, summary, keywords, keyresults, limitations = self._read(Paper, reader)\n        response = {\n          'title': title,\n          'authors': authors,\n          'summary': summary,\n          'key_concepts': keywords,\n          'highlights': keyresults,\n          'limitations': limitations,\n          'figures': [],\n          'tables': [],\n        }\n        \n        if not os.path.exists(self.resources_path + '/' + file_name[:-4]):\n            os.mkdir(self.resources_path + '/' + file_name[:-4])\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/info.json', 'w') as f:\n            json.dump(response, f)\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/title.txt', 'w') as f:\n            f.write(title)\n        \n        for idx, table in enumerate(tables):\n            im = Image.fromarray(table)\n            local_fn = file_name[:-4] + '*' + str(idx) + '_table.png'\n            table_fn = self.resources_path + '/' + file_name[:-4] + '/' + str(idx) + '_table.png'\n            im.save(table_fn)\n            response['tables'].append(local_fn)\n        \n        for idx, fig in enumerate(figures):\n            im = Image.fromarray(fig)\n            local_fn = file_name[:-4] + '*' + str(idx) + '_fig.png'\n            fig_fn = self.resources_path + '/' + file_name[:-4] + '/' + str(idx) + '_fig.png'\n            im.save(fig_fn)\n            response['figures'].append(local_fn)\n        \n        return response\n\n    def retrieve_summary(self, file_name: str):\n        \"\"\"\n        Returns a summary of the document (retrieve from resources), this document is a pdf file that already in the server.\n        \"\"\"\n        if not os.path.exists(self.resources_path + '/' + file_name[:-4]):\n            raise Exception('File not found')\n        \n        response = {\n          'title': None,\n          'authors': None,\n          'summary': None,\n          'key_concepts': None,\n          'highlights': None,\n          'limitations': None,\n          'figures': [],\n          'tables': [],\n        }\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/title.txt', 'r') as f:\n            response['title'] = f.read()\n        \n        response_js = json.load(open(self.resources_path + '/' + file_name[:-4] + '/info.json'))\n        response['authors'] = response_js['authors']\n        response['summary'] = response_js['summary']\n        response['key_concepts'] = response_js['key_concepts']\n        response['highlights'] = response_js['highlights']\n        response['limitations'] = response_js['limitations']\n          \n        for fn in os.listdir(self.resources_path + '/' + file_name[:-4]):\n            fn = file_name[:-4] + '*' + fn\n            if 'fig' in fn:\n                response['figures'].append(fn)\n            else:\n                response['tables'].append(fn)\n        return response\n    \n    def _read(self, Paper, reader):\n        \"\"\"\n        Read the text and returns the authors, summary, keywords, keyresults and limitations\n        \"\"\"\n        # TODO: Currently we use the Doc Reader service to read the text, but we need to implement our own service\n        response = defaultdict(str)\n        for query_type, prompt in self.prompt_tail.items():\n            ans_query = reader.predict(prompt + \"\".join(Paper[:500].split(\" \")[:20]))\n            response[query_type] = ans_query\n        \n        return response['authors'], response['summary'], response['keywords'], response['keyresults'], response['limitations']\n```\nThe `DocSummarizer` class continues where the `DocLayout` leaves. The text information retrieved from a document will be concatenated with a suitable prompt tail to send to OpenAI. Notice that each prompt tail is provided with a format for the model to follow (and it did follow!) in the response. For the graphics, they are converted from NumPy arrays to .PNG files in a folders that are accessible from the UI.\n\n## `app.py`\n\nAdd some magic from Flask\n\n## `client`\n\nAdd some magic from React.js\n\n## Result\n\nHere is the final demo capture of SumMed\n\n![*Left is the navigation bar displaying the papers. Center is the key information (text and graphics) + Slide maker that is not yet implemented. Right is the chatbox with information on the paper*](329814980 859809608651093_535713842991494898.png)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}