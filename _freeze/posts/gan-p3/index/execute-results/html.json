{
  "hash": "72cf89bbea70980c8351d1a9b06d23c9",
  "result": {
    "markdown": "---\ntitle: That Unstable GAN\nsubtitle: My notes on taking the specialization by deeplearning.ai series\nauthor: Pham Nguyen Hung\ndraft: false\ndate: '2023-01-24'\ncategories:\n  - code\n  - GAN\nformat:\n  html:\n    toc: true\n    code-fold: true\n---\n\nIn the last two posts, I have gone over the concept and process of training GANs. I made it seem so straightforward, but reality is harsher. Recall that for GANs, we are training two neural networks at once, and these two compete with each other to mutually improve. Much like real life, if one competitor is too good, the other will not learn anything, if the at point before the end the Generator or the Discriminator becomes too good (or the other becomes too bad), then training breaks down. Thus, training GANs is highly unstable, and any way to ensure smoother training is encouraged.\n\n![*I tried to find an xkcd comic for training GANs, but found none. Instead I found this [repo](https://github.com/generic-github-user/xkcd-Generator/) about using GANs to generate xkcd comic. It is not even close for a substitute, but you can defintely see that training has broken down: the loss of Generator is way much more than the loss of the Discriminator, and the difference between THIS and an [xkcd comic](https://xkcd.com/1838/) is obvious*](test 17.png)\n\n# General methods\n## Activation function\nActivation function is a requirement for neural networks' ability to approximate complex function. Without it, a neural network will become just another linear function.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\n\nimport numpy as np\nimport seaborn as sb\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(17)\ntorch.manual_seed(17)\ndef linear(a, b, x):\n    return a*x + b\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nx = torch.randn(50)\n\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, linear(.5, 4, x) + linear(3.56, -5.32, x) + linear(-1.86, 3.74, x), 'o--')\nax2.plot(x, torch.relu(0.5*x) + torch.relu(3.56*x) + torch.relu(-1.86*x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Stacking linear functions on top of each other is just a linear function. Meanwhile, stacking ReLU functions on top of each other create a piecemeal linear function that approximates a curve.](index_files/figure-html/fig-1-output-1.png){#fig-1 width=707 height=259}\n:::\n:::\n\n\nWe all starts with the sigmoid function in a binary cross-entropy problem. However, sigmoid, together with tanh, leads to the \"vanishing gradient\" problem. When the output value of gets close to 0 or 1 for sigmoid (or -1 or 1 for tanh), the gradient gets close to 0, so the weights either are updated very slowly or stop learning altogether. That was when ReLU came into play: the function has a clear, positive gradient when output value is greater than 0, while the bend makes sure that ReLU stacking on each other can produce a curve.\n\n![*Each neural network had three hidden layers with three units in each one. The only difference was the activation function. Learning rate: 0.03, regularization: L2. [Source](https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792)*](1 KKjPz4KaEERCpvI04D6Bng.webp)\n\n\nHowever, the joy ReLU brought came to halt when \"dying ReLU\" problem was reported. Suppose we have an output smaller or equal 0, then our derivative will be 0. The 0 derivative on the node means that it will not get updated, and that's the end for it. Worse, the previous components connected to the node are affected as well, so the whole structure of our neural network will be \"dead\". To fix, we have the variation: LeakyReLU. For LeakyReLU, the output value below 0 is not set at 0, but is multiplied by a constant (such as 0.2). Gradient for such value will still be non-zero, provide information to update the weights.\n\nAnother, more advanced variation is [GeLU](https://ar5iv.labs.arxiv.org/html/1606.08415v4), where the output is multiplied with i.e. weighted by its percentile. Sounds too complicated? Look at the formula:\n$$GELU(x)=x*P(X<x)=x*\\Phi(x)$$\nfor $X$ ~ $\\mathcal{N}(0, 1)$\n\nGELU has been successfully applied in Transformer models such as [BERT](https://ar5iv.labs.arxiv.org/html/1810.04805v2), [GPT-3](https://ar5iv.labs.arxiv.org/html/2005.14165v4), and especially in CNN such as [ConvNeXts](https://ar5iv.labs.arxiv.org/html/2201.03545). (Yeah, look at ConvNeXts - it started with a ResNet, the great ConvNet architecture, then the authors slowly introduced all the modern training tricks, until the result surpassed the Swin Transformer in the cheer of CNN-backer/Transformer-haters. Okay, that was eaxaggerating, but still...)\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, F.leaky_relu(x, negative_slope=0.1), 'o--')\nax2.plot(x, F.gelu(x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![LeakyReLU and GELU](index_files/figure-html/fig-2-output-1.png){#fig-2 width=719 height=259}\n:::\n:::\n\n\nNow let's move on to the second general trick that we have already done: batch normalization.\n\n## Batch normalization\nWe all know that neural netowrk is trying to appromixate a certain way of mapping inputs i.e. data to outputs. The parameters of a neural network therefore depend on the data we receive, characteristically the *distribution of the data*. Here I have this example of an HDR image, which captures a farther range of color and exposure than a compressed format such as JPG or PNG. I found the original image from the Internet [here](https://blog.gregzaal.com/2014/03/29/pano-golden-gate/)\n\n![*The curve at the bottom that may remind you of a bell curve is the curve for the distribution of pixel values a.k.a colors*](Screenshot 2023-01-24 205502.png)\n\nNow, we train a neural network on data having similar color distribution such as this image, possibly for the task of recognizing grass. The model was trained well. Alas, the testing image contains one such as this\n\n![*This was the exact same image, but compressed at a differen color distribution (shifted to the right)*](Screenshot 2023-01-24 205554.png)\n\nHere we say that the data distribution *has shifted between training data and testing data*. This generally will cause model problems (decrease accuracy, etc.). Data distribution shift (or covariate shift) can also happen between batches of training data, leading to slow convergence (imagine the model has to take a zig-zag path instead of a straight one). This can be dealt with by *normalization*, where make sure that the distributions of the training set and the testing set are similar e.g. centered around a mean of 0 and a standard deviation of 1. This could be done by taking the mean and standard deviation for each training batch of image and normalize the inputs of each training batch, then take the accumulated statistics to normalize the testing set during testing. This will smooth out the cost function and increases model performance (you might not need to do this if your training set and testing set are already similar to each other).\n\nHowever, model is susceptible to *internal covariate shift* as well, where the activation output distributions shift between each layer. This can happen due to the change in the weights of each layer. Batch normalization came into play here by normalizing the inputs to each layer (\"batch\" means that we do so for each batch of image). For example, supposed are at nueron $i$ of non-last layer $l$, with activated output from the last layer to this neuron being $a_{i}^{[l-1]}$. The logit out of this neuron will be\n$$z_{i}^{[l]}=\\Sigma W_{i}^{[l]}a_{i}^{[l-1]}$$\n\nWithout batch normalization, the logit will be passed into activation to output $a_{i}^{[l]}$. But here, we will perform batch normalization:\n\n1. We get the statistics mean $\\mu _{z_{i}^{[l]}}$ and variance $\\sigma _{z_{i}^{[l]}} ^{2}$ for the batch.\n2. We use them in the formula\n$$\\hat{z}_{i}^{[l]}=\\frac{z_{i}^{[l]}-\\mu _{z_{i}^{[l]}}}{\\sqrt{\\sigma _{z_{i}^{[l]}} ^{2} + \\epsilon}}$$\nNothing too fancy - it's just the normalization formula that you encounter in any statistics course/textbook: substract the value by the mean, then divide it by the square root of variance a.k.a the standard deviation. The $\\epsilon$ term is a positive constant there to make sure that the denominator is always positive.\n3. We map the normalized value $\\hat{z}_{i}^{[l]}$ to a new distribution with the formula\n$$y_{i}^{[l]}=\\gamma*\\hat{z}_{i}^{[l]} + \\beta$$\nwhere $\\gamma$ is *scale factor* and $\\beta$ the *shift factor*. These two are learnable inputs in the batch normalization layer, and will be tuned to figure out the best distribution for the task at hand.\n4. We pass $y_{i}^{[l]}$ through the activation function to the output $a_{i}^{[l]}$.\n\nThe batch normalization layer seems complicated, but we usually does not need to all the things. As backpropagation is reduced to just calling [`loss.backward`](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop) in PyTorch, the [`nn.BatchNorm2d()`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) (for images) will take care of this during training.\n\nThere is another normalization method called *layer normalization*. I will not go into details here, though I very much want to because it was used in the training of ConvNeXts as well (seriously, I want to make a blog post just about the tricks used in pushing this CNN to surpass Swin). Here is a [post](https://www.pinecone.io/learn/batch-layer-normalization/) about the two normalizations that also have great images. In PyTorch, this is implemented in [`nn.LayerNorm()`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n\n# GAN's specific method\nTo be honest, there should be tens of tricks for GANs. But I will only cover one this post: Wasserstein GAN (WGAN) and the accompanied Gradient Penalty.\n\n## WGAN:\nFirst, we need to talk about *mode collapse*. Now, a mode in statistical term is the value that we are most likely to get from a distribution (not too correct for continuous distribution, but still great for understanding). This will be represented by a peak in the data distribution, such as the mean in a normal distribution. A distribution can have just one mode, like the normal distribution, or multiple modes like below.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nsample1 = np.random.normal(loc=20, scale=5, size=300)\nsample2 = np.random.normal(loc=40, scale=5, size=700)\n# Concatenating the two sample along the second axis\nsample = np.hstack((sample1, sample2))\n\nsb.kdeplot(sample)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A bimodal distribution created by merging two normal distributions](index_files/figure-html/fig-3-output-1.png){#fig-3 width=606 height=404}\n:::\n:::\n\n\nThe outputs have their modes alright. For example, in our KMNIST dataset, there are 10 modes for the output, corresponding to 10 characters. So we have a new way to think about training: we are trying to make the model learn to shift the distribution of the outputs to approximate the one we want. For illustration, suppose initially our model is outputing each value for each pixel randomly, leading to an output distribution like this.\n\n![*Initial output distribution*](Copy of GAN-p2.png)\n\n\nWe want to change the output to this kind of distribution\n\n![*We want to shift from the circle to just 10 peaks i.e. just outputting 1 from 10 classes at a time*](Copy of GAN-p3.png)\n\n\nIn the ideal scenario, our model will be guided by the loss function to make the right shift. However, notice a lack in the BCE loss: it only promotes the model generating images close to real images, regardless of the class of the image. This means that there exists a quick n' dirty way for the Generator to reduce the loss by *only generating images from 1 class that the Discriminator is most fooled by*. So there exists a case where we end up with a Generator that outputs very realistic kanji character, only that it generates *kanji character*, say, only \"tsu\". That's boring. \n\nMy last paragraph gives hint to the source of the problem: our loss function. BCE loss works to push the Generator forward, but it cannot capture the information of class within the image. We need something else. And that something else is [Wasserstein GAN](https://ar5iv.labs.arxiv.org/html/1701.07875), introducing a new kind of loss function: the Wasserstein Loss (no surprise) a.k.a the *Earth Mover's distance*, or EM distance. It measures the difference between two distributions, and can be informally defined as the least amount of energy required to move and mold a earth pile in the shape of one distribution to the shape of another distribution (hence earth mover).\n\nIn mathematical form, if we have the noise vector $z$, the fake image data $x$, the Generator model $g()$, the Discriminator who becomes the Critic $c()$, then the Wasserstein Loss is the difference between the expected value i.e. the mean of the Critic outputs for real images and the mean of the Critic outputs for generated images.\n$$WLoss=E(c(x))-E(c(g(z)))$$\n\nWe still have a minimax game here, with the Generator's goal being minimize the above difference and the Critic's goal being maximize the above difference. Notice also that the Critic now is no longer a classifier: it can give any real value possible e.g. higher score for real(istic) examples. This means that the Generator will get useful feedback for all classes of examples, and is less prone to mode collapse. Getting rid of the classifier i.e. the sigmoid function in the output also means that vanishing gradient is also less likely. Two birds, one stone.\n\nW-Loss has one condition: the function of the Critic should be 1-Lipschitz Continuity. That looks intimidating but it just means that the norm of the gradient (the value in 2D math, the $\\sqrt{x^{2}+y^{2}+...}$ as example in higher dimensions) for the Critic can be at most 1 at any point. In other words, the output of the Critic cannot increase/decrease more than linearly at any point. To achieve this, the first proposed (and terrible way, according to the original author) was *weight clipping* - forcing the weights to a fixed interval, say, [0,1]. Any negative value will be set to 0, and any value more than 1 will be set to 1. This was terrible (I have to say it again) because it limits the potential of the Critic. Another less strict way is [gradient penalty](https://arxiv.org/abs/1704.00028) (the first dead ar5iv link), where you add a regularization term in the loss function to *promote* the Critic to be 1-Lipschitz Continuity, as oppposed to forcing it. Formula is\n$$WLoss=E(c(x))-E(c(g(z))) + \\lambda * pen$$\nwith $pen=E((|| \\nabla c(\\hat{x})||_{2}-1)^{2})$, $\\hat{x}=\\epsilon x + (1-\\epsilon)g(z)$\n\nFor completeness, the Critic gradient needs checking at every point in the feature space, which is impractical. What we do is sampling some points from real examples, and then some points from generated examples with weights for each, and then we calculate the penalty for the interpolated examples. An example for the code of WGAN can be found here (nothing for now). For a more technical review of WGAN, check out this [paper](https://ar5iv.labs.arxiv.org/html/1904.08994), also available as a [blog post](https://lilianweng.github.io/posts/2017-08-20-gan/).\n\nNext in line: Conditional GANs.\n\n*All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the [tool](https://ar5iv.labs.arxiv.org/). To change to the abstract page, follow this example:* `https://ar5iv.labs.arxiv.org/html/1409.1556` &rarr; `https://arxiv.org/abs/1409.1556`.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}