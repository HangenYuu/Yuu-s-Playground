{
  "hash": "e742775067df377a3f5a6b59c00b42b9",
  "result": {
    "markdown": "---\ntitle: Penalize that Unstable GAN\nsubtitle: My notes on taking the specialization by deeplearning.ai series\nauthor: Pham Nguyen Hung\ndraft: false\ndate: '2023-01-24'\ncategories:\n  - code\n  - GAN\nformat:\n  html:\n    toc: true\n    code-fold: true\n---\n\nIn the last two posts, I have gone over the concept and process of training GANs. I made it seem so straightforward, but reality is harsher. Recall that for GANs, we are training two neural networks at once, and these two compete with each other to mutually improve. Much like real life, if one competitor is too good, the other will not learn anything, if the at point before the end the Generator or the Discriminator becomes too good (or the other becomes too bad), then training breaks down. Thus, training GANs is highly unstable, and any way to ensure smoother training is encouraged.\n\n![*I tried to find an xkcd comic for training GANs, but found none. Instead I found this [repo](https://github.com/generic-github-user/xkcd-Generator/) about using GANs to generate xkcd comic. It is not even close for a substitute, but you can defintely see that training has broken down: the loss of Generator is way much more than the loss of the Discriminator, and the difference is obvious*](test 17.png)\n\n# General methods:\n## Activation function:\nActivation function is a requirement for neural networks' ability to approximate complex function. Without it, a neural network will become just another linear function.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(17)\ndef linear(a, b, x):\n    return a*x + b\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nx = torch.randn(50)\n\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, linear(.5, 4, x) + linear(3.56, -5.32, x) + linear(-1.86, 3.74, x), 'o--')\nax2.plot(x, torch.relu(0.5*x) + torch.relu(3.56*x) + torch.relu(-1.86*x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Stacking linear functions on top of each other is just a linear function. Meanwhile, stacking ReLU functions on top of each other create a piecemeal linear function that approximates a curve.](index_files/figure-html/fig-1-output-1.png){#fig-1 width=707 height=259}\n:::\n:::\n\n\nWe all starts with the sigmoid function in a binary cross-entropy problem. However, sigmoid, together with tanh, leads to the \"vanishing gradient\" problem. When the output value of gets close to 0 or 1 for sigmoid (or -1 or 1 for tanh), the gradient gets close to 0, so the weights either are updated very slowly or stop learning altogether. That was when ReLU came into play: the function has a clear, positive gradient when output value is greater than 0, while the bend makes sure that ReLU stacking on each other can produce a curve.\n\n![*Each neural network had three hidden layers with three units in each one. The only difference was the activation function. Learning rate: 0.03, regularization: L2. [Source](https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792)*](1 KKjPz4KaEERCpvI04D6Bng.webp)\n\n\nHowever, the joy ReLU brought came to halt when \"dying ReLU\" problem was reported. Suppose we have an output smaller or equal 0, then our derivative will be 0. The 0 derivative on the node means that it will not get updated, and that's the end for it. Worse, the previous components connected to the node are affected as well, so the whole structure of our neural network will be \"dead\". To fix, we have the variation: LeakyReLU. For LeakyReLU, the output value below 0 is not set at 0, but is multiplied by a constant (such as 0.2). Gradient for such value will still be non-zero, provide information to update the weights.\n\nAnother, more advanced variation is [GeLU](https://ar5iv.labs.arxiv.org/html/1606.08415v4), where the output is multiplied with i.e. weighted by its percentile. Sounds too complicated? Look at the formula:\n$$GELU(x)=x*P(X<x)=x*\\Phi(x)$$\nfor $X$ ~ $\\mathcal{N}(0, 1)$\nGELU has been successfully applied in Transformer models such as [BERT](https://ar5iv.labs.arxiv.org/html/1810.04805v2), [GPT-3](https://ar5iv.labs.arxiv.org/html/2005.14165v4), and especially in CNN such as [ConvNeXts](https://ar5iv.labs.arxiv.org/html/2201.03545). (Yeah, look at ConvNeXts - it started with a ResNet, the great ConvNet architecture, then the authors slowly introduced all the modern training tricks, until the result surpassed the Swin Transformer in the cheer of CNN-backer/Transformer-haters. Okay, that was eaxaggerating, but still...)\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, F.leaky_relu(x, negative_slope=0.1), 'o--')\nax2.plot(x, F.gelu(x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![LeakyReLU and GELU](index_files/figure-html/fig-2-output-1.png){#fig-2 width=719 height=259}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}