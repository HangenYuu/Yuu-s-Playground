{
  "hash": "0c32b6fd54b0274796383024fc270f36",
  "result": {
    "markdown": "---\ntitle: Building a simple GAN\nsubtitle: My notes on taking the specialization by deeplearning.ai series\nauthor: Pham Nguyen Hung\ndraft: false\ndate: '2023-01-23'\nhighlight-style: pygments\ncategories:\n  - code\n  - GAN\nformat:\n  html:\n    toc: true\n    code-fold: false\n---\n\nIn the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, [KMNIST](https://github.com/rois-codh/kmnist).\n\n# Concept - Training GANs:\nLike any training process, the process GANs can be decomposed into feed-forward and backpropagation, though with distinct features from, say, training a classifier. The parameters updated after backpropagation can also be divided into two steps, for Discriminator and Generator. These are summed up in the images below:\n\n![First, in the feed-forward, we pass some random noise (denoted by $\\xi$) into the Generator, which outputs some fake examples (denoted by $\\hat{X}$). The fake examples are then merged with a dataset of real examples (just $X$) and feed separately into the Discriminator, and we receive the outputs as a vector containing the possibilities of each example being real (between 0 and 1).](GAN-p2-1.png)\n\n![Second, for training the Discriminator. We will calculate the loss as binary cross-entropy (BCE) loss for two components: how closely to 0 the Discriminator predicted the fake examples, and how closely to 1 the Discriminator predicted the real examples. Here, we need to detach the Generator from the gradient flow as we want to update the Discriminator's parameters only](GAN-p2-2.png)\n\n![Third, for training the Generator. From the predictions for the fake examples, we calculate the BCE loss as how closely the Discriminator predicted them to 1. We then update the Generator's parameters.](GAN-p2-3.png)\n\nHopefully the ideas are not too complicated. If they are so, hopefully things will make more sense when we look at the codes.\n\n# Hands-on - Creating GANs:\n## The dataset:\nFirst rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.\n\n![*The 10 classes of Kuzushiji-MNIST, with the first column showing each character's modern hiragana counterpart. [Source](https://github.com/rois-codh/kmnist#the-dataset)*](kmnist examples.png)\n\n\nSimilar to MNIST, a KMNIST image has only one channel. Let's visualize one.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Function learnt from GAN's Specialization Course 1 Week 1\ndef tensor_show(image_tensor, num_images=25, size=(1, 28, 28)):\n    # The original image tensor could be stored on GPU and \n    # have been flattened out for training, so we restore it\n    # first.\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    # torch uses (color channel, height, width) while \n    # matplotlib used (height, width, color channel)\n    # so we fix it here\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Download needs to be set to True the first time you run it.\nbatch_size = 32\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=False, transform=transforms.ToTensor()),\n    batch_size=batch_size,\n    shuffle=True)\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimage_tensor = next(iter(dataloader))[0]\ntensor_show(image_tensor)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=416 height=408}\n:::\n:::\n\n\n## The Discriminator:\n> The architecture for each block of Discriminator and Generator follows the suggestions from the [Deep Convolutional GAN](https://ar5iv.labs.arxiv.org/html/1511.06434) paper.\n\nThe Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol' linear model, but I will skip a bit to the year 2015, when DCGAN was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A non-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN's training. We will touch upon tricks to stabilize GAN's training in the next post).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass Discriminator(nn.Module):\n    def __init__(self, image_channel=1, hidden_dim=56):\n        super().__init__()\n        self.disc = nn.Sequential(\n            self.make_disc_block(image_channel, hidden_dim),\n            self.make_disc_block(hidden_dim, hidden_dim),\n            self.make_disc_block(hidden_dim, 1, final_layer=True),\n        )\n\n    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                    nn.BatchNorm2d(output_channels),\n                    nn.LeakyReLU(negative_slope=0.25)\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n            )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.disc(x)\n        # The input can be a tensor of multiple images\n        # We want to return a tensor with the possibility\n        # of real/fake for each image.\n        return x.view(len(x), -1)\n```\n:::\n\n\n## The Generator:\nA point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially *distilling* the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the *transposed convolution* operation, which *increases* the dimensions of data, essentially *magnifying* the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this [notebook](https://github.com/HangenYuu/vision_learner/blob/main/ARCHITECTURE/CNN/Tiny/TinyCNN.ipynb) as my draft.)\n\nFirst, we need a function to generate noise. Basically, we need some tensor containing random numbers, and we can conveniently return a tensor filled with random numbers from a normal distribution with `torch.randn()`. For the dimensions, we define argument `z_dim` as the dimension of the noise input, and `n_samples` as the number of samples we need.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef generate_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn((n_samples, z_dim), device=device)\n```\n:::\n\n\nFor the `Generator` class, I will also create a function to construct each layer. A non-last layer will have a transposed convolution, followed by batch normalization and ReLU activation. The final layer does not have batch normalization but will have Tanh activation to squish the pixels in range.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nclass Generator(nn.Module):\n    def __init__(self, z_dim=14, image_channel=1, hidden_dim=56):\n        # z_dim is the dimension of the input noise vector\n        self.z_dim = z_dim\n        super().__init__()\n        self.gen = nn.Sequential(\n            self.make_gen_block(z_dim, hidden_dim * 4),\n            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n            self.make_gen_block(hidden_dim * 2, hidden_dim),\n            self.make_gen_block(hidden_dim, image_channel, kernel_size=4, final_layer=True),\n        )\n    \n    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.ReLU()\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh()\n            )\n    # Recall torch expect an image to be in the form (color channel, height, width).\n    # In a batch, torch expects it to be (no. of images in batch, color channel, height, width)\n    # So we need to transform the noise, originally in (no. of images in batch, input dimension)\n    # to (no. of images in batch, input dimension, 1, 1)\n    # See more here:\n    # https://pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images\n    def unsqueeze_noise(self, noise):\n        return noise.view(len(noise), self.z_dim, 1, 1)\n\n    def forward(self, noise):\n        x = self.unsqueeze_noise(noise)\n        return self.gen(x)\n```\n:::\n\n\n## Optimizers and Criterion\n\nNext, we want to define our optimizers (one for each model) and our criterion.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# We do not have activation at the output for Discriminator, so the outputs\n# are raw (logits).\ncriterion = nn.BCEWithLogitsLoss()\nz_dim = 64\ndisplay_step = 500\nbatch_size = 1000\n# Learning rate of 0.0002 and beta_1 (momentum term for Adam optimizer) of \n# 0.5 works well for DCGAN, according to the paper (yes, I seriously searched\n# for keyword \"learning rate\" in the paper)\nlr = 0.0002\nbeta_1 = 0.5 \nbeta_2 = 0.999\n# Device-agnostic code\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# You can tranform the image values to be between -1 and 1 (the range of the Tanh activation)\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n])\n\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=False, transform=transform),\n    batch_size=batch_size,\n    shuffle=True)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ngen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\ndisc = Discriminator().to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n\n# You initialize the weights to the normal distribution\n# with mean 0 and standard deviation 0.02\n# (Yes, the paper said so.)\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\n# Apply recursively weights_init() according to the docs:\n# https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init)\n```\n:::\n\n\n## Training\n\nOkay, now onto training!\n```python\nn_epochs = 100\ncur_step = 1 # For visualization purpose\nmean_generator_loss = 0\nmean_discriminator_loss = 0\nfor epoch in range(n_epochs):\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n        real = real.to(device)\n\n        ## Update Discriminator\n\n        # Empty the optimizer\n        disc_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise = generate_noise(cur_batch_size, z_dim, device=device)\n        fake = gen(fake_noise)\n        disc_fake_pred = disc(fake.detach())\n\n        # Calculate loss\n        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n\n        # Same for real examples\n        disc_real_pred = disc(real)\n        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n\n        # The Discriminator's loss is the average of the two\n        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n\n        # Keep track of the average Discriminator loss\n        mean_discriminator_loss += disc_loss.item() / display_step\n\n        # Update Discriminator's gradients a.k.a backpropagation\n        # Normally don't set retain_graph=True, but we do so for GAN\n        # as we need to propagate through the graph a second time\n        # when updating the Generator.\n        disc_loss.backward(retain_graph=True)\n\n        # Update Discriminator's optimizer\n        disc_opt.step()\n\n        ## Update Generator\n\n        # Empty the optimizer\n        gen_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise_2 = generate_noise(cur_batch_size, z_dim, device=device)\n        fake_2 = gen(fake_noise_2)\n        disc_fake_pred = disc(fake_2)\n\n        # Calculate loss\n        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n\n        # Backpropagation for Generator's loss\n        gen_loss.backward()\n\n        # Update Generator's optimizer\n        gen_opt.step()\n\n        # Keep track of the average Generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ## Visualization code\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n            tensor_show(fake)\n            tensor_show(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1\n```\n# Results:\nAfter training for 100 epochs with the settings, I received the final results.\n![*The above were the generated images, and the below were the real images.*](Screenshot 2023-01-23 175133.png)\n\n\nOne definitely can still discern which kanji characters were real and which were fake. However, one must admit that the model did manage to learn some of the intricate features of Japanese calligraphy. Then, after another 100 epochs...\n![](Screenshot 2023-01-23 183057.png)\n\nWhile it is still not yet indistinguishables, the Generator has gotten very close. The third image in the first row or the one at bottom left corner could definitely be passed off as real ones.\n\n# End of part 2\nIn this part, I have walked you through the training process and the building blocks of GANS. We have trained and witness good results from a simple model on the KMNIST dataset. In the next part, we will continue with the development of GANs, namely different ways to make training more stable. Up to now, it seemed really easy to achieve relatively good result with GANs (to be honest, we haven't tried anything too complicated, too), but it will much harder when we touch upon larger models. See you then.\n\n*All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the [tool](https://ar5iv.labs.arxiv.org/). To change to the abstract page, follow this example:* `https://ar5iv.labs.arxiv.org/html/1409.1556` &rarr; `https://arxiv.org/abs/1409.1556`.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}