{
  "hash": "1cc70fbcc3cd694752d859228d741687",
  "result": {
    "markdown": "---\ntitle: Building a simple GAN\nsubtitle: My notes on taking the specialization by deeplearning.ai series\nauthor: Pham Nguyen Hung\ndraft: false\ndate: '2023-01-22'\ncategories:\n  - code\n  - GAN\nformat:\n  html:\n    toc: true\n    code-fold: false\n---\n\nIn the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, [KMNIST](https://github.com/rois-codh/kmnist).\n\n# Concept - Training GANs:\nLike any training process, the process GANs can be decomposed into feed-forward and backpropagation, though with distinct features from, say, training a classifier. The parameters updated after backpropagation can also be divided into two steps, for Discriminator and Generator. These are summed up in the images below:\n\n![First, in the feed-forward, we pass some random noise (denoted by $\\xi$) into the Generator, which outputs some fake examples (denoted by $\\hat{X}$). The fake examples are then merged with a dataset of real examples (just $X$) and feed separately into the Discriminator, and we receive the outputs as a vector containing the possibilities of each example being real (between 0 and 1).](GAN-p2-1.png)\n\n![Second, for training the Discriminator. We will calculate the loss as binary cross-entropy (BCE) loss for two components: how closely to 0 the Discriminator predicted the fake examples, and how closely to 1 the Discriminator predicted the real examples. Here, we need to detach the Generator from the gradient flow as we want to update the Discriminator's parameters only](GAN-p2-2.png)\n\n![Third, for training the Generator. From the predictions for the fake examples, we calculate the BCE loss as how closely the Discriminator predicted them to 1. We then update the Generator's parameters.](GAN-p2-3.png)\nHopefully the ideas are not too complicated. If they are so, hopefully things will make more sense when we look at the codes.\n\n# Hands-on - Creating GANs:\n## The dataset:\nFirst rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.\n![*The 10 classes of Kuzushiji-MNIST, with the first column showing each character's modern hiragana counterpart. [Source](https://github.com/rois-codh/kmnist#the-dataset)*](kmnist_examples.png)\nSimilar to MNIST, a KMNIST image has only one channel. Let's visualize one.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Function learnt from GAN's Specialization Course 1 Week 1\ndef tensor_show(image_tensor, num_images=25, size=(1, 28, 28)):\n    # The original image tensor could be stored on GPU and \n    # have been flattened out for training, so we restore it\n    # first.\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    # torch uses (color channel, height, width) while \n    # matplotlib used (height, width, color channel)\n    # so we fix it here\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nbatch_size = 32\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=True, transform=transforms.ToTensor()),\n    batch_size=batch_size,\n    shuffle=True)\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimage_tensor = next(iter(dataloader))[0]\ntensor_show(image_tensor)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=416 height=408}\n:::\n:::\n\n\n## The Discriminator:\nThe Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol' linear model, but I will skip a bit to the year 2015, when [Deep Convolutional GAN](https://ar5iv.labs.arxiv.org/html/1511.06434) was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A not-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN's training. We will touch upon tricks to stabilize GAN's training in the next post).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nclass Discriminator(nn.Module):\n    def __init__(self, im_chan=1, hidden_dim=56):\n        super().__init__()\n        self.disc = nn.Sequential(\n            self.make_disc_block(im_chan, hidden_dim),\n            self.make_disc_block(hidden_dim, hidden_dim),\n            self.make_disc_block(hidden_dim, 1, final_layer=True),\n        )\n\n    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                    nn.BatchNorm2d(output_channels),\n                    nn.LeakyReLU(negative_slope=0.25)\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n            )\n    \n    def forward(self, x: torch.Tensor):\n        # The input can be a tensor of multiple images\n        # We want to return a tensor with the possibility\n        # of real/fake for each image.\n        x = self.disc(x)\n        return x.view(len(x), -1)\n```\n:::\n\n\n## The Generator:\nA point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially *distilling* the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the *transposed convolution* operation, which *increases* the dimensions of data, essentially *magnifying* the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this [notebook](https://github.com/HangenYuu/vision_learner/blob/main/ARCHITECTURE/CNN/Tiny/TinyCNN.ipynb) as my draft.)\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()        \n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}