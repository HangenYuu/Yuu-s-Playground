{
  "hash": "16ee692ee7018d10e4b9c9a5daca6c2c",
  "result": {
    "markdown": "---\ntitle: A Primer on Generative Adversarial Networks (GANs)\nsubtitle: My notes on taking the specialization by deeplearning.ai\nauthor: Pham Nguyen Hung\ndraft: false\ndate: '2023-01-21'\ncategories:\n  - code\n  - GAN\nformat:\n  html:\n    toc: true\n    code-fold: true\n---\n\nIf you have studied deep learning before, you will notice that we will encounter classification many times. To be honest, it is fun in a way, having your own model to classify anime characters. Alas, it is a bit dry to me. Intelligence, for me, is creativity, the ability to create something *new*. I want a model that can create, especially work of art. That led me right to GANs, not so much a model but an elegant way of thinking.\n\n# A brief history of GANs\n*For a fuller account, check out the [MIT Technology Review article](https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/).*\n\nBack in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) to [VGG](https://ar5iv.labs.arxiv.org/html/1409.1556). (Not to mention [ResNet](https://ar5iv.labs.arxiv.org/html/1512.03385) in 2015, an architecture with so interesting an idea that I had to [make a project](https://github.com/HangenYuu/vision_learner/tree/main/ARCHITECTURE/CNN/Paper) for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. \"Complex statistical analysis of the elements that make up a photograph\" was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow's seminal paper, [Generative Adversarial Nets](https://ar5iv.labs.arxiv.org/html/1406.2661).\n\n![*The image was not from the era, but was representative of what you got from the model at that time (and still now with GANs). [Source](https://www.kaggle.com/c/generative-dog-images/discussion/97753)*](Screenshot 2023-01-20 at 19-53-43 Generative Dog Images Kaggle.png)\n\nNow I wanted to make two quick detours before going into the inside of GANs:\n\n1. At its core sense, a *function* is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is *trying to update its parameters such that the model will approximate the optimal function as closely as possible*. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word \"tree\".\n2. Advances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.\n\n# The GANs game:\n**Note:** I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer's memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.\n\nThe word \"adversarial\" in GAN means \"involving or characterized by conflict or opposition\" according to Oxford Dictionary. Simply put, a GANs' system consists of, instead of one, two neural networks pitted against each other. The first one is called *Generator*, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called *Discriminator*, (or *Critic*, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.\n\n| | **Generator** | **Discriminator** |\n| --- | --- | --- |\n| **Input** | Random numbers | Images (real & generated) |\n| **Output** | Images | Class of image (binary) |\n| **Role** | Forger | Appraiser |\n\n**Quick detour:** the GAN concept advances generative AI the same way backpropagation does so. The approach of trying to know the distribution of the image features was right, but the method was wrong a.k.a too complex and computationally expensive. With GAN, we have an elegant way to start with any random distribution while moving towards the optimal distribution incrementally. No need to know everything any more.\n\nOur loss function will be the good ol' binary cross-entropy:\n$$J(\\theta) = -\\frac{1}{m}*[y^{(i)}log(h(x^{(i)}, \\theta)) + (1 - y^{(i)})log(1 - (h(x^{(i)}, \\theta)))]$$\n\nThat surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know $y^{(i)}$ is the true label of the i^th^ example (0 or 1), $h(x^{(i)}, \\theta)$ is the predicted label for the i^th^ example with input $x^{(i)}$ and parameters $\\theta$. With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the BCE function\ndef bce(y_true, y_pred):\n    return -1*(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ny_true = torch.zeros(50)\ny_pred = torch.linspace(0., 1., 50)\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![BCE loss when y = 0](index_files/figure-html/fig-0-output-1.png){#fig-0 width=605 height=439}\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ny_true = torch.ones(50)\ny_pred = torch.linspace(0., 1., 50)\ncriterion = torch.nn.BCELoss(reduction='none')\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![BCE loss when y = 1](index_files/figure-html/fig-1-output-1.png){#fig-1 width=605 height=439}\n:::\n:::\n\n\nI mentioned that this is a conflict between Generator and Discriminator. For Discriminator, it wants to classify correctly i.e. catch the Generator every time while approve the value of the real images. In other words, it wants to *minimize* its loss function. For Generator, it wants the reverse i.e. pass a fake as a real to the Discriminator every single time. In other words, it wants to *maximize* the loss function (of the Discriminator). This leads to the ter *minimax game* that you may hear some people use to describe GAN.\n\nThe game can be considered complete when the Discriminator's accuracy drops to 50% i.e. it can no longer discern, and essentially has to guess at random for each image. At this, our Generator will become potent enough to fool even us with its [humans](https://thispersondoesnotexist.com/) and [cats](https://thiscatdoesnotexist.com/).\n\n# End of part 1:\nAs a primer this is far enough. I will continue on the subject, describing each model's simplest architecture possible, the process of training one, as well as the difficulty in training GANs. (Training a model is hard enough, now we have two.)\n\n*All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the [tool](https://ar5iv.labs.arxiv.org/). To change to the abstract page, follow this example:* `https://ar5iv.labs.arxiv.org/html/1409.1556` &rarr; `https://arxiv.org/abs/1409.1556`.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}