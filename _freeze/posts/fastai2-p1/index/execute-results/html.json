{
  "hash": "8c0e3a0fa774d9073911b13fbcb4b0f4",
  "result": {
    "markdown": "---\ntitle: Deep Learning from the ground up - From tensor to multi-layer perceptron\nsubtitle: 'Lesson 9 - 14 of fast.ai course, part 2'\nauthor: Pham Nguyen Hung\ndraft: false\ndate: last-modified\ncategories:\n  - code\n  - From scratch\nformat:\n  html:\n    toc: true\n    code-fold: false\n---\n\nI have finished and fallen in love with fast.ai course 1. It has been very informative. It showed me the rope about PyTorch and two important building blocks of deep learning: Embedding and Convolution. I was excited to learn that there was a part 2. In this part, Jeremy will dive deeper into the design of a deep learning framework, and implement one from the scratch the way PyTorch was designed. Here were my (verbose) writtent version for it.\n\n# Rules\n\n- Permitted at the beginning: Python and all standard libraries, matplotlib, Jupyter Notebook and nbdev.\n- After deriving something, we can use the implemented version for that.\n\n# Get the data\nThe first thing you need to do is getting the data and visualize it in some way. The data we use is the good ol' MNIST, available from the Internet. Based on good practice, let's assign the URL to a variable, and prepare a directory to store the data.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n```\n:::\n\n\n`pathlib.Path` is a handy object as you can do special operations with string to receive a new `Path`, such as the division above. It is more readable than just raw strings.\n\nTo get the data, let's use `urllib.request.urlretrieve`\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n```\n:::\n\n\nThe data is compressed in the `.pkl.gz` format, which can be decompressed sequentially with `gzip` and `pickle`.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nwith gzip.open(path_gz, 'rb') as f:\n    # Omit test set for simplicity\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n```\n:::\n\n\n# Visualize the data\n\nGreat, we have decompressed the data, but what exactly are stored inside these variables?\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nx_train, x_valid\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n```\n:::\n:::\n\n\nOkay, they are `numpy.ndarray`. We are not allowed to use `numpy` yet, so we will need to convert it to list. Yep, very sorry about that. But first, uh, let's cheat a bit by checking the shape of the arrays.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx_train.shape, x_valid.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n((50000, 784), (10000, 784))\n```\n:::\n:::\n\n\nOkay, it seems that the $28 \\times 28 $ images are flattened into 784-element arrays. Convert any of the `x`s to a list will yield a big list of lists. It is unnecessary, so let's just take 1 data point.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx = list(x_train[0])\nx[:10], len(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 784)\n```\n:::\n:::\n\n\nTo visualize `x`, we need to convert it into a list of 28 lists, each with 28 elements. Is there a way to do that in Python? I said \"No\", but Jeremy showed that there are at least two ways\n\nFirstly, we can make use of the `yield` keyword, which is used to generate iterators in Python. We want to generate 28 iterators, each containing 28 elements, from `x`. In Python, it is as simple as\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef chunks(x, size):\n    max_size = len(x)\n    for i in range(0, max_size, size): yield x[i:min(max_size, i+size)]\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(x, 28)));\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=415 height=411}\n:::\n:::\n\n\nSecondly, we can use `itertools.islice`.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom itertools import islice\n\nit = iter(x[:10])\nprint('First',  list(islice(it, 5)))\nprint('Second', list(islice(it, 5)))\nprint('Third',  list(islice(it, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst [0.0, 0.0, 0.0, 0.0, 0.0]\nSecond [0.0, 0.0, 0.0, 0.0, 0.0]\nThird []\n```\n:::\n:::\n\n\nSimply, `islice(iterable, stop)` will return a new iterator from the iterable (which can be another iterator), stop at `stop`. Paired with default `start` and `step` of 0 and 1 respectively, it means that `islice(it, 5)` will return an iterator containing the first 5 values of `it`. Now we realize that doing so will also exhaust these first 5 values of `it`, so the next call will call the next 5 values of `it`. Paired with a loop, it works exactly like `chunks()` defined above.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nit = iter(x)\nimg = list(iter(lambda: list(islice(it, 28)), []))\nplt.imshow(img);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=415 height=411}\n:::\n:::\n\n\nWork like a charm. A note: that empty list passed in after the lambda function is for the sentinel argument of `iter()`, instructing `iter()` to stop when it encounters the sentinel.\n\n# Tensor, Matrix\nOkay. Now we need a way to represent data that is *machine-readable*. And the machine only knows number. So we need to represent the data as numbers. We are actually already there, with our list of lists of data, each element representing the intensity of the corresponding pixel of the image. So why the need for a different representation?\n\nThe simplest answer is *efficiency*. Python list was designed to be a generic, dynamic data structure, hence memory and operation overhead. Machine learning requires doing linear algebra and calculus (read: matrix multiplication, one way or another, and differentiation) on the data, which is not inherently supported. There arises the need for something more specialized. And that thing was *tensor*.\n\nSimply put, tensor is the generalized matrix, expanding to as many dimensions as it needs to be. It first proved to be an excellent way of representing data in physics, where it was the core of [tensor analysis](https://en.wikipedia.org/wiki/Tensor_calculus) (yep, Albert Einstein used that). Tensor made it way into computer firstly with the development of [APL](https://en.wikipedia.org/wiki/APL_(programming_language)) (A Programming Language) by Kenneth E. Iverson, where the central data structure is  multidimensional array (yes, `numpy.ndarray` borrowed this). NumPy was most famous project to bring this to Python, paving the way for easy and flexible data analysis in Python (Pandas, sci-kitlearn, etc.). Deep Learning frameworks such as TensorFlow, JAX, or PyTorch all took from this design, with its own tensor the core data structure.\n\nAnthropic's Claude summarized the key differences as follow:\n\n> Prompt: What is the different between Python list and Torch.tensor?\n\n> Here are the main differences between Python list and Torch.tensor:\n\n> 1. Data type: Python lists can contain elements of different data types, while Tensors have a single data type. All elements of a Tensor must be floats, ints, etc.\n\n> 2. Operations: Python lists support basic operations like insert, append, pop, etc. Tensors support tensor operations like sum, mean, transpose, dot product, etc.\n\n> 3. Memory management: Python lists are managed by the Python memory manager while Tensors are managed by the PyTorch library. This makes Tensors more optimized for machine learning tasks.\n\n> 4. Shape: Python lists don't have a fixed shape, they can have any number of elements. Tensors have a defined shape, usually 2D or higher.\n\n> 5. Parallelism: Operations on Tensors can take advantage of GPUs and multiple CPU cores, while list operations are sequential.\n\n> 6. Gradient tracking: Only Tensors can track gradients to enable backpropagation in neural networks.\n\n> So in summary, Python lists are more general purpose data structures while Torch Tensors are specialized data structures optimized for machine learning and neural network tasks. Tensors provide the functionality needed for training deep learning models.\n\nFor our case, let's start with matrix first. We will implement a minimal object to allow the usual tensor indexing on Python list. This requires us to define the dunder method `__getitem__()`\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nclass Matrix:\n    def __init__(self, data) -> None:\n        self.data = data\n    def __getitem__(self, idxs):\n        return self.data[idxs[0]][idxs[1]]\n\nm = Matrix(img)\nm[20,15]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.98828125\n```\n:::\n:::\n\n\nOkay, cool. Now we are allowed to use `torch.tensor`. Let's quickly convert our data to tensor with `map` and reshape an image for visualization it. Notice how easily we could do so with PyTorch.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport torch\nfrom torch import tensor\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\n\nprint('Notice the shape:', x_train.shape)\nprint('Notice the type:', x_train.type())\n\nimgs = x_train.reshape((-1,28,28))\nplt.imshow(imgs[0]);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNotice the shape: torch.Size([50000, 784])\nNotice the type: torch.FloatTensor\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=415 height=411}\n:::\n:::\n\n\nThe mapping was successfully. The shape and data type all becomes PyTorch's built-in ones. The whole `x_train` dataset can be reshaped into 10000 $28 \\times 28 $ quickly with `.reshape()` method and -1 indexing. \n\nWe now have matrix. Let's do matrix multiplication.\n\n# Linear Algebra: Matrix multiplication\n\n## Brute-force\n\nTo get to multilayer perceptron, we need to able to do matrix multiplication. Let's start from the basic first, with pen and paper.\n\nHere's an example:\n\n![](matmul 1.png)\n\nLet's zoom in at one cell of the result matrix:\n\n![](matmul 2.png)\n\nTo summarize, for each element in the result matrix, we get it by summing the product of each element in the corresponding row of the left matrix and each element in the corresponding column of the right matrix. In codes, this is translated into three nested loops:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntorch.manual_seed(1)\n# Randomly initialize weights and biases as in a real layer\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n# Work with a batch of 5 first - a 3 nested loops should be slow.\nm1 = x_valid[:5]\nm2 = weights\n\nm1.shape,m2.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(torch.Size([5, 784]), torch.Size([784, 10]))\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Store the number of rows and columns of each matrix\nar,ac = m1.shape\nbr,bc = m2.shape\n\n# t1 is the placeholder result matrix\nt1 = torch.zeros(ar, bc)\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\nt1, t1.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n(tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n         [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n         [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n         [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n         [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]]),\n torch.Size([5, 10]))\n```\n:::\n:::\n\n\nLet's package this into a function:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.11 s ± 5.27 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n## Speed-up: Dot product\n\nI want to get the objective clear: removing all of the three loops sequentially to speed things up. The first clue to do that is with dot product of two vectors. From the illustration, it is clear that each cell in result matrix is the result of the dot product between the left row vector and the right column vector. Unfortunately, we cannot use `torch.dot` yet. However, we can use *element-wise operation*, the trademark of tensor  \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.97 ms ± 417 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\nOkay, now `torch.dot` is free:\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = torch.dot(a[i,:], b[:,j])\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.82 ms ± 644 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n## Speed-up: Broadcasting\n\nThe next clue is another trademark of tensor: **broadcasting**, which allows tensors of different shapes to be multiplied together. This is the trademark of tensors, so make some time to familiarize yourself with the rules from [NumPy documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n\nFor demonstration, it is better to refer to the [original notebook](https://nbviewer.org/github/fastai/course22p2/blob/master/nbs/). I just want to mention two things. Firstly, `.unsqueeze()` and `None` indexing. Simply put, we can create a *unit dimension* in an array by using `.unsqueeze()` or passing in the special keyword `None` inside the indexing brackets.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nc = tensor([10.,20,30])\n\nprint('The first dimension:', c.unsqueeze(0), c[None, :])\nprint('The second dimension:', c.unsqueeze(1), c[:, None])\nprint('We can skip trailing \":\"', c[None])\nprint('We can also pass in \"...\"', c[..., None])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe first dimension: tensor([[10., 20., 30.]]) tensor([[10., 20., 30.]])\nThe second dimension: tensor([[10.],\n        [20.],\n        [30.]]) tensor([[10.],\n        [20.],\n        [30.]])\nWe can skip trailing \":\" tensor([[10., 20., 30.]])\nWe can also pass in \"...\" tensor([[10.],\n        [20.],\n        [30.]])\n```\n:::\n:::\n\n\nSecondly, that broadcasting compare array dimensions from *right to left*. This can lead to behavior such as this:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nc[None,:] * c[:,None]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n```\n:::\n:::\n\n\n![](matmul 3.png)\n\nAnyway, with broadcasting, we can now calculate the result matrix one row at a time and skip another loop:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        c[i]   = (a[i,:,None] * b).sum(dim=0)\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n344 µs ± 115 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\nAt this point, we basically arrive at matrix multiplication. However, let's up the amp a bit and (re)introduce Einstein.\n\n## Einstein summation\n\n> [Einstein summation](https://ajcr.net/Basic-guide-to-einsum/) (`einsum`) is a compact representation for combining products and sums in a general way. The key rules are:\n> - Repeating letters between input arrays means that values along those axes will be multiplied together.\n> - Omitting a letter from the output means that values along that axis will be summed.\n\nExample:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nm1.shape, m2.shape\n\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntorch.Size([5, 784, 10])\n```\n:::\n:::\n\n\nTo use `torch.einsum`, we need to pass in a string telling the operation we want to achieve. The string above means \"multiplying each column of `m1` by `m2`\". Notice that we can sum the result matrix along the first dimension to get the result matrix.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nmr.sum(1)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\nThis is equivalent to the notation of matrix multiplication\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ntorch.einsum('ik,kj->ij', m1, m2)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\nSo we can replace everything with a `torch.einsum` call:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 4.28 times longer than the fastest. This could mean that an intermediate result is being cached.\n65 µs ± 38 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\nHaving come to this point, let's introduce `torch` predefined operations:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n%timeit -n 5 torch.matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 12.71 times longer than the fastest. This could mean that an intermediate result is being cached.\n26.1 µs ± 39.2 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n%timeit -n 5 m1@m2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 17.55 times longer than the fastest. This could mean that an intermediate result is being cached.\n28.2 µs ± 47 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n# Calculus: Differentiation and Chain Rule\nOkay, great. We have built the linear algebra needed for \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}