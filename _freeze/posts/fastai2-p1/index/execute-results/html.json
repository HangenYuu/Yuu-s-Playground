{
  "hash": "3e009046ffc493ded9f40f5138d8212a",
  "result": {
    "markdown": "---\ntitle: Deep Learning from the ground up - From tensor to multi-layer perceptron\nsubtitle: 'Lesson 9 - 14 of fast.ai course, part 2'\nauthor: Pham Nguyen Hung\ndraft: false\ndate: last-modified\ncategories:\n  - code\n  - From scratch\nformat:\n  html:\n    toc: true\n    code-fold: false\n---\n\nI have finished and fallen in love with fast.ai course 1. It has been very informative. It showed me the rope about PyTorch and two important building blocks of deep learning: Embedding and Convolution. I was excited to learn that there was a part 2. In this part, Jeremy will dive deeper into the design of a deep learning framework, and implement one from the scratch the way PyTorch was designed. Here were my (verbose) writtent version for it.\n\n# Rules\n\n- Permitted at the beginning: Python and all standard libraries, matplotlib, Jupyter Notebook and nbdev.\n- After deriving something, we can use the implemented version for that.\n\n# Get the data\nThe first thing you need to do is getting the data and visualize it in some way. The data we use is the good ol' MNIST, available from the Internet. Based on good practice, let's assign the URL to a variable, and prepare a directory to store the data.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n```\n:::\n\n\n`pathlib.Path` is a handy object as you can do special operations with string to receive a new `Path`, such as the division above. It is more readable than just raw strings.\n\nTo get the data, let's use `urllib.request.urlretrieve`\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n```\n:::\n\n\nThe data is compressed in the `.pkl.gz` format, which can be decompressed sequentially with `gzip` and `pickle`.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nwith gzip.open(path_gz, 'rb') as f:\n    # Omit test set for simplicity\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n```\n:::\n\n\n# Visualize the data\n\nGreat, we have decompressed the data, but what exactly are stored inside these variables?\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nx_train, x_valid\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n```\n:::\n:::\n\n\nOkay, they are `numpy.ndarray`. We are not allowed to use `numpy` yet, so we will need to convert it to list. Yep, very sorry about that. But first, uh, let's cheat a bit by checking the shape of the arrays.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nx_train.shape, x_valid.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n((50000, 784), (10000, 784))\n```\n:::\n:::\n\n\nOkay, it seems that the $28 \\times 28 $ images are flattened into 784-element arrays. Convert any of the `x`s to a list will yield a big list of lists. It is unnecessary, so let's just take 1 data point.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nx = list(x_train[0])\nx[:10], len(x)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 784)\n```\n:::\n:::\n\n\nTo visualize `x`, we need to convert it into a list of 28 lists, each with 28 elements. Is there a way to do that in Python? I said \"No\", but Jeremy showed that there are at least two ways\n\nFirstly, we can make use of the `yield` keyword, which is used to generate iterators in Python. We want to generate 28 iterators, each containing 28 elements, from `x`. In Python, it is as simple as\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef chunks(x, size):\n    max_size = len(x)\n    for i in range(0, max_size, size): yield x[i:min(max_size, i+size)]\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(x, 28)));\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=415 height=411}\n:::\n:::\n\n\nSecondly, we can use `itertools.islice`.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom itertools import islice\n\nit = iter(x[:10])\nprint('First',  list(islice(it, 5)))\nprint('Second', list(islice(it, 5)))\nprint('Third',  list(islice(it, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst [0.0, 0.0, 0.0, 0.0, 0.0]\nSecond [0.0, 0.0, 0.0, 0.0, 0.0]\nThird []\n```\n:::\n:::\n\n\nSimply, `islice(iterable, stop)` will return a new iterator from the iterable (which can be another iterator), stop at `stop`. Paired with default `start` and `step` of 0 and 1 respectively, it means that `islice(it, 5)` will return an iterator containing the first 5 values of `it`. Now we realize that doing so will also exhaust these first 5 values of `it`, so the next call will call the next 5 values of `it`. Paired with a loop, it works exactly like `chunks()` defined above.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nit = iter(x)\nimg = list(iter(lambda: list(islice(it, 28)), []))\nplt.imshow(img);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=415 height=411}\n:::\n:::\n\n\nWork like a charm. A note: that empty list passed in after the lambda function is for the sentinel argument of `iter()`, instructing `iter()` to stop when it encounters the sentinel.\n\n# Tensor, Matrix\nOkay. Now we need a way to represent data that is *machine-readable*. And the machine only knows number. So we need to represent the data as numbers. We are actually already there, with our list of lists of data, each element representing the intensity of the corresponding pixel of the image. So why the need for a different representation?\n\nThe simplest answer is *efficiency*. Python list was designed to be a generic, dynamic data structure, hence memory and operation overhead. Machine learning requires doing linear algebra and calculus (read: matrix multiplication, one way or another, and differentiation) on the data, which is not inherently supported. There arises the need for something more specialized. And that thing was *tensor*.\n\nSimply put, tensor is the generalized matrix, expanding to as many dimensions as it needs to be. It first proved to be an excellent way of representing data in physics, where it was the core of [tensor analysis](https://en.wikipedia.org/wiki/Tensor_calculus) (yep, Albert Einstein used that). Tensor made it way into computer firstly with the development of [APL](https://en.wikipedia.org/wiki/APL_(programming_language)) (A Programming Language) by Kenneth E. Iverson, where the central data structure is  multidimensional array (yes, `numpy.ndarray` borrowed this). NumPy was most famous project to bring this to Python, paving the way for easy and flexible data analysis in Python (Pandas, sci-kitlearn, etc.). Deep Learning frameworks such as TensorFlow, JAX, or PyTorch all took from this design, with its own tensor the core data structure.\n\nAnthropic's Claude summarized the key differences as follow:\n\n> Prompt: What is the different between Python list and Torch.tensor?\n\n> Here are the main differences between Python list and Torch.tensor:\n\n> 1. Data type: Python lists can contain elements of different data types, while Tensors have a single data type. All elements of a Tensor must be floats, ints, etc.\n\n> 2. Operations: Python lists support basic operations like insert, append, pop, etc. Tensors support tensor operations like sum, mean, transpose, dot product, etc.\n\n> 3. Memory management: Python lists are managed by the Python memory manager while Tensors are managed by the PyTorch library. This makes Tensors more optimized for machine learning tasks.\n\n> 4. Shape: Python lists don't have a fixed shape, they can have any number of elements. Tensors have a defined shape, usually 2D or higher.\n\n> 5. Parallelism: Operations on Tensors can take advantage of GPUs and multiple CPU cores, while list operations are sequential.\n\n> 6. Gradient tracking: Only Tensors can track gradients to enable backpropagation in neural networks.\n\n> So in summary, Python lists are more general purpose data structures while Torch Tensors are specialized data structures optimized for machine learning and neural network tasks. Tensors provide the functionality needed for training deep learning models.\n\nFor our case, let's start with matrix first. We will implement a minimal object to allow the usual tensor indexing on Python list. This requires us to define the dunder method `__getitem__()`\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nclass Matrix:\n    def __init__(self, data) -> None:\n        self.data = data\n    def __getitem__(self, idxs):\n        return self.data[idxs[0]][idxs[1]]\n\nm = Matrix(img)\nm[20,15]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.98828125\n```\n:::\n:::\n\n\nOkay, cool. Now we are allowed to use `torch.tensor`. Let's quickly convert our data to tensor with `map` and reshape an image for visualization it. Notice how easily we could do so with PyTorch.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport torch\nfrom torch import tensor\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\n\nprint('Notice the shape:', x_train.shape)\nprint('Notice the type:', x_train.type())\n\nimgs = x_train.reshape((-1,28,28))\nplt.imshow(imgs[0]);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNotice the shape: torch.Size([50000, 784])\nNotice the type: torch.FloatTensor\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=415 height=411}\n:::\n:::\n\n\nThe mapping was successfully. The shape and data type all becomes PyTorch's built-in ones. The whole `x_train` dataset can be reshaped into 10000 $28 \\times 28 $ quickly with `.reshape()` method and -1 indexing. \n\nWe now have matrix. Let's do matrix multiplication.\n\n# Linear Algebra: Matrix multiplication\n\n## Brute-force\n\nTo get to multilayer perceptron, we need to able to do matrix multiplication. Let's start from the basic first, with pen and paper.\n\nHere's an example:\n\n![](matmul 1.png)\n\nLet's zoom in at one cell of the result matrix:\n\n![](matmul 2.png)\n\nTo summarize, for each element in the result matrix, we get it by summing the product of each element in the corresponding row of the left matrix and each element in the corresponding column of the right matrix. In codes, this is translated into three nested loops:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntorch.manual_seed(1)\n# Randomly initialize weights and biases as in a real layer\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n# Work with a batch of 5 first - a 3 nested loops should be slow.\nm1 = x_valid[:5]\nm2 = weights\n\nm1.shape,m2.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(torch.Size([5, 784]), torch.Size([784, 10]))\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Store the number of rows and columns of each matrix\nar,ac = m1.shape\nbr,bc = m2.shape\n\n# t1 is the placeholder result matrix\nt1 = torch.zeros(ar, bc)\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\nt1, t1.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n(tensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n         [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n         [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n         [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n         [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]]),\n torch.Size([5, 10]))\n```\n:::\n:::\n\n\nLet's package this into a function:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.02 s ± 10 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n## Speed-up: Dot product\n\nI want to get the objective clear: removing all of the three loops sequentially to speed things up. The first clue to do that is with dot product of two vectors. From the illustration, it is clear that each cell in result matrix is the result of the dot product between the left row vector and the right column vector. Unfortunately, we cannot use `torch.dot` yet. However, we can use *element-wise operation*, the trademark of tensor  \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.67 ms ± 319 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\nOkay, now `torch.dot` is free:\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = torch.dot(a[i,:], b[:,j])\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.27 ms ± 251 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n## Speed-up: Broadcasting\n\nThe next clue is another trademark of tensor: **broadcasting**, which allows tensors of different shapes to be multiplied together. This is the trademark of tensors, so make some time to familiarize yourself with the rules from [NumPy documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n\nFor demonstration, it is better to refer to the [original notebook](https://nbviewer.org/github/fastai/course22p2/blob/master/nbs/). I just want to mention two things. Firstly, `.unsqueeze()` and `None` indexing. Simply put, we can create a *unit dimension* in an array by using `.unsqueeze()` or passing in the special keyword `None` inside the indexing brackets.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nc = tensor([10.,20,30])\n\nprint('The first dimension:', c.unsqueeze(0), c[None, :])\nprint('The second dimension:', c.unsqueeze(1), c[:, None])\nprint('We can skip trailing \":\"', c[None])\nprint('We can also pass in \"...\"', c[..., None])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe first dimension: tensor([[10., 20., 30.]]) tensor([[10., 20., 30.]])\nThe second dimension: tensor([[10.],\n        [20.],\n        [30.]]) tensor([[10.],\n        [20.],\n        [30.]])\nWe can skip trailing \":\" tensor([[10., 20., 30.]])\nWe can also pass in \"...\" tensor([[10.],\n        [20.],\n        [30.]])\n```\n:::\n:::\n\n\nSecondly, that broadcasting compare array dimensions from *right to left*. This can lead to behavior such as this:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nc[None,:] * c[:,None]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n```\n:::\n:::\n\n\n![](matmul 3.png)\n\nAnyway, with broadcasting, we can now calculate the result matrix one row at a time and skip another loop:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        c[i]   = (a[i,:,None] * b).sum(dim=0)\n    return c\n\n%timeit -n 5 matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n357 µs ± 141 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\nAt this point, we basically arrive at matrix multiplication. However, let's up the amp a bit and (re)introduce Einstein.\n\n## Einstein summation\n\n> [Einstein summation](https://ajcr.net/Basic-guide-to-einsum/) (`einsum`) is a compact representation for combining products and sums in a general way. The key rules are:\n>\n> - Repeating letters between input arrays means that values along those axes will be multiplied together.\n> \n> - Omitting a letter from the output means that values along that axis will be summed.\n\nExample:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nm1.shape, m2.shape\n\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntorch.Size([5, 784, 10])\n```\n:::\n:::\n\n\nTo use `torch.einsum`, we need to pass in a string telling the operation we want to achieve. The string above means \"multiplying each column of `m1` by `m2`\". Notice that we can sum the result matrix along the first dimension to get the result matrix.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nmr.sum(1)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\nThis is equivalent to the notation of matrix multiplication\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ntorch.einsum('ik,kj->ij', m1, m2)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\nSo we can replace everything with a `torch.einsum` call:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n%timeit -n 5 _=matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 4.03 times longer than the fastest. This could mean that an intermediate result is being cached.\n51.5 µs ± 32.5 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\nHaving come to this point, let's introduce `torch` predefined operations:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n%timeit -n 5 _=torch.matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 11.13 times longer than the fastest. This could mean that an intermediate result is being cached.\n16.5 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n%timeit -n 5 _=m1@m2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 17.03 times longer than the fastest. This could mean that an intermediate result is being cached.\n31.6 µs ± 42.8 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n# Calculus: Differentiation and Chain Rule\nOkay, great. We have built the linear algebra needed for deep learning, specifically, for the forward pass. For the backward pass, we need to do some calculus - calculating the differentiation.\n\nGreat! But *why differentiation in the first place*?\n\nAt this point, it is best if we agree on a general answer to *What's exactly is a (machine learning) model?*\n\nSimply, we have a black box, or a \"magic API\" as Jeremy called it, where we pass inputs in and receive outputs out close to the way we want. But more often than not, we will not know the perfect parameters for the model, so we will need to start with random parameters and update them along. But what is the corect direction to do so? We will need a *metric* to measure progress. Hence, the loss function is introduced. But well, how do we construct such a function?\n\nThis is the time I introduce the topic of *Optimization problem*, where the goal is to find the maximum or minimum value of a variable. A technique, or even *the* technique that we can use is [Newton method](https://en.wikipedia.org/wiki/Newton%27s_method). It was first used to solve some equation $f(x)=0$. Let's say $f(x)$ is complex so it is tedious to solve direcly. We can guess the initial and then iteratively calculate a better one with the formula:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n\nThe intuition is that for a(n infinitesimally) small range around any value, the function can be approximated to be linear (within some error). For each iteration of Newton's method, we approximate the function as a linear one. Of course, for guesses far from a true root, it wil be incorrect. However, following the gradient, we will get to the next guess that is closer to the root than before. If we keep repeating, we will eventually get to the real root, or a very close approximation. And there it is!\n\n![](NewtonIteration_Ani.gif)\n\nFor optimization problem, it is not $f(x)=0$, but $f'(x)=0$. We can use the same technique, but modify the equation to.\n$$x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}$$\n\nGoing to multidimensional matrix, the equation becomes:\n$$x_{n+1} = x_n - H^{-1} \\nabla f(x_n)$$\n\nwhere $H$ is the Hessian matrix, the second derivative of $f(x)$. This is the core of Newton's method. However, it is not used in practice because of the computational cost of *calculating the Hessian matrix* and then *inverse it* (trust me, it's not a pleasant experience, and I tried with just two dimensions). Instead, we use a parameter called *learning rate* in gradient descent, which is essentially a small constant to replace the Hessian. The equation becomes:\n$$x_{n+1} = x_n - \\alpha \\nabla f(x_n)$$\n\nYes, we are using a constant to estimate the Hessian. It is not as accurate, but it is much faster and more straightforward to implement. And with the introduction of momentums, regularizations, learning rate scheduler, etc., we can achieve great result [fast](https://arxiv.org/abs/1708.07120) without the need for the Hessian.\n\nEnough with the history and theory.\n\n## Forward pass\n\nLet's start with a simple model: 2 linear layers, 1 ReLU activation. Ignore the softmax for now.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nbatch_size, feature = x_train.shape\nclasses = y_train.max() + 1\nnum_hidden = 50 \n```\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nw1 = torch.randn(feature,num_hidden)\nb1 = torch.zeros(num_hidden)\nw2 = torch.randn(num_hidden,1)\nb2 = torch.zeros(1)\n\ndef lin(x, w, b):\n    return x@w + b\n\ndef relu(x):\n    return x.clamp_min(0.)\n\ndef model(xb):\n    l1 = lin(xb, w1, b1)\n    l2 = relu(l1)\n    return lin(l2, w2, b2)\n\nyhat_train = model(x_train)\nyhat_train.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\ntorch.Size([50000, 1])\n```\n:::\n:::\n\n\nPerfect! Now we need something to measure the performance of the model. Ideally, this number should involve the final output of the model and the true label to use in the backward pass later. The most straightforward is MSE loss.\n\nThe formula for this case is:\n$$\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n\nwhere $n$ is the number of samples, $y_i$ is the true label, and $\\hat{y_i}$ is the predicted label.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n(yhat_train-y_train).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\ntorch.Size([50000, 50000])\n```\n:::\n:::\n\n\nHuh? The shape received is incorrect. We are expecting `torch.Size([50000])` but receive `torch.Size([50000, 50000])` instead. The likeliest cause is a difference in dimension. Let's check the shape of `yhat_train` and `y_train`:\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nyhat_train.shape, y_train.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n(torch.Size([50000, 1]), torch.Size([50000]))\n```\n:::\n:::\n\n\nAha! There is a trailing dimension in `yhat_train`. We can fix this by using `squeeze`:\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nyhat_train[:,0].shape, yhat_train.squeeze().shape\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n(torch.Size([50000]), torch.Size([50000]))\n```\n:::\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n(yhat_train.squeeze()-y_train).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\ntorch.Size([50000])\n```\n:::\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ndef mse(yhat, y):\n    return (yhat.squeeze(-1) - y).pow(2).mean()\nmse(yhat_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\ntensor(922.41)\n```\n:::\n:::\n\n\n## Backward pass\n\nTo do backward pass, we need to calculate gradient. This example is easy - I can do it with pen and paper and code the formulae in. However, to scale up to bigger network, we need a more general way called *computational graph*.\n\nA framework such as PyTorch will have mechanism (`autograd`) behind the scene to keep track of the computational graph. Afterwards, it will calculate the gradient of the parameters to the loss function. The gradient is not calculated at once, but *sequentially*  in the reverse order of the forward pass, making use of the chain rule to reduce operations for each parameter. This is called *backpropagation*. To emphasize, yes, the coolest term in the history of deep learning is just *using chain rule to calculate gradient sequentially in the reverse order*.\n\n> To find out more about backpropagation, the most complete resource is a lecture from a university such as Michigan or Stanford (e.g., [video](https://youtu.be/d14TUNcbn1k), [notes](https://cs231n.github.io/optimization-2/)). You may also want to check the PyTorch documentation for [autograd](https://pytorch.org/docs/stable/notes/autograd.html).\n\nAnyway, we are yet to have an autograd framework, so for this example, let's use pen and paper. We will calculate the gradient of the loss function with respect to each parameter.\n\n**Note:** The whole phrase is \"gradient of the loss function with respect to a parameter $x$\", which is usually referred to just as \"gradient of $x$\". This seems to be an implicit rule in the field, so I will follow.\n\n### Gradient of MSE loss\nFrom\n$$\nJ = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2,\n$$\n$$\n\\frac{\\partial J}{\\partial \\hat{y}} = \\frac{2}{n} (y - \\hat{y})\n$$\nwith $y$ and $\\hat{y}$ matrices.\n\nThis will give the first gradient for the backward pass. We will use this to calculate the gradient of the next layer. Recall that $\\hat{y_i}$ is also the activation of the last linear layer (say, $n$): $\\hat{y} = a^j = w^j \\times a^{j-1} + w^j$. Generally, *the input to the next layer is the activation of the current layer*. The formula we will derive shortly shows that we will need the output of the layer to calculate the gradient of the input to the layer (which happens to be the activation of the previous layer, hence required to calculate the graidnet of the input to the previous layer (which happens to be...)...).\n\n### Gradient of linear layer\nFrom\n$$\na^j = w^j \\times a^{j-1} + b^j, \\\\\n$$\nThe gradients are:\n$$\n\\frac{\\partial J}{\\partial w^j} = \\frac{\\partial J}{\\partial a^j} \\times \\frac{\\partial a^j}{\\partial w^j} = \\frac{\\partial J}{\\partial a^j} \\times a^{j-1},\n$$\n$$\n\\frac{\\partial J}{\\partial a^{j-1}} = \\frac{\\partial J}{\\partial a^j} \\times \\frac{\\partial a^j}{\\partial a^{j-1}} = \\frac{\\partial J}{\\partial a^j} \\times w^{j},\n$$\n$$\n\\frac{\\partial J}{\\partial b^{j}} = \\frac{\\partial J}{\\partial a^j} \\times \\frac{\\partial a^j}{\\partial b^{j}} = \\frac{\\partial J}{\\partial a^j}\n$$\nIn Python, we will store the gradient as an attribute of the matrix itself. This is similar to PyTorch's behaviors. The attribute `grad` is taken, so let's use `g` instead.\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ndef lin_grad(inp, out, w, b):\n    # .g must have the same shape as the original matrix\n    # inp has shape (i, j), w has shape (j, k), so out has shape (i, k)\n    # to get back to (i, j) from (i, k), we need shape (k, j), so we transpose w\n    # This analysis based on shape is not correct as I obviously do not care\n    # about actual values here, but is often enought to get the correct answer \n    inp.g = out.g @ w.t()\n    # For w.g, it is getting (j, k) from (i, j) and (i, k)\n    # Besides einsum, from broadcasting rule, we need to add a unit axis at the\n    # end of inp and at axis 1 (zero-indexing) of out, multiply to get (i, j, k),\n    # and sum along the first axis\n    # w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    w.g = torch.einsum('ij,ik->jk', inp, out.g)\n    # For b.g, the shape is just (k) because of broadcasting, so sum out.g along\n    # the first axis.\n    b.g = out.g.sum(0)\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = lin(inp, w1, b1)\n    l2 = relu(l1)\n    out = lin(l2, w2, b2)\n    diff = out[:,0]-targ\n    loss = diff.pow(2).mean()\n    \n    # backward pass:\n    out.g = 2.*diff[:,None] / inp.shape[0]\n    lin_grad(l2, out, w2, b2)\n    l1.g = (l1>0).float() * l2.g\n    lin_grad(inp, l1, w1, b1)\n\nforward_and_backward(x_train, y_train)\n```\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# Testing - from Jeremy\nfrom fastcore.test import test_close\n\ndef get_grad(x): return x.g.clone()\nchks = w1,w2,b1,b2,x_train\ngrads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))\n\ndef mkgrad(x): return x.clone().requires_grad_(True)\nptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))\n\ndef forward(inp, targ):\n    l1 = lin(inp, w12, b12)\n    l2 = relu(l1)\n    out = lin(l2, w22, b22)\n    return mse(out, targ)\n\nloss = forward(xt2, y_train)\nloss.backward()\n\nfor a,b in zip(grads, ptgrads): test_close(a, b.grad, eps=0.01)\n```\n:::\n\n\nNo expeption was raised, which means that our codes worked correctly! We successfully created an MLP!\n\n\n\n*The formulae for the optimization problems were referenced from Chapra, & Canale, R. P. (2015). Numerical methods for engineers (Seventh edition.). McGraw-Hill Education*\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}