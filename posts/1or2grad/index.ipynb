{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: A thing or two about gradient\n",
        "subtitle: From an interesting internship post\n",
        "author: Pham Nguyen Hung\n",
        "draft: false\n",
        "date: '2022-12-18'\n",
        "categories:\n",
        "  - basic\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    code-fold: true\n",
        "---"
      ],
      "id": "337840ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While scrolling through LinkedIn, I found this curious requirement for the machine learning engineer intern position:\n",
        "![](Screenshot 2022-12-18 at 12-07-18 (7) data scientist intern Jobs in Singapore LinkedIn.png)\n",
        "\n",
        "To be honest, I have no idea what is \"gradient taping\", though I vaguely remember that TensorFlow has a [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape) class. And yes, it does. So what exactly is a gradient tape, and why do we need it?\n",
        "\n",
        "# Machine learning and gradient:\n",
        "In machine learning, you have a *model*, which takes in *inputs* a.k.a *data* and gives *outputs* a.k.a *predictions*. A *loss function*, whose value depends on the *parameters* of your model, measures the difference between the predictions and the actual results. \"Training a model\" describes the process where you run an algorithm to change the parameters such as the loss function reaches a minimum. The algorithm you run is typically *gradient descent*, which involves calculating the *gradient* of the loss function with respect to (w.r.t) each parameter a.k.a *how the loss function would change* and *in what direction* if you increases the parameter. The takeaway points are: 1) calculating gradient is very important and 2) if the paragraph above does not make sense, you should go back and check out a beginner's machine learning course.\n",
        "\n",
        "Given you have taken that beginner's course, you would remember that the formula to calculate the gradient for linear or logistic regression is fairly straightforward and you could even use Numpy. However, for deep neural networks, with the current extreme case of any GPT models, the formula is very forbidding. We need a more general approach, and that was when we received *backpropagation*.\n",
        "![Try finding the gradients for all 6 billion parameters of GPT-J](GPT-vs-GPT-J-uai-1440x933.webp)\n",
        "\n",
        "# Backpropagation, or Autodifferentiation, or GradientTape, or Autograd, or...\n",
        "You probably have heard/will need to Google the *chain rule* in calculus. In lay terms, it just means that \"you can treat gradient just like fractions\". For example, in general case, if we have $z$ whose value partly depends on $y$, and $y$ whose value partly depends on $x$, then the gradient of z w.r.t to x is\n",
        "$$\\frac{\\delta z}{\\delta x} = \\frac{\\delta z}{\\delta y}*\\frac{\\delta y}{\\delta x}$$\n",
        "just like with normal fractions.\n",
        "\n",
        "This formula holds for as many relationships as it could get, not just 2 like here. In a neural network, we have parameters in layers, and the output value of each layer depends partly on the output value of the last layer. This means that if we can keep track of all the operations in the forward calculation (passing the data through the layers, one at a time), we can calculate the gradient for each parameter, also one layer at a time. That is the essential of backpropagation. And this is when tf.GradientTape comes in: this \"tape\" keeps track of all the operations to a particular variable, and helps you calculate the gradient value of any variable w.r.t to that variable.\n",
        "> For a detailed explanation of backpropagation, check out the [notes](https://cs231n.github.io/optimization-2/) and [lecture](https://www.youtube.com/watch?v=59Hbtz7XgjM) from CS231n, Stanford.\n",
        "\n",
        "Here is the example from [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/GradientTape):"
      ],
      "id": "1373275a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as g:\n",
        "    g.watch(x)\n",
        "    y = x * x\n",
        "dy_dx = g.gradient(y, x)\n",
        "print(dy_dx)"
      ],
      "id": "6b54ea0b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}