---
title: "A thing or two about gradient"
subtitle: "From an interesting internship post"
author: "Pham Nguyen Hung"
draft: false
date: "2022-12-18"
categories: [code]
format:
    html:
        toc: true
        code-fold: false
        highlight-style: pygments
jupyter: python3 
---
While scrolling through LinkedIn, I found this curious requirement for the machine learning engineer intern position:
![](Screenshot 2022-12-18 at 12-07-18 (7) data scientist intern Jobs in Singapore LinkedIn.png)

To be honest, I have no idea what is "gradient taping", though I vaguely remember that TensorFlow has a [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape) class. And yes, it does. So what exactly is a gradient tape, and why do we need it?

# Machine learning and gradient:
In machine learning, you have a *model*, which takes in *inputs* a.k.a *data* and gives *outputs* a.k.a *predictions*. A *loss function*, whose value depends on the *parameters* of your model, measures the difference between the predictions and the actual results. "Training a model" describes the process where you run an algorithm to change the parameters such as the loss function reaches a minimum. The algorithm you run is typically *gradient descent*, which involves calculating the *gradient* of the loss function with respect to (w.r.t) each parameter a.k.a *how the loss function would change* and *in what direction* if you increases the parameter. The takeaway points are: 1) calculating gradient is very important and 2) if the paragraph above does not make sense, you should go back and check out a beginner's machine learning course.

Given you have taken that beginner's course, you would remember that the formula to calculate the gradient for linear or logistic regression is fairly straightforward and you could even use Numpy. However, for deep neural networks, with the current extreme case of any GPT models, the formula is very forbidding. We need a more general approach, and that was when we received *backpropagation*.
![Try finding the gradients for all 6 billion parameters of GPT-J](GPT-vs-GPT-J-uai-1440x933.webp)

# Backpropagation, or Autodifferentiation, or GradientTape, or Autograd, or...
You probably have heard/will need to Google the *chain rule* in calculus. In lay terms, it just means that "you can treat gradient just like fractions". For example, in general case, if we have $z$ whose value partly depends on $y$, and $y$ whose value partly depends on $x$, then the gradient of z w.r.t to x is
$$\frac{\delta z}{\delta x} = \frac{\delta z}{\delta y}*\frac{\delta y}{\delta x}$$
just like with normal fractions.

This formula holds for as many relationships as it could get, not just 2 like here. In a neural network, we have parameters in layers, and the output value of each layer depends partly on the output value of the last layer. This means that if we can keep track of all the operations in the forward calculation (passing the data through the layers, one at a time), we can calculate the gradient for each parameter, also one layer at a time. That is the essential of backpropagation. And this is when tf.GradientTape comes in: this "tape" keeps track of all the operations to a particular variable, and helps you calculate the gradient value of any variable w.r.t to that variable.
> For a detailed explanation of backpropagation, check out the [notes](https://cs231n.github.io/optimization-2/) and [lecture](https://www.youtube.com/watch?v=59Hbtz7XgjM) from CS231n, Stanford.

Here is the example from [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/GradientTape):
```{python}
import tensorflow as tf
x = tf.constant(3.0)
with tf.GradientTape() as g:
    g.watch(x)
    y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
```
So that's it for gradient taping - it's a way to keep track of the operations on a variable, which is used to calculate the gradient in backpropagation time. Most major framework, such as TensorFlow, have a way to implement this, so you don't need to do that from scratch (if you are interested in doing so, check out this [tutorial](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)), you just need to know how to use them. For example, this is a code block in PyTorch, another popular deep learning framework, that does the exact same thing:
```{python}
import torch
x = torch.tensor([3.], requires_grad=True)
y = x*x
dy_dx = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y))
print(dy_dx)
```

We could also store the gradient value in the `.grad` attribute of variable `x`:
```{python}
import torch
x = torch.tensor([3.], requires_grad=True)
y = x*x
y.backward()
print(x.grad)
```
The second way is more commonly seen in model training.

## Image Credits:
From top to bottom:

1. This [internship post](https://www.linkedin.com/jobs/search/?currentJobId=3389948741&f_C=18706840&f_E=1&geoId=102454443&keywords=machine%20learning%20engineer&location=Singapore&refresh=true&position=1&pageNum=0) by Datature.
2. This [blog post](https://www.cerebras.net/blog/cerebras-makes-it-easy-to-harness-the-predictive-power-of-gpt-j) on Cerebras