---
title: "Deep Learning from the ground up - From tensor to multi-layer perceptron (1)"
subtitle: "Lesson 9 - 14 of fast.ai course part 2"
author: "Pham Nguyen Hung"
draft: true
date: last-modified
categories: [code, From scratch]
format:
    html:
        toc: true
        code-fold: false
jupyter:
    kernelspec:
        name: "cb0494"
        language: "python"
        display_name: "cb0494"
---
In the [first post](https://hangenyuu.github.io/h-notes/posts/fastai2-p1/), I finished backpropagation and the simple maths behind it. Now let's talk about the design of PyTorch...

> Actually I am not including it in my post. You can read about it [here](https://pytorch.org/docs/stable/community/design.html). To feel the need for PyTorch or Keras (and then TensorFlow 2.x), check out this [script](https://github.com/jsyoon0823/TimeGAN/blob/master/timegan.py). **Warning:** the author defined four models as *functions* and *updated them together* in TensorFlow 1.x.

... which leads to our need to refactor the layers into *objects*, instead of *functions*. On defining them as class, we make them reusable and reduce the amount of codes to be written.

# Setup
```{python}
# Follow the previous post
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from pathlib import Path
from torch import tensor,nn
import torch.nn.functional as F
from fastcore.test import test_close

torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)
torch.manual_seed(1)
mpl.rcParams['image.cmap'] = 'gray'

path_data = Path('data')
path_gz = path_data/'mnist.pkl.gz'
with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])
```
```{python}
batch_size, feature = x_train.shape
classes = y_train.max() + 1
num_hidden = 50 
```
