---
title: "Conditional & Controllable GAN"
subtitle: "My notes on taking the specialization by deeplearning.ai series"
author: "Pham Nguyen Hung"
draft: false
date: "2023-02-12"
categories: [code, GAN]
format:
    html:
        toc: true
        code-fold: true
jupyter: python3 
---
Apologize for being late: it has been nearly 3 weeks already since the latest post. But I am back for the last week of content.

When I started writing, it was near the time to bed, and without a doubt, I am hungry. So let's deal with cookies this time.

Up until now, our GAN has managed to do some interesting stuff ("writing" Kanji characters, or numbers, if you used the traditional MNIST dataset). However, one thing you must notice is that we have *no control over what the Generator will give us*. It can be a "na", it can be a "tsu", it can be a "ki" - no control whatsoever. In our cookie analogy, or cookanalogy, our current is like a goodwilled roomie who bakes for us every day, but each day we will receive a random cookie type.

![*You know it is cookie, but you have no idea what type is it*](cookie 1.png)

Now, if you love randomness and can tolerate the taste as well as the sugar, fine. But we usually want our model to be controllable, that is, we get to decide, or at least influence, what will be included in the output.

![*It's much nicer to control that you have matcha on Monday, chocochip on Tuesday, and so on.*](cookie 2.png)

With the objective set, let's explore way to implement conditional GANs a.k.a way to make sure we have the correct cookie each day.

# Limiting to just one category:

This is a no-brainer solution. To prevent random category generation (and mode collapse as well), who don't just feed in variational data of a single class only? In our cookanalogy, it is like always mixing matcha powder into the dough to make the cookies. Being an inexperienced chef and afraid of being cause, we sneak matcha powder in inconsistently between the days, but we are sure that every day we will get matcha cookies.

![*A matcha cookie junkie's dream. (Anyone else besides me?)*](cookie 3.png)

Obviously this solution is for when you want to generate examples of one class only. One example would be augmenting data for brain EMR of a certain disease at a certain region. The other ~~trolled~~ example is [GANyu](https://www.kaggle.com/datasets/andy8744/ganyu-genshin-impact-anime-faces-gan-training), a dataset and models fine-tuned on it for the task of generating faces of the Genshin Impact character Ganyu (Check out the [GA(N)Rem](https://www.kaggle.com/datasets/andy8744/rezero-rem-anime-faces-for-gan-training) as well).

> I don't know what is the thing for animes and GANs, but the moment I discovered GANs, I instantly thought of generating anime girls' faces. Is is the same phenomenon as researchers in the 90's instantly thought of classifying cat from everything else the moment they got a decent classifier... - A certain unfamous author on the web

# Conditional Generation:

One thing that we must make clear for our GAN: we cannot generate examples of something that is not in the training data. So first, we must ensure the thing that we want (matcha cookie) is in the training data (ingredients) fed to our two networks.

Next, we must be able to call on the Generator to produce what we want. This means that the data needs labelling. Simply put, the Discriminator needs to learn what a "matcha cookie" looks like before it can give any meaningful feedback - constrast between an original image and a generated one - to the Generator; the Generator also needs to know that the class it is generating to update the parameters accordingly. After training finishes, we can tell the Generator which class we want it to produce, and, voil√†! A solution to achieve this was proposed in the [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784.pdf) by passing some extra information $y$ (the easiest is class labels, possibly as one-hot vector) to both Generator and Discriminator.

![*An input vector to Generator now is made up of two components: noise inputs (to ensure that each generation will be unique) & one-hot class vector (to ensure that the generate example will be of the class we want)*](cgan 1.png)

![*Similarly, an input to the Discriminator now is an image together with the an one-hot class vector. For an example to be regarded as real, not only it needs to look realistic (reflected by a low binary corss-entropy or Wasserstein loss) but it also needs to look like examples from the same class. Here, an original matcha cookie definitely looks like a matcha cookie.*](cgan 2.png)

![*Here is the earlier generated matcha cookie. Let's say that our Discriminator is a bit better than the Generator. It means that it will detect this looks rather like a matcha cookie, but not too alike.*](cgan 3.png)

![*Now suppose that the required class is black chocochip, but our Generator gives a matcha cookie. The Discriminator will recognize in this case and gives a low chance that the example is real.*](cgan 4.png)

The question now is how do we go on implementing this? From the descriptions, it seems that we need to do 2 things: 1) figure a way to pass the additional information into our two models and 2) update the loss function. However, we just need to do 1), as the same loss function (binary cross-entropy) can be used, we just need to make sure that the class of the examples are included in the output as well. For Generator, you just need to concatenate it with the noise vector above. For Discriminator, it is a bit trickier. We feed the images in by passing values of three channels, so the simplest way will be to create n channels more for n classes. This way works for dataset such as the good ol' MNIST, whether we flatten out images before concatentating or we keep the same matrix and just call `torch.cat()` (which will create 10 more channels, each of size 28*28, with one of them full of 1 and the rest full of 0). For larger images or ones we cannot/do not want to flatten, this simple approach will create a huge memory issue. We will want to pass class information in a different, such as via a different head of input layer. Here are the code snippets.

```python
import torch
import torch.nn.functional as F

def get_one_hot_labels(labels: torch.Tensor, n_classes: int):
    '''
    Function for creating one-hot vectors for the data.

    :param labels: a vector containing the labels of all examples in a batch.
                   Get from each DataLoader. Have shape (n_samples, 1)
    :param n_classes: an integer for number of classes in the data.
                      Get from the dataset created
    :return: the one-hot vector for a batch of data
    '''
    return F.one_hot(labels,n_classes)

def combine_vectors(x, y):
    '''
    Generic function for combining two 2-D maxtrices with the same 0-shape
    In our case, they will be (n_samples, x_1) and (n_samples, y_1).

    :param x: the first matrix, shape (n_samples, x_1)
    :param y: the second matrix, shape (n_samples, y_1)
    :return: the concatenated matrix of shape (n_samples, x_1 + y_1)
    '''
    # To ensure unity of data types, we want the return matrix to have float
    # type.
    combined = torch.cat((x.float(),y.float()), 1)
    return combined
```

```python
# Code will not run if just copy-paste
# Pre-training
# Just the basic part.
kmnist_shape = (1, 28, 28)
n_classes = 10

device = 'cuda' if torch.cuda.available() else 'cpu'
criterion = nn.BCEWithLogitsLoss()
z_dim = 64 # Size of the noise vector
gen = Generator(input_dim=generator_input_dim).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
disc = Discriminator(im_chan=discriminator_im_chan).to(device)
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)
epochs = 200

def weights_init(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
    if isinstance(m, nn.BatchNorm2d):
        torch.nn.init.normal_(m.weight, 0.0, 0.02)
        torch.nn.init.constant_(m.bias, 0)
gen = gen.apply(weights_init)
disc = disc.apply(weights_init)

# Training loop
cur_step = 0
generator_losses = []
discriminator_losses = []

for epoch in range(epochs):
    for real, labels in tqdm(dataloader):
        n_samples = len(real)
        real = real.to(device)

        # Get image one-hot labels for this batch
        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)
        # Remember that the DataLoader is in size (n_samples, 1, 28, 28) while the one hot label matrix 
        # has size (n_samples, 1). We need to extend 2 more dimensions if we want to concatenate the two.
        image_one_hot_labels = one_hot_labels[:, :, None, None]
        # Now the one-hot labels matrix has size (n_samples, 1, 1, 1). We need to turn it into
        # (n_samples, 1, 28, 28) to pass into the input layer.
        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, kmnist_shape[1], kmnist_shape[2])

        ### Update discriminator
        # Zero out the discriminator gradients
        disc_opt.zero_grad()

        # Get noise corresponding to the current batch_size 
        fake_noise = get_noise(n_samples, z_dim, device=device)
        
        # Combine the label and the noise and generate fake examples
        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)
        fake = gen(noise_and_labels)

        # Get Discriminator's predictiopn on the real and the fake examples
        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)
        real_image_and_labels = combine_vectors(real, image_one_hot_labels)
        disc_fake_pred = disc(fake_image_and_labels.detach()) # do not update the Generator
        disc_real_pred = disc(real_image_and_labels)

        # Calculate loss
        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))
        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))
        disc_loss = (disc_fake_loss + disc_real_loss) / 2

        # Backpropagation
        disc_loss.backward(retain_graph=True)

        # Update the parameters
        disc_opt.step()

        # Keep track of the average discriminator loss for visualization
        discriminator_losses += [disc_loss.item()]

        ### Update generator
        # Zero out the generator gradients
        gen_opt.zero_grad()

        # Regenerate the fake examples with gradients to update
        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)
        disc_fake_pred = disc(fake_image_and_labels)

        # Calculate loss
        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))

        # Backpropgation
        gen_loss.backward()

        # Update the parameters
        gen_opt.step()

        # Keep track of the generator losses for visualization
        generator_losses += [gen_loss.item()]
```

# Controllable Generation:
Up until now, the noise vector fed into GANs is just "noise" - sounds meaningless. However, the numbers in the noise vector do mean something. You can think that each number represent one feature that is recognized by the Generator. The combinations of all these features form a *latent space* - a space containing a simpler but hidden (to humans) information for generated examples. This is best understood with the example of amino acid.

The basic of every bodily function is protein, which is a chain of amino acids (you don't need to know what they are). Each amino acid in the chain is encoded as a sequence of 3 nucleotides, which have 4 in total (there are $4^{3}=64$ total combinations, but several combinations encoding the same amino acid, and there are special ones called *ending combinations* that signify the end but do not encode).

All the 64 combinations can thought of as the *latent space* of the amino acid. It's like we have a well-trained Generator on 22 classes that output the exact amino acid or ending signal we want by passing into it certain 3 nucleotides. This is a latent space because the information has been simplified, but it is not quite latent because we now know the exact *embedding* of the information.

![*["The genetic code"](https://openstax.org/books/biology/pages/15-1-the-genetic-code#fig-ch15_01_04) by OpenStax College, Biology*](genetic code.png)

In our KMNIST example, each image can be represented as a 28 by 28 matrix where each position stores the intensity of the pixel. It can be visualized as below.

![*A certain Japanese letter*](KMNIST eg.png)

In the noise vector above, we try to compress this information down to a vector of 64 numbers (recalling the `z_dim` above), hoping that this is sufficient to store the information to construct all 10 classes of handwritten kanji characters. But let's fall back to our cookies for a more easily visualizable example.

![*Before, we have been generating random numbers in the noise vector. Let's say that we have been able to decode that the second number in our noise vector encodes information for color of the cookie, with 2.1 signifies the green matcha color we wants.*](cgan 5.png)

![*Now, after training, we now know that 1.4 corresponds to the brown color of chochip cookies. We can now pass the number to get a brown cookie.*](cgan 6.png)

Let's compare Conditional and Controllable Generation. For Conditional Generation, we want *a specific class*, while for Controllable Generation, we want *a specific feature* for the output. Secondly, if data for Conditional Generation must contain class information, data for Controllable Generation must contain the feature(s) that we want (we cannot get matcha cookie from if we don't have matcha powder); however, the data now do not need labeling. In both ways, we influence the input by the noise vector $z$, but if we concatenate it with class information before, now we try to directly modify the noise vector.

In practice, we actually do not learn the exact embedding (if the word "embedding" still seems confusing, it just means a look-up table, or a dictionary, where there is a code for )