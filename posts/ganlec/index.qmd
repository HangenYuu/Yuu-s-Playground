---
title: "A Primer on Generative Adversarial Networks (GANs)"
subtitle: "My notes on taking the specialization by deeplearning.ai"
author: "Pham Nguyen Hung"
draft: true
date: "2023-01-20"
categories: [code, GAN]
format:
    html:
        toc: true
        code-fold: true
jupyter: python3 
---
If you have studied deep learning before, you will notice that we will encounter classification many times. To be honest, it is fun in a way, having your own model to classify anime characters. Alas, it is a bit dry to me. Intelligence, for me, is creativity, the ability to create something *new*. I want a model that can create, especially work of art. That led me right to GANs, not so much a model but an elegant way of thinking.

# A brief history of GANs
*For a fuller account, check out the [MIT Technology Review article](https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/).*

Back in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) to [VGG](https://ar5iv.labs.arxiv.org/html/1409.1556). (Not to mention [ResNet](https://ar5iv.labs.arxiv.org/html/1512.03385) in 2015, an architecture with so interesting an idea that I had to [make a project](https://github.com/HangenYuu/vision_learner/tree/main/ARCHITECTURE/CNN/Paper) for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. "Complex statistical analysis of the elements that make up a photograph" was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow's seminal paper, [Generative Adversarial Nets](https://ar5iv.labs.arxiv.org/html/1406.2661).

![My dog also has a face in his buttocks](Screenshot 2023-01-20 at 19-53-43 Generative Dog Images Kaggle.png)
*The image was not from the era, but was representative of what you got from the model at that time (and still now with GANs). [Source](https://www.kaggle.com/c/generative-dog-images/discussion/97753)*

Now I wanted to make two quick detours before going into the inside of GANs:

1. At its core sense, a *function* is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is *trying to update its parameters such that the model will approximate the optimal function as closely as possible*. Neural networks are powerful because 

*All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the [tool](https://ar5iv.labs.arxiv.org/). To change to the abstract page, follow this example:* `https://ar5iv.labs.arxiv.org/html/1409.1556` &rarr; `https://arxiv.org/abs/1409.1556`.