<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>H's notes</title>
<link>https://hangenyuu.github.io/h_notes/index.html</link>
<atom:link href="https://hangenyuu.github.io/h_notes/index.xml" rel="self" type="application/rss+xml"/>
<description>H's Notes on Deep Learning</description>
<generator>quarto-1.1.251</generator>
<lastBuildDate>Sat, 21 Jan 2023 16:00:00 GMT</lastBuildDate>
<item>
  <title>Building a simple GAN</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan_p2/index.html</link>
  <description><![CDATA[ 




<p>In the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, <a href="https://github.com/rois-codh/kmnist">KMNIST</a>.</p>
<section id="concept---training-gans" class="level1">
<h1>Concept - Training GANs:</h1>
<p>Like any training process, the process GANs can be decomposed into feed-forward and backpropagation, though with distinct features from, say, training a classifier. The parameters updated after backpropagation can also be divided into two steps, for Discriminator and Generator. These are summed up in the images below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">First, in the feed-forward, we pass some random noise (denoted by <img src="https://latex.codecogs.com/png.latex?%5Cxi">) into the Generator, which outputs some fake examples (denoted by <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">). The fake examples are then merged with a dataset of real examples (just <img src="https://latex.codecogs.com/png.latex?X">) and feed separately into the Discriminator, and we receive the outputs as a vector containing the possibilities of each example being real (between 0 and 1).</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Second, for training the Discriminator. We will calculate the loss as binary cross-entropy (BCE) loss for two components: how closely to 0 the Discriminator predicted the fake examples, and how closely to 1 the Discriminator predicted the real examples. Here, we need to detach the Generator from the gradient flow as we want to update the Discriminator’s parameters only</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Third, for training the Generator. From the predictions for the fake examples, we calculate the BCE loss as how closely the Discriminator predicted them to 1. We then update the Generator’s parameters.</figcaption><p></p>
</figure>
</div>
<p>Hopefully the ideas are not too complicated. If they are so, hopefully things will make more sense when we look at the codes.</p>
</section>
<section id="hands-on---creating-gans" class="level1">
<h1>Hands-on - Creating GANs:</h1>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">The dataset:</h2>
<p>First rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/kmnist%20examples.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The 10 classes of Kuzushiji-MNIST, with the first column showing each character’s modern hiragana counterpart. <a href="https://github.com/rois-codh/kmnist#the-dataset">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>Similar to MNIST, a KMNIST image has only one channel. Let’s visualize one.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> nn</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> datasets, transforms</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> DataLoader</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> torchvision.utils <span class="im" style="color: #00769E;">import</span> make_grid</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Function learnt from GAN's Specialization Course 1 Week 1</span></span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;">def</span> tensor_show(image_tensor, num_images<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">25</span>, size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">28</span>, <span class="dv" style="color: #AD0000;">28</span>)):</span>
<span id="cb2-3">    <span class="co" style="color: #5E5E5E;"># The original image tensor could be stored on GPU and </span></span>
<span id="cb2-4">    <span class="co" style="color: #5E5E5E;"># have been flattened out for training, so we restore it</span></span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;"># first.</span></span>
<span id="cb2-6">    image_unflat <span class="op" style="color: #5E5E5E;">=</span> image_tensor.detach().cpu().view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">*</span>size)</span>
<span id="cb2-7">    image_grid <span class="op" style="color: #5E5E5E;">=</span> make_grid(image_unflat[:num_images], nrow<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb2-8">    <span class="co" style="color: #5E5E5E;"># torch uses (color channel, height, width) while </span></span>
<span id="cb2-9">    <span class="co" style="color: #5E5E5E;"># matplotlib used (height, width, color channel)</span></span>
<span id="cb2-10">    <span class="co" style="color: #5E5E5E;"># so we fix it here</span></span>
<span id="cb2-11">    plt.imshow(image_grid.permute(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>).squeeze())</span>
<span id="cb2-12">    plt.show()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">32</span></span>
<span id="cb3-2">dataloader <span class="op" style="color: #5E5E5E;">=</span> DataLoader(</span>
<span id="cb3-3">    datasets.KMNIST(<span class="st" style="color: #20794D;">'data'</span>, download<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, transform<span class="op" style="color: #5E5E5E;">=</span>transforms.ToTensor()),</span>
<span id="cb3-4">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb3-5">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">image_tensor <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dataloader))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-2">tensor_show(image_tensor)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/index_files/figure-html/cell-5-output-1.png" width="416" height="408"></p>
</div>
</div>
</section>
<section id="the-discriminator" class="level2">
<h2 class="anchored" data-anchor-id="the-discriminator">The Discriminator:</h2>
<p>The Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol’ linear model, but I will skip a bit to the year 2015, when <a href="https://ar5iv.labs.arxiv.org/html/1511.06434">Deep Convolutional GAN</a> was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A not-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN’s training. We will touch upon tricks to stabilize GAN’s training in the next post).</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">class</span> Discriminator(nn.Module):</span>
<span id="cb5-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, im_chan<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, hidden_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">56</span>):</span>
<span id="cb5-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb5-4">        <span class="va" style="color: #111111;">self</span>.disc <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb5-5">            <span class="va" style="color: #111111;">self</span>.make_disc_block(im_chan, hidden_dim),</span>
<span id="cb5-6">            <span class="va" style="color: #111111;">self</span>.make_disc_block(hidden_dim, hidden_dim),</span>
<span id="cb5-7">            <span class="va" style="color: #111111;">self</span>.make_disc_block(hidden_dim, <span class="dv" style="color: #AD0000;">1</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>),</span>
<span id="cb5-8">        )</span>
<span id="cb5-9"></span>
<span id="cb5-10">    <span class="kw" style="color: #003B4F;">def</span> make_disc_block(<span class="va" style="color: #111111;">self</span>, input_channels, output_channels, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb5-11">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> final_layer:</span>
<span id="cb5-12">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb5-13">                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span>
<span id="cb5-14">                    nn.BatchNorm2d(output_channels),</span>
<span id="cb5-15">                    nn.LeakyReLU(negative_slope<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.25</span>)</span>
<span id="cb5-16">            )</span>
<span id="cb5-17">        <span class="cf" style="color: #003B4F;">else</span>: <span class="co" style="color: #5E5E5E;"># Final Layer</span></span>
<span id="cb5-18">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb5-19">                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)</span>
<span id="cb5-20">            )</span>
<span id="cb5-21">    </span>
<span id="cb5-22">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x: torch.Tensor):</span>
<span id="cb5-23">        <span class="co" style="color: #5E5E5E;"># The input can be a tensor of multiple images</span></span>
<span id="cb5-24">        <span class="co" style="color: #5E5E5E;"># We want to return a tensor with the possibility</span></span>
<span id="cb5-25">        <span class="co" style="color: #5E5E5E;"># of real/fake for each image.</span></span>
<span id="cb5-26">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.disc(x)</span>
<span id="cb5-27">        <span class="cf" style="color: #003B4F;">return</span> x.view(<span class="bu" style="color: null;">len</span>(x), <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
</section>
<section id="the-generator" class="level2">
<h2 class="anchored" data-anchor-id="the-generator">The Generator:</h2>
<p>A point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially <em>distilling</em> the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the <em>transposed convolution</em> operation, which <em>increases</em> the dimensions of data, essentially <em>magnifying</em> the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this <a href="https://github.com/HangenYuu/vision_learner/blob/main/ARCHITECTURE/CNN/Tiny/TinyCNN.ipynb">notebook</a> as my draft.)</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">class</span> Generator(nn.Module):</span>
<span id="cb6-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb6-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()        </span></code></pre></div>
</div>


</section>
</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan_p2/index.html</guid>
  <pubDate>Sat, 21 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-1.png" medium="image" type="image/png" height="44" width="144"/>
</item>
<item>
  <title>A Primer on Generative Adversarial Networks (GANs)</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan_p1/index.html</link>
  <description><![CDATA[ 




<p>If you have studied deep learning before, you will notice that we will encounter classification many times. To be honest, it is fun in a way, having your own model to classify anime characters. Alas, it is a bit dry to me. Intelligence, for me, is creativity, the ability to create something <em>new</em>. I want a model that can create, especially work of art. That led me right to GANs, not so much a model but an elegant way of thinking.</p>
<section id="a-brief-history-of-gans" class="level1">
<h1>A brief history of GANs</h1>
<p><em>For a fuller account, check out the <a href="https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/">MIT Technology Review article</a>.</em></p>
<p>Back in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from <a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a> to <a href="https://ar5iv.labs.arxiv.org/html/1409.1556">VGG</a>. (Not to mention <a href="https://ar5iv.labs.arxiv.org/html/1512.03385">ResNet</a> in 2015, an architecture with so interesting an idea that I had to <a href="https://github.com/HangenYuu/vision_learner/tree/main/ARCHITECTURE/CNN/Paper">make a project</a> for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, <a href="https://ar5iv.labs.arxiv.org/html/1406.2661">Generative Adversarial Nets</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/Screenshot%202023-01-20%20at%2019-53-43%20Generative%20Dog%20Images%20Kaggle.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The image was not from the era, but was representative of what you got from the model at that time (and still now with GANs, if your model was trained poorly or prematurely). <a href="https://www.kaggle.com/c/generative-dog-images/discussion/97753">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>Now I wanted to make two quick detours before going into the inside of GANs:</p>
<ol type="1">
<li>At its core sense, a <em>function</em> is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is <em>trying to update its parameters such that the model will approximate the optimal function as closely as possible</em>. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.</li>
<li>Advances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.</li>
</ol>
</section>
<section id="the-gans-game" class="level1">
<h1>The GANs game:</h1>
<p><strong>Note:</strong> I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.</p>
<p>The word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called <em>Generator</em>, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called <em>Discriminator</em>, (or <em>Critic</em>, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Generator</strong></th>
<th><strong>Discriminator</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>Random numbers</td>
<td>Images (real &amp; generated)</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Images</td>
<td>Class of image (binary)</td>
</tr>
<tr class="odd">
<td><strong>Role</strong></td>
<td>Forger</td>
<td>Appraiser</td>
</tr>
</tbody>
</table>
<p><strong>Quick detour:</strong> the GAN concept advances generative AI the same way backpropagation does so. The approach of trying to know the distribution of the image features was right, but the method was wrong a.k.a too complex and computationally expensive. With GAN, we have an elegant way to start with any random distribution while moving towards the optimal distribution incrementally. No need to know everything any more.</p>
<p>Our loss function will be the good ol’ binary cross-entropy: <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)%20=%20-%5Cfrac%7B1%7D%7Bm%7D*%5By%5E%7B(i)%7Dlog(h(x%5E%7B(i)%7D,%20%5Ctheta))%20+%20(1%20-%20y%5E%7B(i)%7D)log(1%20-%20(h(x%5E%7B(i)%7D,%20%5Ctheta)))%5D"></p>
<p>That surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know <img src="https://latex.codecogs.com/png.latex?y%5E%7B(i)%7D"> is the true label of the i<sup>th</sup> example (0 or 1), <img src="https://latex.codecogs.com/png.latex?h(x%5E%7B(i)%7D,%20%5Ctheta)"> is the predicted label for the i<sup>th</sup> example with input <img src="https://latex.codecogs.com/png.latex?x%5E%7B(i)%7D"> and parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Define the BCE function</span></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;">def</span> bce(y_true, y_pred):</span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;">return</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">*</span>(y_true<span class="op" style="color: #5E5E5E;">*</span>torch.log(y_pred) <span class="op" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>y_true)<span class="op" style="color: #5E5E5E;">*</span>torch.log(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>y_pred))</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">y_true <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-2">y_pred <span class="op" style="color: #5E5E5E;">=</span> torch.linspace(<span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">plt.figure()</span>
<span id="cb2-5">plt.plot(y_pred, bce(y_true, y_pred), <span class="st" style="color: #20794D;">"o--"</span>)</span>
<span id="cb2-6">plt.xlabel(<span class="st" style="color: #20794D;">"prediction"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb2-7">plt.ylabel(<span class="st" style="color: #20794D;">"loss"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb2-8">plt.grid()</span>
<span id="cb2-9">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-0" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/index_files/figure-html/fig-0-output-1.png" width="605" height="439" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: BCE loss when y = 0</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y_true <span class="op" style="color: #5E5E5E;">=</span> torch.ones(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb3-2">y_pred <span class="op" style="color: #5E5E5E;">=</span> torch.linspace(<span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb3-3">criterion <span class="op" style="color: #5E5E5E;">=</span> torch.nn.BCELoss(reduction<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'none'</span>)</span>
<span id="cb3-4"></span>
<span id="cb3-5">plt.figure()</span>
<span id="cb3-6">plt.plot(y_pred, bce(y_true, y_pred), <span class="st" style="color: #20794D;">"o--"</span>)</span>
<span id="cb3-7">plt.xlabel(<span class="st" style="color: #20794D;">"prediction"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb3-8">plt.ylabel(<span class="st" style="color: #20794D;">"loss"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb3-9">plt.grid()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/index_files/figure-html/fig-1-output-1.png" width="605" height="439" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: BCE loss when y = 1</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>I mentioned that this is a conflict between Generator and Discriminator. For Discriminator, it wants to classify correctly i.e.&nbsp;catch the Generator every time while approve the value of the real images. In other words, it wants to <em>minimize</em> its loss function. For Generator, it wants the reverse i.e.&nbsp;pass a fake as a real to the Discriminator every single time. In other words, it wants to <em>maximize</em> the loss function (of the Discriminator). This leads to the ter <em>minimax game</em> that you may hear some people use to describe GAN.</p>
<p>The game can be considered complete when the Discriminator’s accuracy drops to 50% i.e.&nbsp;it can no longer discern, and essentially has to guess at random for each image. At this, our Generator will become potent enough to fool even us with its <a href="https://thispersondoesnotexist.com/">humans</a> and <a href="https://thiscatdoesnotexist.com/">cats</a>.</p>
</section>
<section id="end-of-part-1" class="level1">
<h1>End of part 1:</h1>
<p>As a primer this is far enough. I will continue on the subject, describing each model’s simplest architecture possible, the process of training one, as well as the difficulty in training GANs. (Training a model is hard enough, now we have two.)</p>
<p><em>All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the <a href="https://ar5iv.labs.arxiv.org/">tool</a>. To change to the abstract page, follow this example:</em> <code>https://ar5iv.labs.arxiv.org/html/1409.1556</code> → <code>https://arxiv.org/abs/1409.1556</code>.</p>


</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan_p1/index.html</guid>
  <pubDate>Fri, 20 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://www.kaggle.com/c/generative-dog-images/discussion/97753" medium="image"/>
</item>
<item>
  <title>Hello World</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/hello_world/index.html</link>
  <description><![CDATA[ 




<section id="up-and-running-with-quarto" class="level2">
<h2 class="anchored" data-anchor-id="up-and-running-with-quarto">Up and running with Quarto!</h2>
<div id="hello-world" class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Hello World!"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Hello World!</code></pre>
</div>
</div>


</section>

 ]]></description>
  <category>new</category>
  <category>code</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/hello_world/index.html</guid>
  <pubDate>Fri, 21 Oct 2022 16:00:00 GMT</pubDate>
</item>
</channel>
</rss>
