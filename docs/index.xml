<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>H&#39;s notes</title>
<link>https://hangenyuu.github.io/h_notes/index.html</link>
<atom:link href="https://hangenyuu.github.io/h_notes/index.xml" rel="self" type="application/rss+xml"/>
<description>H&#39;s Notes on Deep Learning</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Sat, 11 Feb 2023 16:00:00 GMT</lastBuildDate>
<item>
  <title>Conditional GAN</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/index.html</link>
  <description><![CDATA[ 




<p>Apologize for being late: it has been nearly 3 weeks already since the latest post. But I am back for the last week of content.</p>
<p>When I started writing, it was near the time to bed, and without a doubt, I am hungry. So let’s deal with cookies this time.</p>
<p>Up until now, our GAN has managed to do some interesting stuff (“writing” Kanji characters, or numbers, if you used the traditional MNIST dataset). However, one thing you must notice is that we have <em>no control over what the Generator will give us</em>. It can be a “na”, it can be a “tsu”, it can be a “ki” - no control whatsoever. In our cookie analogy, or cookanalogy, our current is like a goodwilled roomie who bakes for us every day, but each day we will receive a random cookie type.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>You know it is cookie, but you have no idea what type is it</em></figcaption><p></p>
</figure>
</div>
<p>Now, if you love randomness and can tolerate the taste as well as the sugar, fine. But we usually want our model to be controllable, that is, we get to decide, or at least influence, what will be included in the output.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>It’s much nicer to control that you have matcha on Monday, chocochip on Tuesday, and so on.</em></figcaption><p></p>
</figure>
</div>
<p>With the objective set, let’s explore way to implement conditional GANs a.k.a way to make sure we have the correct cookie each day.</p>
<section id="limiting-to-just-one-category" class="level1">
<h1>Limiting to just one category:</h1>
<p>This is a no-brainer solution. To prevent random category generation (and mode collapse as well), who don’t just feed in variational data of a single class only? In our cookanalogy, it is like always mixing matcha powder into the dough to make the cookies. Being an inexperienced chef and afraid of being cause, we sneak matcha powder in inconsistently between the days, but we are sure that every day we will get matcha cookies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>A matcha cookie junkie’s dream. (Anyone else besides me?)</em></figcaption><p></p>
</figure>
</div>
<p>Obviously this solution is for when you want to generate examples of one class only. One example would be augmenting data for brain EMR of a certain disease at a certain region. The other <del>trolled</del> example is <a href="https://www.kaggle.com/datasets/andy8744/ganyu-genshin-impact-anime-faces-gan-training">GANyu</a>, a dataset and models fine-tuned on it for the task of generating faces of the Genshin Impact character Ganyu (Check out the <a href="https://www.kaggle.com/datasets/andy8744/rezero-rem-anime-faces-for-gan-training">GA(N)Rem</a> as well).</p>
<blockquote class="blockquote">
<p>I don’t know what is the thing for animes and GANs, but the moment I discovered GANs, I instantly thought of generating anime girls’ faces. Is is the same phenomenon as researchers in the 90’s instantly thought of classifying cat from everything else the moment they got a decent classifier… - A certain unfamous author on the web</p>
</blockquote>
</section>
<section id="conditional-gans-conditional-inputs" class="level1">
<h1>Conditional GANs (Conditional inputs):</h1>
<p>One thing that we must make clear for our GAN: we cannot generate examples of something that is not in the training data. So first, we must ensure the thing that we want (matcha cookie) is in the training data (ingredients) fed to our two networks.</p>
<p>Next, we must be able to call on the Generator to produce what we want. This means that the data needs labelling. Simply put, the Discriminator needs to learn what a “matcha cookie” looks like before it can give any meaningful feedback - constrast between an original image and a generated one - to the Generator; the Generator also needs to know that the class it is generating to update the parameters accordingly. After training finishes, we can tell the Generator which class we want it to produce, and, voilà! A solution to achieve this was proposed in the <a href="https://arxiv.org/pdf/1411.1784.pdf">Conditional Generative Adversarial Nets</a> by passing some extra information <img src="https://latex.codecogs.com/png.latex?y"> (the easiest is class labels, possibly as one-hot vector) to both Generator and Discriminator.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>An input vector to Generator now is made up of two components: noise inputs (to ensure that each generation will be unique) &amp; one-hot class vector (to ensure that the generate example will be of the class we want)</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Similarly, an input to the Discriminator now is an image together with the an one-hot class vector. For an example to be regarded as real, not only it needs to look realistic (reflected by a low binary corss-entropy or Wasserstein loss) but it also needs to look like examples from the same class. Here, an original matcha cookie definitely looks like a matcha cookie.</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Here is the earlier generated matcha cookie. Let’s say that our Discriminator is a bit better than the Generator. It means that it will detect this looks rather like a matcha cookie, but not too alike.</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Now suppose that the required class is black chocochip, but our Generator gives a matcha cookie. The Discriminator will recognize in this case and gives a low chance that the example is real.</em></figcaption><p></p>
</figure>
</div>
<p>The question now is how do we go on implementing this? From the descriptions, it seems that we need to do 2 things: 1) figure a way to pass the additional information into our two models and 2) update the loss function. However, we just need to do 1), as the same loss function (binary cross-entropy) can be used, we just need to make sure that the class of the examples are included in the output as well. For Generator, you just need to concatenate it with the noise vector above. For Discriminator, it is a bit trickier. We feed the images in by passing values of three channels, so the simplest way will be to create n channels more for n classes. This way works for dataset such as the good ol’ MNIST, whether we flatten out images before concatentating or we keep the same matrix and just call <code>torch.cat()</code> (which will create 10 more channels, each of size 28*28, with one of them full of 1 and the rest full of 0). For larger images or ones we cannot/do not want to flatten, this simple approach will create a huge memory issue. We will want to pass class information in a different, such as via a different head of input layer. Here are the code snippets.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;">def</span> get_one_hot_labels(labels: torch.Tensor, n_classes: <span class="bu" style="color: null;">int</span>):</span>
<span id="cb1-5">    <span class="co" style="color: #5E5E5E;">'''</span></span>
<span id="cb1-6"><span class="co" style="color: #5E5E5E;">    Function for creating one-hot vectors for the data.</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;">    :param labels: a vector containing the labels of all examples in a batch.</span></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;">                   Get from each DataLoader. Have shape (n_samples, 1)</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;">    :param n_classes: an integer for number of classes in the data.</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;">                      Get from the dataset created</span></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;">    :return: the one-hot vector for a batch of data</span></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb1-14">    <span class="cf" style="color: #003B4F;">return</span> F.one_hot(labels,n_classes)</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="kw" style="color: #003B4F;">def</span> combine_vectors(x, y):</span>
<span id="cb1-17">    <span class="co" style="color: #5E5E5E;">'''</span></span>
<span id="cb1-18"><span class="co" style="color: #5E5E5E;">    Generic function for combining two 2-D maxtrices with the same 0-shape</span></span>
<span id="cb1-19"><span class="co" style="color: #5E5E5E;">    In our case, they will be (n_samples, x_1) and (n_samples, y_1).</span></span>
<span id="cb1-20"></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;">    :param x: the first matrix, shape (n_samples, x_1)</span></span>
<span id="cb1-22"><span class="co" style="color: #5E5E5E;">    :param y: the second matrix, shape (n_samples, y_1)</span></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;">    :return: the concatenated matrix of shape (n_samples, x_1 + y_1)</span></span>
<span id="cb1-24"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb1-25">    <span class="co" style="color: #5E5E5E;"># To ensure unity of data types, we want the return matrix to have float</span></span>
<span id="cb1-26">    <span class="co" style="color: #5E5E5E;"># type.</span></span>
<span id="cb1-27">    combined <span class="op" style="color: #5E5E5E;">=</span> torch.cat((x.<span class="bu" style="color: null;">float</span>(),y.<span class="bu" style="color: null;">float</span>()), <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-28">    <span class="cf" style="color: #003B4F;">return</span> combined</span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Code will not run if just copy-paste</span></span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;"># Pre-training</span></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;"># Just the basic part.</span></span>
<span id="cb2-4">kmnist_shape <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">28</span>, <span class="dv" style="color: #AD0000;">28</span>)</span>
<span id="cb2-5">n_classes <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb2-6"></span>
<span id="cb2-7">device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'cuda'</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cpu'</span></span>
<span id="cb2-8">criterion <span class="op" style="color: #5E5E5E;">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb2-9">z_dim <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">64</span> <span class="co" style="color: #5E5E5E;"># Size of the noise vector</span></span>
<span id="cb2-10">gen <span class="op" style="color: #5E5E5E;">=</span> Generator(input_dim<span class="op" style="color: #5E5E5E;">=</span>generator_input_dim).to(device)</span>
<span id="cb2-11">gen_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(gen.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr)</span>
<span id="cb2-12">disc <span class="op" style="color: #5E5E5E;">=</span> Discriminator(im_chan<span class="op" style="color: #5E5E5E;">=</span>discriminator_im_chan).to(device)</span>
<span id="cb2-13">disc_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(disc.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr)</span>
<span id="cb2-14">epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">200</span></span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="kw" style="color: #003B4F;">def</span> weights_init(m):</span>
<span id="cb2-17">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.Conv2d) <span class="kw" style="color: #003B4F;">or</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.ConvTranspose2d):</span>
<span id="cb2-18">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb2-19">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.BatchNorm2d):</span>
<span id="cb2-20">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb2-21">        torch.nn.init.constant_(m.bias, <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb2-22">gen <span class="op" style="color: #5E5E5E;">=</span> gen.<span class="bu" style="color: null;">apply</span>(weights_init)</span>
<span id="cb2-23">disc <span class="op" style="color: #5E5E5E;">=</span> disc.<span class="bu" style="color: null;">apply</span>(weights_init)</span>
<span id="cb2-24"></span>
<span id="cb2-25"><span class="co" style="color: #5E5E5E;"># Training loop</span></span>
<span id="cb2-26">cur_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb2-27">generator_losses <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb2-28">discriminator_losses <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb2-29"></span>
<span id="cb2-30"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb2-31">    <span class="cf" style="color: #003B4F;">for</span> real, labels <span class="kw" style="color: #003B4F;">in</span> tqdm(dataloader):</span>
<span id="cb2-32">        n_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(real)</span>
<span id="cb2-33">        real <span class="op" style="color: #5E5E5E;">=</span> real.to(device)</span>
<span id="cb2-34"></span>
<span id="cb2-35">        <span class="co" style="color: #5E5E5E;"># Get image one-hot labels for this batch</span></span>
<span id="cb2-36">        one_hot_labels <span class="op" style="color: #5E5E5E;">=</span> get_one_hot_labels(labels.to(device), n_classes)</span>
<span id="cb2-37">        <span class="co" style="color: #5E5E5E;"># Remember that the DataLoader is in size (n_samples, 1, 28, 28) while the one hot label matrix </span></span>
<span id="cb2-38">        <span class="co" style="color: #5E5E5E;"># has size (n_samples, 1). We need to extend 2 more dimensions if we want to concatenate the two.</span></span>
<span id="cb2-39">        image_one_hot_labels <span class="op" style="color: #5E5E5E;">=</span> one_hot_labels[:, :, <span class="va" style="color: #111111;">None</span>, <span class="va" style="color: #111111;">None</span>]</span>
<span id="cb2-40">        <span class="co" style="color: #5E5E5E;"># Now the one-hot labels matrix has size (n_samples, 1, 1, 1). We need to turn it into</span></span>
<span id="cb2-41">        <span class="co" style="color: #5E5E5E;"># (n_samples, 1, 28, 28) to pass into the input layer.</span></span>
<span id="cb2-42">        image_one_hot_labels <span class="op" style="color: #5E5E5E;">=</span> image_one_hot_labels.repeat(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, kmnist_shape[<span class="dv" style="color: #AD0000;">1</span>], kmnist_shape[<span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb2-43"></span>
<span id="cb2-44">        <span class="co" style="color: #5E5E5E;">### Update discriminator</span></span>
<span id="cb2-45">        <span class="co" style="color: #5E5E5E;"># Zero out the discriminator gradients</span></span>
<span id="cb2-46">        disc_opt.zero_grad()</span>
<span id="cb2-47"></span>
<span id="cb2-48">        <span class="co" style="color: #5E5E5E;"># Get noise corresponding to the current batch_size </span></span>
<span id="cb2-49">        fake_noise <span class="op" style="color: #5E5E5E;">=</span> get_noise(n_samples, z_dim, device<span class="op" style="color: #5E5E5E;">=</span>device)</span>
<span id="cb2-50">        </span>
<span id="cb2-51">        <span class="co" style="color: #5E5E5E;"># Combine the label and the noise and generate fake examples</span></span>
<span id="cb2-52">        noise_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(fake_noise, one_hot_labels)</span>
<span id="cb2-53">        fake <span class="op" style="color: #5E5E5E;">=</span> gen(noise_and_labels)</span>
<span id="cb2-54"></span>
<span id="cb2-55">        <span class="co" style="color: #5E5E5E;"># Get Discriminator's predictiopn on the real and the fake examples</span></span>
<span id="cb2-56">        fake_image_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(fake, image_one_hot_labels)</span>
<span id="cb2-57">        real_image_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(real, image_one_hot_labels)</span>
<span id="cb2-58">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake_image_and_labels.detach()) <span class="co" style="color: #5E5E5E;"># do not update the Generator</span></span>
<span id="cb2-59">        disc_real_pred <span class="op" style="color: #5E5E5E;">=</span> disc(real_image_and_labels)</span>
<span id="cb2-60"></span>
<span id="cb2-61">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb2-62">        disc_fake_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span>
<span id="cb2-63">        disc_real_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span>
<span id="cb2-64">        disc_loss <span class="op" style="color: #5E5E5E;">=</span> (disc_fake_loss <span class="op" style="color: #5E5E5E;">+</span> disc_real_loss) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb2-65"></span>
<span id="cb2-66">        <span class="co" style="color: #5E5E5E;"># Backpropagation</span></span>
<span id="cb2-67">        disc_loss.backward(retain_graph<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-68"></span>
<span id="cb2-69">        <span class="co" style="color: #5E5E5E;"># Update the parameters</span></span>
<span id="cb2-70">        disc_opt.step()</span>
<span id="cb2-71"></span>
<span id="cb2-72">        <span class="co" style="color: #5E5E5E;"># Keep track of the average discriminator loss for visualization</span></span>
<span id="cb2-73">        discriminator_losses <span class="op" style="color: #5E5E5E;">+=</span> [disc_loss.item()]</span>
<span id="cb2-74"></span>
<span id="cb2-75">        <span class="co" style="color: #5E5E5E;">### Update generator</span></span>
<span id="cb2-76">        <span class="co" style="color: #5E5E5E;"># Zero out the generator gradients</span></span>
<span id="cb2-77">        gen_opt.zero_grad()</span>
<span id="cb2-78"></span>
<span id="cb2-79">        <span class="co" style="color: #5E5E5E;"># Regenerate the fake examples with gradients to update</span></span>
<span id="cb2-80">        fake_image_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(fake, image_one_hot_labels)</span>
<span id="cb2-81">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake_image_and_labels)</span>
<span id="cb2-82"></span>
<span id="cb2-83">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb2-84">        gen_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span>
<span id="cb2-85"></span>
<span id="cb2-86">        <span class="co" style="color: #5E5E5E;"># Backpropgation</span></span>
<span id="cb2-87">        gen_loss.backward()</span>
<span id="cb2-88"></span>
<span id="cb2-89">        <span class="co" style="color: #5E5E5E;"># Update the parameters</span></span>
<span id="cb2-90">        gen_opt.step()</span>
<span id="cb2-91"></span>
<span id="cb2-92">        <span class="co" style="color: #5E5E5E;"># Keep track of the generator losses for visualization</span></span>
<span id="cb2-93">        generator_losses <span class="op" style="color: #5E5E5E;">+=</span> [gen_loss.item()]</span></code></pre></div>
</section>
<section id="controllable-generation-noise-is-not-just-noise" class="level1">
<h1>Controllable Generation (Noise is not just noise):</h1>
<p>Up until now, the noise vector fed into GANs is just “noise” - sounds meaningless. However, the numbers in the noise vector do mean something. You can think that each number represent one feature that is recognized by the Generator. The combinations of all these features form a <em>latent space</em> - a space containing a simpler but hidden (to humans) information for generated examples. This is best understood with the example of amino acid.</p>
<p>The basic of every bodily function is protein, which is a chain of amino acids (you don’t need to know what they are). Each amino acid in the chain is encoded as a sequence of 3 nucleotides, which have 4 in total (there are <img src="https://latex.codecogs.com/png.latex?4%5E%7B3%7D=64"> total combinations, but several combinations encoding the same amino acid, and there are special ones called <em>ending combinations</em> that signify the end but do not encode).</p>
<p>All the 64 combinations can thought of as the <em>latent space</em> of the amino acid. It’s like we have a well-trained Generator on 22 classes that output the exact amino acid or ending signal we want by passing into it certain 3 nucleotides. This is a latent space because the information has been simplified, but it is not quite latent because we now know the exact <em>embedding</em> of the information.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/genetic code.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em><a href="https://openstax.org/books/biology/pages/15-1-the-genetic-code#fig-ch15_01_04">“The genetic code”</a> by OpenStax College, Biology</em></figcaption><p></p>
</figure>
</div>
<p>In our KMNIST example, each image can be represented as a 28 by 28 matrix where each position stores the intensity of the pixel. It can be visualized as below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/KMNIST eg.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>A certain Japanese letter</em></figcaption><p></p>
</figure>
</div>
<p>In the noise vector above, we try to compress this information down to a vector of 64 numbers (recalling the <code>z_dim</code> above), hoping that this is sufficient to store the information to construct all 10 classes of handwritten kanji characters. But let’s fall back to our cookies for a more easily visualizable example.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 5.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Before, we have been generating random numbers in the noise vector. Let’s say that we have been able to decode that the second number in our noise vector encodes information for color of the cookie, with 2.1 signifies the green matcha color we wants.</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Now, after training, we now know that 1.4 corresponds to the brown color of chochip cookies. We can now pass the number to get a brown cookie.</em></figcaption><p></p>
</figure>
</div>


</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/index.html</guid>
  <pubDate>Sat, 11 Feb 2023 16:00:00 GMT</pubDate>
  <media:content url="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 1.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>That Unstable GAN</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan-p3/index.html</link>
  <description><![CDATA[ 




<p>In the last two posts, I have gone over the concept and process of training GANs. I made it seem so straightforward, but reality is harsher. Recall that for GANs, we are training two neural networks at once, and these two compete with each other to mutually improve. Much like real life, if one competitor is too good, the other will not learn anything, if the at point before the end the Generator or the Discriminator becomes too good (or the other becomes too bad), then training breaks down. Thus, training GANs is highly unstable, and any way to ensure smoother training is encouraged.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/test 17.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>I tried to find an xkcd comic for training GANs, but found none. Instead I found this <a href="https://github.com/generic-github-user/xkcd-Generator/">repo</a> about using GANs to generate xkcd comic. It is not even close for a substitute, but you can defintely see that training has broken down: the loss of Generator is way much more than the loss of the Discriminator, and the difference between THIS and an <a href="https://xkcd.com/1838/">xkcd comic</a> is obvious</em></figcaption><p></p>
</figure>
</div>
<section id="general-methods" class="level1">
<h1>General methods</h1>
<section id="activation-function" class="level2">
<h2 class="anchored" data-anchor-id="activation-function">Activation function</h2>
<p>Activation function is a requirement for neural networks’ ability to approximate complex function. Without it, a neural network will become just another linear function.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sb</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-8"></span>
<span id="cb1-9">np.random.seed(<span class="dv" style="color: #AD0000;">17</span>)</span>
<span id="cb1-10">torch.manual_seed(<span class="dv" style="color: #AD0000;">17</span>)</span>
<span id="cb1-11"><span class="kw" style="color: #003B4F;">def</span> linear(a, b, x):</span>
<span id="cb1-12">    <span class="cf" style="color: #003B4F;">return</span> a<span class="op" style="color: #5E5E5E;">*</span>x <span class="op" style="color: #5E5E5E;">+</span> b</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">x <span class="op" style="color: #5E5E5E;">=</span> torch.randn(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-2"></span>
<span id="cb2-3">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">9</span>,<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb2-4">ax1 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">121</span>)</span>
<span id="cb2-5">ax2 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">122</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7">ax1.plot(x, linear(<span class="fl" style="color: #AD0000;">.5</span>, <span class="dv" style="color: #AD0000;">4</span>, x) <span class="op" style="color: #5E5E5E;">+</span> linear(<span class="fl" style="color: #AD0000;">3.56</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">5.32</span>, x) <span class="op" style="color: #5E5E5E;">+</span> linear(<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.86</span>, <span class="fl" style="color: #AD0000;">3.74</span>, x), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb2-8">ax2.plot(x, torch.relu(<span class="fl" style="color: #AD0000;">0.5</span><span class="op" style="color: #5E5E5E;">*</span>x) <span class="op" style="color: #5E5E5E;">+</span> torch.relu(<span class="fl" style="color: #AD0000;">3.56</span><span class="op" style="color: #5E5E5E;">*</span>x) <span class="op" style="color: #5E5E5E;">+</span> torch.relu(<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.86</span><span class="op" style="color: #5E5E5E;">*</span>x), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb2-9"></span>
<span id="cb2-10">ax1.grid()</span>
<span id="cb2-11">ax2.grid()</span>
<span id="cb2-12">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/index_files/figure-html/fig-1-output-1.png" width="707" height="259" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Stacking linear functions on top of each other is just a linear function. Meanwhile, stacking ReLU functions on top of each other create a piecemeal linear function that approximates a curve.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We all starts with the sigmoid function in a binary cross-entropy problem. However, sigmoid, together with tanh, leads to the “vanishing gradient” problem. When the output value of gets close to 0 or 1 for sigmoid (or -1 or 1 for tanh), the gradient gets close to 0, so the weights either are updated very slowly or stop learning altogether. That was when ReLU came into play: the function has a clear, positive gradient when output value is greater than 0, while the bend makes sure that ReLU stacking on each other can produce a curve.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/1 KKjPz4KaEERCpvI04D6Bng.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Each neural network had three hidden layers with three units in each one. The only difference was the activation function. Learning rate: 0.03, regularization: L2. <a href="https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>However, the joy ReLU brought came to halt when “dying ReLU” problem was reported. Suppose we have an output smaller or equal 0, then our derivative will be 0. The 0 derivative on the node means that it will not get updated, and that’s the end for it. Worse, the previous components connected to the node are affected as well, so the whole structure of our neural network will be “dead”. To fix, we have the variation: LeakyReLU. For LeakyReLU, the output value below 0 is not set at 0, but is multiplied by a constant (such as 0.2). Gradient for such value will still be non-zero, provide information to update the weights.</p>
<p>Another, more advanced variation is <a href="https://ar5iv.labs.arxiv.org/html/1606.08415v4">GeLU</a>, where the output is multiplied with i.e.&nbsp;weighted by its percentile. Sounds too complicated? Look at the formula: <img src="https://latex.codecogs.com/png.latex?GELU(x)=x*P(X%3Cx)=x*%5CPhi(x)"> for <img src="https://latex.codecogs.com/png.latex?X"> ~ <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BN%7D(0,%201)"></p>
<p>GELU has been successfully applied in Transformer models such as <a href="https://ar5iv.labs.arxiv.org/html/1810.04805v2">BERT</a>, <a href="https://ar5iv.labs.arxiv.org/html/2005.14165v4">GPT-3</a>, and especially in CNN such as <a href="https://ar5iv.labs.arxiv.org/html/2201.03545">ConvNeXts</a>. (Yeah, look at ConvNeXts - it started with a ResNet, the great ConvNet architecture, then the authors slowly introduced all the modern training tricks, until the result surpassed the Swin Transformer in the cheer of CNN-backer/Transformer-haters. Okay, that was eaxaggerating, but still…)</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">9</span>,<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb3-2">ax1 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">121</span>)</span>
<span id="cb3-3">ax2 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">122</span>)</span>
<span id="cb3-4"></span>
<span id="cb3-5">ax1.plot(x, F.leaky_relu(x, negative_slope<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb3-6">ax2.plot(x, F.gelu(x), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb3-7"></span>
<span id="cb3-8">ax1.grid()</span>
<span id="cb3-9">ax2.grid()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/index_files/figure-html/fig-2-output-1.png" width="719" height="259" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: LeakyReLU and GELU</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Now let’s move on to the second general trick that we have already done: batch normalization.</p>
</section>
<section id="batch-normalization" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalization">Batch normalization</h2>
<p>We all know that neural netowrk is trying to appromixate a certain way of mapping inputs i.e.&nbsp;data to outputs. The parameters of a neural network therefore depend on the data we receive, characteristically the <em>distribution of the data</em>. Here I have this example of an HDR image, which captures a farther range of color and exposure than a compressed format such as JPG or PNG. I found the original image from the Internet <a href="https://blog.gregzaal.com/2014/03/29/pano-golden-gate/">here</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Screenshot 2023-01-24 205502.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The curve at the bottom that may remind you of a bell curve is the curve for the distribution of pixel values a.k.a colors</em></figcaption><p></p>
</figure>
</div>
<p>Now, we train a neural network on data having similar color distribution such as this image, possibly for the task of recognizing grass. The model was trained well. Alas, the testing image contains one such as this</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Screenshot 2023-01-24 205554.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>This was the exact same image, but compressed at a differen color distribution (shifted to the right)</em></figcaption><p></p>
</figure>
</div>
<p>Here we say that the data distribution <em>has shifted between training data and testing data</em>. This generally will cause model problems (decrease accuracy, etc.). Data distribution shift (or covariate shift) can also happen between batches of training data, leading to slow convergence (imagine the model has to take a zig-zag path instead of a straight one). This can be dealt with by <em>normalization</em>, where make sure that the distributions of the training set and the testing set are similar e.g.&nbsp;centered around a mean of 0 and a standard deviation of 1. This could be done by taking the mean and standard deviation for each training batch of image and normalize the inputs of each training batch, then take the accumulated statistics to normalize the testing set during testing. This will smooth out the cost function and increases model performance (you might not need to do this if your training set and testing set are already similar to each other).</p>
<p>However, model is susceptible to <em>internal covariate shift</em> as well, where the activation output distributions shift between each layer. This can happen due to the change in the weights of each layer. Batch normalization came into play here by normalizing the inputs to each layer (“batch” means that we do so for each batch of image). For example, supposed are at nueron <img src="https://latex.codecogs.com/png.latex?i"> of non-last layer <img src="https://latex.codecogs.com/png.latex?l">, with activated output from the last layer to this neuron being <img src="https://latex.codecogs.com/png.latex?a_%7Bi%7D%5E%7B%5Bl-1%5D%7D">. The logit out of this neuron will be <img src="https://latex.codecogs.com/png.latex?z_%7Bi%7D%5E%7B%5Bl%5D%7D=%5CSigma%20W_%7Bi%7D%5E%7B%5Bl%5D%7Da_%7Bi%7D%5E%7B%5Bl-1%5D%7D"></p>
<p>Without batch normalization, the logit will be passed into activation to output <img src="https://latex.codecogs.com/png.latex?a_%7Bi%7D%5E%7B%5Bl%5D%7D">. But here, we will perform batch normalization:</p>
<ol type="1">
<li>We get the statistics mean <img src="https://latex.codecogs.com/png.latex?%5Cmu%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D%20%5E%7B2%7D"> for the batch.</li>
<li>We use them in the formula <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bz%7D_%7Bi%7D%5E%7B%5Bl%5D%7D=%5Cfrac%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D-%5Cmu%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D%7D%7B%5Csqrt%7B%5Csigma%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D%20%5E%7B2%7D%20+%20%5Cepsilon%7D%7D"> Nothing too fancy - it’s just the normalization formula that you encounter in any statistics course/textbook: substract the value by the mean, then divide it by the square root of variance a.k.a the standard deviation. The <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> term is a positive constant there to make sure that the denominator is always positive.</li>
<li>We map the normalized value <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bz%7D_%7Bi%7D%5E%7B%5Bl%5D%7D"> to a new distribution with the formula <img src="https://latex.codecogs.com/png.latex?y_%7Bi%7D%5E%7B%5Bl%5D%7D=%5Cgamma*%5Chat%7Bz%7D_%7Bi%7D%5E%7B%5Bl%5D%7D%20+%20%5Cbeta"> where <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is <em>scale factor</em> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> the <em>shift factor</em>. These two are learnable inputs in the batch normalization layer, and will be tuned to figure out the best distribution for the task at hand.</li>
<li>We pass <img src="https://latex.codecogs.com/png.latex?y_%7Bi%7D%5E%7B%5Bl%5D%7D"> through the activation function to the output <img src="https://latex.codecogs.com/png.latex?a_%7Bi%7D%5E%7B%5Bl%5D%7D">.</li>
</ol>
<p>The batch normalization layer seems complicated, but we usually does not need to all the things. As backpropagation is reduced to just calling <a href="https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop"><code>loss.backward</code></a> in PyTorch, the <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html"><code>nn.BatchNorm2d()</code></a> (for images) will take care of this during training.</p>
<p>There is another normalization method called <em>layer normalization</em>. I will not go into details here, though I very much want to because it was used in the training of ConvNeXts as well (seriously, I want to make a blog post just about the tricks used in pushing this CNN to surpass Swin). Here is a <a href="https://www.pinecone.io/learn/batch-layer-normalization/">post</a> about the two normalizations that also have great images. In PyTorch, this is implemented in <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"><code>nn.LayerNorm()</code></a>.</p>
</section>
</section>
<section id="gans-specific-method" class="level1">
<h1>GAN’s specific method</h1>
<p>To be honest, there should be tens of tricks for GANs. But I will only cover one this post: Wasserstein GAN (WGAN) and the accompanied Gradient Penalty.</p>
<section id="wgan" class="level2">
<h2 class="anchored" data-anchor-id="wgan">WGAN:</h2>
<p>First, we need to talk about <em>mode collapse</em>. Now, a mode in statistical term is the value that we are most likely to get from a distribution (not too correct for continuous distribution, but still great for understanding). This will be represented by a peak in the data distribution, such as the mean in a normal distribution. A distribution can have just one mode, like the normal distribution, or multiple modes like below.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">sample1 <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(loc<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">20</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">300</span>)</span>
<span id="cb4-2">sample2 <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(loc<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">700</span>)</span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;"># Concatenating the two sample along the second axis</span></span>
<span id="cb4-4">sample <span class="op" style="color: #5E5E5E;">=</span> np.hstack((sample1, sample2))</span>
<span id="cb4-5"></span>
<span id="cb4-6">sb.kdeplot(sample)</span>
<span id="cb4-7">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/index_files/figure-html/fig-3-output-1.png" width="606" height="404" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: A bimodal distribution created by merging two normal distributions</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The outputs have their modes alright. For example, in our KMNIST dataset, there are 10 modes for the output, corresponding to 10 characters. So we have a new way to think about training: we are trying to make the model learn to shift the distribution of the outputs to approximate the one we want. For illustration, suppose initially our model is outputing each value for each pixel randomly, leading to an output distribution like this.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Copy of GAN-p2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Initial output distribution</em></figcaption><p></p>
</figure>
</div>
<p>We want to change the output to this kind of distribution</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Copy of GAN-p3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>We want to shift from the circle to just 10 peaks i.e.&nbsp;just outputting 1 from 10 classes at a time</em></figcaption><p></p>
</figure>
</div>
<p>In the ideal scenario, our model will be guided by the loss function to make the right shift. However, notice a lack in the BCE loss: it only promotes the model generating images close to real images, regardless of the class of the image. This means that there exists a quick n’ dirty way for the Generator to reduce the loss by <em>only generating images from 1 class that the Discriminator is most fooled by</em>. So there exists a case where we end up with a Generator that outputs very realistic kanji character, only that it generates <em>kanji character</em>, say, only “tsu”. That’s boring.</p>
<p>My last paragraph gives hint to the source of the problem: our loss function. BCE loss works to push the Generator forward, but it cannot capture the information of class within the image. We need something else. And that something else is <a href="https://ar5iv.labs.arxiv.org/html/1701.07875">Wasserstein GAN</a>, introducing a new kind of loss function: the Wasserstein Loss (no surprise) a.k.a the <em>Earth Mover’s distance</em>, or EM distance. It measures the difference between two distributions, and can be informally defined as the least amount of energy required to move and mold a earth pile in the shape of one distribution to the shape of another distribution (hence earth mover).</p>
<p>In mathematical form, if we have the noise vector <img src="https://latex.codecogs.com/png.latex?z">, the fake image data <img src="https://latex.codecogs.com/png.latex?x">, the Generator model <img src="https://latex.codecogs.com/png.latex?g()">, the Discriminator who becomes the Critic <img src="https://latex.codecogs.com/png.latex?c()">, then the Wasserstein Loss is the difference between the expected value i.e.&nbsp;the mean of the Critic outputs for real images and the mean of the Critic outputs for generated images. <img src="https://latex.codecogs.com/png.latex?WLoss=E(c(x))-E(c(g(z)))"></p>
<p>We still have a minimax game here, with the Generator’s goal being minimize the above difference and the Critic’s goal being maximize the above difference. Notice also that the Critic now is no longer a classifier: it can give any real value possible e.g.&nbsp;higher score for real(istic) examples. This means that the Generator will get useful feedback for all classes of examples, and is less prone to mode collapse. Getting rid of the classifier i.e.&nbsp;the sigmoid function in the output also means that vanishing gradient is also less likely. Two birds, one stone.</p>
<p>W-Loss has one condition: the function of the Critic should be 1-Lipschitz Continuity. That looks intimidating but it just means that the norm of the gradient (the value in 2D math, the <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bx%5E%7B2%7D+y%5E%7B2%7D+...%7D"> as example in higher dimensions) for the Critic can be at most 1 at any point. In other words, the output of the Critic cannot increase/decrease more than linearly at any point. To achieve this, the first proposed (and terrible way, according to the original author) was <em>weight clipping</em> - forcing the weights to a fixed interval, say, [0,1]. Any negative value will be set to 0, and any value more than 1 will be set to 1. This was terrible (I have to say it again) because it limits the potential of the Critic. Another less strict way is <a href="https://arxiv.org/abs/1704.00028">gradient penalty</a> (the first dead ar5iv link), where you add a regularization term in the loss function to <em>promote</em> the Critic to be 1-Lipschitz Continuity, as oppposed to forcing it. Formula is <img src="https://latex.codecogs.com/png.latex?WLoss=E(c(x))-E(c(g(z)))%20+%20%5Clambda%20*%20pen"> with <img src="https://latex.codecogs.com/png.latex?pen=E((%7C%7C%20%5Cnabla%20c(%5Chat%7Bx%7D)%7C%7C_%7B2%7D-1)%5E%7B2%7D)">, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D=%5Cepsilon%20x%20+%20(1-%5Cepsilon)g(z)"></p>
<p>For completeness, the Critic gradient needs checking at every point in the feature space, which is impractical. What we do is sampling some points from real examples, and then some points from generated examples with weights for each, and then we calculate the penalty for the interpolated examples. An example for the code of WGAN can be found here (nothing for now). For a more technical review of WGAN, check out this <a href="https://ar5iv.labs.arxiv.org/html/1904.08994">paper</a>, also available as a <a href="https://lilianweng.github.io/posts/2017-08-20-gan/">blog post</a>.</p>
<p>Next in line: Conditional GANs.</p>
<p><em>All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the <a href="https://ar5iv.labs.arxiv.org/">tool</a>. To change to the abstract page, follow this example:</em> <code>https://ar5iv.labs.arxiv.org/html/1409.1556</code> → <code>https://arxiv.org/abs/1409.1556</code>.</p>


</section>
</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan-p3/index.html</guid>
  <pubDate>Mon, 23 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://github.com/generic-github-user/xkcd-Generator/" medium="image"/>
</item>
<item>
  <title>Building a simple GAN</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan_p2/index.html</link>
  <description><![CDATA[ 




<p>In the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, <a href="https://github.com/rois-codh/kmnist">KMNIST</a>.</p>
<section id="concept---training-gans" class="level1">
<h1>Concept - Training GANs:</h1>
<p>Like any training process, the process GANs can be decomposed into feed-forward and backpropagation, though with distinct features from, say, training a classifier. The parameters updated after backpropagation can also be divided into two steps, for Discriminator and Generator. These are summed up in the images below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">First, in the feed-forward, we pass some random noise (denoted by <img src="https://latex.codecogs.com/png.latex?%5Cxi">) into the Generator, which outputs some fake examples (denoted by <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">). The fake examples are then merged with a dataset of real examples (just <img src="https://latex.codecogs.com/png.latex?X">) and feed separately into the Discriminator, and we receive the outputs as a vector containing the possibilities of each example being real (between 0 and 1).</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Second, for training the Discriminator. We will calculate the loss as binary cross-entropy (BCE) loss for two components: how closely to 0 the Discriminator predicted the fake examples, and how closely to 1 the Discriminator predicted the real examples. Here, we need to detach the Generator from the gradient flow as we want to update the Discriminator’s parameters only</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Third, for training the Generator. From the predictions for the fake examples, we calculate the BCE loss as how closely the Discriminator predicted them to 1. We then update the Generator’s parameters.</figcaption><p></p>
</figure>
</div>
<p>Hopefully the ideas are not too complicated. If they are so, hopefully things will make more sense when we look at the codes.</p>
</section>
<section id="hands-on---creating-gans" class="level1">
<h1>Hands-on - Creating GANs:</h1>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">The dataset:</h2>
<p>First rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/kmnist%20examples.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The 10 classes of Kuzushiji-MNIST, with the first column showing each character’s modern hiragana counterpart. <a href="https://github.com/rois-codh/kmnist#the-dataset">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>Similar to MNIST, a KMNIST image has only one channel. Let’s visualize one.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> nn</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> datasets, transforms</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> DataLoader</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> torchvision.utils <span class="im" style="color: #00769E;">import</span> make_grid</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Function learnt from GAN's Specialization Course 1 Week 1</span></span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;">def</span> tensor_show(image_tensor, num_images<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">25</span>, size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">28</span>, <span class="dv" style="color: #AD0000;">28</span>)):</span>
<span id="cb2-3">    <span class="co" style="color: #5E5E5E;"># The original image tensor could be stored on GPU and </span></span>
<span id="cb2-4">    <span class="co" style="color: #5E5E5E;"># have been flattened out for training, so we restore it</span></span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;"># first.</span></span>
<span id="cb2-6">    image_unflat <span class="op" style="color: #5E5E5E;">=</span> image_tensor.detach().cpu().view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">*</span>size)</span>
<span id="cb2-7">    image_grid <span class="op" style="color: #5E5E5E;">=</span> make_grid(image_unflat[:num_images], nrow<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb2-8">    <span class="co" style="color: #5E5E5E;"># torch uses (color channel, height, width) while </span></span>
<span id="cb2-9">    <span class="co" style="color: #5E5E5E;"># matplotlib used (height, width, color channel)</span></span>
<span id="cb2-10">    <span class="co" style="color: #5E5E5E;"># so we fix it here</span></span>
<span id="cb2-11">    plt.imshow(image_grid.permute(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>).squeeze())</span>
<span id="cb2-12">    plt.show()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Download needs to be set to True the first time you run it.</span></span>
<span id="cb3-2">batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">32</span></span>
<span id="cb3-3">dataloader <span class="op" style="color: #5E5E5E;">=</span> DataLoader(</span>
<span id="cb3-4">    datasets.KMNIST(<span class="st" style="color: #20794D;">'data'</span>, download<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, transform<span class="op" style="color: #5E5E5E;">=</span>transforms.ToTensor()),</span>
<span id="cb3-5">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb3-6">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">image_tensor <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dataloader))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-2">tensor_show(image_tensor)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/index_files/figure-html/cell-5-output-1.png" width="416" height="408"></p>
</div>
</div>
</section>
<section id="the-discriminator" class="level2">
<h2 class="anchored" data-anchor-id="the-discriminator">The Discriminator:</h2>
<blockquote class="blockquote">
<p>The architecture for each block of Discriminator and Generator follows the suggestions from the <a href="https://ar5iv.labs.arxiv.org/html/1511.06434">Deep Convolutional GAN</a> paper.</p>
</blockquote>
<p>The Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol’ linear model, but I will skip a bit to the year 2015, when DCGAN was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A non-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN’s training. We will touch upon tricks to stabilize GAN’s training in the next post).</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">class</span> Discriminator(nn.Module):</span>
<span id="cb5-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, image_channel<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, hidden_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">56</span>):</span>
<span id="cb5-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb5-4">        <span class="va" style="color: #111111;">self</span>.disc <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb5-5">            <span class="va" style="color: #111111;">self</span>.make_disc_block(image_channel, hidden_dim),</span>
<span id="cb5-6">            <span class="va" style="color: #111111;">self</span>.make_disc_block(hidden_dim, hidden_dim),</span>
<span id="cb5-7">            <span class="va" style="color: #111111;">self</span>.make_disc_block(hidden_dim, <span class="dv" style="color: #AD0000;">1</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>),</span>
<span id="cb5-8">        )</span>
<span id="cb5-9"></span>
<span id="cb5-10">    <span class="kw" style="color: #003B4F;">def</span> make_disc_block(<span class="va" style="color: #111111;">self</span>, input_channels, output_channels, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb5-11">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> final_layer:</span>
<span id="cb5-12">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb5-13">                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span>
<span id="cb5-14">                    nn.BatchNorm2d(output_channels),</span>
<span id="cb5-15">                    nn.LeakyReLU(negative_slope<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.25</span>)</span>
<span id="cb5-16">            )</span>
<span id="cb5-17">        <span class="cf" style="color: #003B4F;">else</span>: <span class="co" style="color: #5E5E5E;"># Final Layer</span></span>
<span id="cb5-18">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb5-19">                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)</span>
<span id="cb5-20">            )</span>
<span id="cb5-21">    </span>
<span id="cb5-22">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x: torch.Tensor):</span>
<span id="cb5-23">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.disc(x)</span>
<span id="cb5-24">        <span class="co" style="color: #5E5E5E;"># The input can be a tensor of multiple images</span></span>
<span id="cb5-25">        <span class="co" style="color: #5E5E5E;"># We want to return a tensor with the possibility</span></span>
<span id="cb5-26">        <span class="co" style="color: #5E5E5E;"># of real/fake for each image.</span></span>
<span id="cb5-27">        <span class="cf" style="color: #003B4F;">return</span> x.view(<span class="bu" style="color: null;">len</span>(x), <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
</section>
<section id="the-generator" class="level2">
<h2 class="anchored" data-anchor-id="the-generator">The Generator:</h2>
<p>A point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially <em>distilling</em> the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the <em>transposed convolution</em> operation, which <em>increases</em> the dimensions of data, essentially <em>magnifying</em> the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this <a href="https://github.com/HangenYuu/vision_learner/blob/main/ARCHITECTURE/CNN/Tiny/TinyCNN.ipynb">notebook</a> as my draft.)</p>
<p>First, we need a function to generate noise. Basically, we need some tensor containing random numbers, and we can conveniently return a tensor filled with random numbers from a normal distribution with <code>torch.randn()</code>. For the dimensions, we define argument <code>z_dim</code> as the dimension of the noise input, and <code>n_samples</code> as the number of samples we need.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">def</span> generate_noise(n_samples, z_dim, device<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'cpu'</span>):</span>
<span id="cb6-2">    <span class="cf" style="color: #003B4F;">return</span> torch.randn((n_samples, z_dim), device<span class="op" style="color: #5E5E5E;">=</span>device)</span></code></pre></div>
</div>
<p>For the <code>Generator</code> class, I will also create a function to construct each layer. A non-last layer will have a transposed convolution, followed by batch normalization and ReLU activation. The final layer does not have batch normalization but will have Tanh activation to squish the pixels in range.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;">class</span> Generator(nn.Module):</span>
<span id="cb7-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, z_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">14</span>, image_channel<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, hidden_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">56</span>):</span>
<span id="cb7-3">        <span class="co" style="color: #5E5E5E;"># z_dim is the dimension of the input noise vector</span></span>
<span id="cb7-4">        <span class="va" style="color: #111111;">self</span>.z_dim <span class="op" style="color: #5E5E5E;">=</span> z_dim</span>
<span id="cb7-5">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb7-6">        <span class="va" style="color: #111111;">self</span>.gen <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb7-7">            <span class="va" style="color: #111111;">self</span>.make_gen_block(z_dim, hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">4</span>),</span>
<span id="cb7-8">            <span class="va" style="color: #111111;">self</span>.make_gen_block(hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">4</span>, hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">2</span>, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb7-9">            <span class="va" style="color: #111111;">self</span>.make_gen_block(hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">2</span>, hidden_dim),</span>
<span id="cb7-10">            <span class="va" style="color: #111111;">self</span>.make_gen_block(hidden_dim, image_channel, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>),</span>
<span id="cb7-11">        )</span>
<span id="cb7-12">    </span>
<span id="cb7-13">    <span class="kw" style="color: #003B4F;">def</span> make_gen_block(<span class="va" style="color: #111111;">self</span>, input_channels, output_channels, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb7-14">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> final_layer:</span>
<span id="cb7-15">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb7-16">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span>
<span id="cb7-17">                nn.BatchNorm2d(output_channels),</span>
<span id="cb7-18">                nn.ReLU()</span>
<span id="cb7-19">            )</span>
<span id="cb7-20">        <span class="cf" style="color: #003B4F;">else</span>: <span class="co" style="color: #5E5E5E;"># Final Layer</span></span>
<span id="cb7-21">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb7-22">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span>
<span id="cb7-23">                nn.Tanh()</span>
<span id="cb7-24">            )</span>
<span id="cb7-25">    <span class="co" style="color: #5E5E5E;"># Recall torch expect an image to be in the form (color channel, height, width).</span></span>
<span id="cb7-26">    <span class="co" style="color: #5E5E5E;"># In a batch, torch expects it to be (no. of images in batch, color channel, height, width)</span></span>
<span id="cb7-27">    <span class="co" style="color: #5E5E5E;"># So we need to transform the noise, originally in (no. of images in batch, input dimension)</span></span>
<span id="cb7-28">    <span class="co" style="color: #5E5E5E;"># to (no. of images in batch, input dimension, 1, 1)</span></span>
<span id="cb7-29">    <span class="co" style="color: #5E5E5E;"># See more here:</span></span>
<span id="cb7-30">    <span class="co" style="color: #5E5E5E;"># https://pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images</span></span>
<span id="cb7-31">    <span class="kw" style="color: #003B4F;">def</span> unsqueeze_noise(<span class="va" style="color: #111111;">self</span>, noise):</span>
<span id="cb7-32">        <span class="cf" style="color: #003B4F;">return</span> noise.view(<span class="bu" style="color: null;">len</span>(noise), <span class="va" style="color: #111111;">self</span>.z_dim, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb7-33"></span>
<span id="cb7-34">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, noise):</span>
<span id="cb7-35">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unsqueeze_noise(noise)</span>
<span id="cb7-36">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.gen(x)</span></code></pre></div>
</div>
</section>
<section id="optimizers-and-criterion" class="level2">
<h2 class="anchored" data-anchor-id="optimizers-and-criterion">Optimizers and Criterion</h2>
<p>Next, we want to define our optimizers (one for each model) and our criterion.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;"># We do not have activation at the output for Discriminator, so the outputs</span></span>
<span id="cb8-2"><span class="co" style="color: #5E5E5E;"># are raw (logits).</span></span>
<span id="cb8-3">criterion <span class="op" style="color: #5E5E5E;">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb8-4">z_dim <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">64</span></span>
<span id="cb8-5">display_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">500</span></span>
<span id="cb8-6">batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1000</span></span>
<span id="cb8-7"><span class="co" style="color: #5E5E5E;"># Learning rate of 0.0002 and beta_1 (momentum term for Adam optimizer) of </span></span>
<span id="cb8-8"><span class="co" style="color: #5E5E5E;"># 0.5 works well for DCGAN, according to the paper (yes, I seriously searched</span></span>
<span id="cb8-9"><span class="co" style="color: #5E5E5E;"># for keyword "learning rate" in the paper)</span></span>
<span id="cb8-10">lr <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.0002</span></span>
<span id="cb8-11">beta_1 <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span> </span>
<span id="cb8-12">beta_2 <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.999</span></span>
<span id="cb8-13"><span class="co" style="color: #5E5E5E;"># Device-agnostic code</span></span>
<span id="cb8-14">device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'cuda'</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cpu'</span></span>
<span id="cb8-15"></span>
<span id="cb8-16"><span class="co" style="color: #5E5E5E;"># You can tranform the image values to be between -1 and 1 (the range of the Tanh activation)</span></span>
<span id="cb8-17">transform <span class="op" style="color: #5E5E5E;">=</span> transforms.Compose([</span>
<span id="cb8-18">    transforms.ToTensor(),</span>
<span id="cb8-19">    transforms.Normalize((<span class="fl" style="color: #AD0000;">0.5</span>,), (<span class="fl" style="color: #AD0000;">0.5</span>,)),</span>
<span id="cb8-20">])</span>
<span id="cb8-21"></span>
<span id="cb8-22">dataloader <span class="op" style="color: #5E5E5E;">=</span> DataLoader(</span>
<span id="cb8-23">    datasets.KMNIST(<span class="st" style="color: #20794D;">'data'</span>, download<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, transform<span class="op" style="color: #5E5E5E;">=</span>transform),</span>
<span id="cb8-24">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb8-25">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">gen <span class="op" style="color: #5E5E5E;">=</span> Generator(z_dim).to(device)</span>
<span id="cb9-2">gen_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(gen.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr, betas<span class="op" style="color: #5E5E5E;">=</span>(beta_1, beta_2))</span>
<span id="cb9-3">disc <span class="op" style="color: #5E5E5E;">=</span> Discriminator().to(device) </span>
<span id="cb9-4">disc_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(disc.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr, betas<span class="op" style="color: #5E5E5E;">=</span>(beta_1, beta_2))</span>
<span id="cb9-5"></span>
<span id="cb9-6"><span class="co" style="color: #5E5E5E;"># You initialize the weights to the normal distribution</span></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;"># with mean 0 and standard deviation 0.02</span></span>
<span id="cb9-8"><span class="co" style="color: #5E5E5E;"># (Yes, the paper said so.)</span></span>
<span id="cb9-9"><span class="kw" style="color: #003B4F;">def</span> weights_init(m):</span>
<span id="cb9-10">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.Conv2d) <span class="kw" style="color: #003B4F;">or</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.ConvTranspose2d):</span>
<span id="cb9-11">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb9-12">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.BatchNorm2d):</span>
<span id="cb9-13">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb9-14">        torch.nn.init.constant_(m.bias, <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb9-15"><span class="co" style="color: #5E5E5E;"># Apply recursively weights_init() according to the docs:</span></span>
<span id="cb9-16"><span class="co" style="color: #5E5E5E;"># https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply</span></span>
<span id="cb9-17">gen <span class="op" style="color: #5E5E5E;">=</span> gen.<span class="bu" style="color: null;">apply</span>(weights_init)</span>
<span id="cb9-18">disc <span class="op" style="color: #5E5E5E;">=</span> disc.<span class="bu" style="color: null;">apply</span>(weights_init)</span></code></pre></div>
</div>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>Okay, now onto training!</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">n_epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100</span></span>
<span id="cb10-2">cur_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="co" style="color: #5E5E5E;"># For visualization purpose</span></span>
<span id="cb10-3">mean_generator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-4">mean_discriminator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-5"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n_epochs):</span>
<span id="cb10-6">    <span class="cf" style="color: #003B4F;">for</span> real, _ <span class="kw" style="color: #003B4F;">in</span> tqdm(dataloader):</span>
<span id="cb10-7">        cur_batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(real)</span>
<span id="cb10-8">        real <span class="op" style="color: #5E5E5E;">=</span> real.to(device)</span>
<span id="cb10-9"></span>
<span id="cb10-10">        <span class="co" style="color: #5E5E5E;">## Update Discriminator</span></span>
<span id="cb10-11"></span>
<span id="cb10-12">        <span class="co" style="color: #5E5E5E;"># Empty the optimizer</span></span>
<span id="cb10-13">        disc_opt.zero_grad()</span>
<span id="cb10-14"></span>
<span id="cb10-15">        <span class="co" style="color: #5E5E5E;"># Generate noise and pass through Discriminator for fake examples</span></span>
<span id="cb10-16">        fake_noise <span class="op" style="color: #5E5E5E;">=</span> generate_noise(cur_batch_size, z_dim, device<span class="op" style="color: #5E5E5E;">=</span>device)</span>
<span id="cb10-17">        fake <span class="op" style="color: #5E5E5E;">=</span> gen(fake_noise)</span>
<span id="cb10-18">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake.detach())</span>
<span id="cb10-19"></span>
<span id="cb10-20">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb10-21">        disc_fake_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span>
<span id="cb10-22"></span>
<span id="cb10-23">        <span class="co" style="color: #5E5E5E;"># Same for real examples</span></span>
<span id="cb10-24">        disc_real_pred <span class="op" style="color: #5E5E5E;">=</span> disc(real)</span>
<span id="cb10-25">        disc_real_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span>
<span id="cb10-26"></span>
<span id="cb10-27">        <span class="co" style="color: #5E5E5E;"># The Discriminator's loss is the average of the two</span></span>
<span id="cb10-28">        disc_loss <span class="op" style="color: #5E5E5E;">=</span> (disc_fake_loss <span class="op" style="color: #5E5E5E;">+</span> disc_real_loss) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb10-29"></span>
<span id="cb10-30">        <span class="co" style="color: #5E5E5E;"># Keep track of the average Discriminator loss</span></span>
<span id="cb10-31">        mean_discriminator_loss <span class="op" style="color: #5E5E5E;">+=</span> disc_loss.item() <span class="op" style="color: #5E5E5E;">/</span> display_step</span>
<span id="cb10-32"></span>
<span id="cb10-33">        <span class="co" style="color: #5E5E5E;"># Update Discriminator's gradients a.k.a backpropagation</span></span>
<span id="cb10-34">        <span class="co" style="color: #5E5E5E;"># Normally don't set retain_graph=True, but we do so for GAN</span></span>
<span id="cb10-35">        <span class="co" style="color: #5E5E5E;"># as we need to propagate through the graph a second time</span></span>
<span id="cb10-36">        <span class="co" style="color: #5E5E5E;"># when updating the Generator.</span></span>
<span id="cb10-37">        disc_loss.backward(retain_graph<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb10-38"></span>
<span id="cb10-39">        <span class="co" style="color: #5E5E5E;"># Update Discriminator's optimizer</span></span>
<span id="cb10-40">        disc_opt.step()</span>
<span id="cb10-41"></span>
<span id="cb10-42">        <span class="co" style="color: #5E5E5E;">## Update Generator</span></span>
<span id="cb10-43"></span>
<span id="cb10-44">        <span class="co" style="color: #5E5E5E;"># Empty the optimizer</span></span>
<span id="cb10-45">        gen_opt.zero_grad()</span>
<span id="cb10-46"></span>
<span id="cb10-47">        <span class="co" style="color: #5E5E5E;"># Generate noise and pass through Discriminator for fake examples</span></span>
<span id="cb10-48">        fake_noise_2 <span class="op" style="color: #5E5E5E;">=</span> generate_noise(cur_batch_size, z_dim, device<span class="op" style="color: #5E5E5E;">=</span>device)</span>
<span id="cb10-49">        fake_2 <span class="op" style="color: #5E5E5E;">=</span> gen(fake_noise_2)</span>
<span id="cb10-50">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake_2)</span>
<span id="cb10-51"></span>
<span id="cb10-52">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb10-53">        gen_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span>
<span id="cb10-54"></span>
<span id="cb10-55">        <span class="co" style="color: #5E5E5E;"># Backpropagation for Generator's loss</span></span>
<span id="cb10-56">        gen_loss.backward()</span>
<span id="cb10-57"></span>
<span id="cb10-58">        <span class="co" style="color: #5E5E5E;"># Update Generator's optimizer</span></span>
<span id="cb10-59">        gen_opt.step()</span>
<span id="cb10-60"></span>
<span id="cb10-61">        <span class="co" style="color: #5E5E5E;"># Keep track of the average Generator loss</span></span>
<span id="cb10-62">        mean_generator_loss <span class="op" style="color: #5E5E5E;">+=</span> gen_loss.item() <span class="op" style="color: #5E5E5E;">/</span> display_step</span>
<span id="cb10-63"></span>
<span id="cb10-64">        <span class="co" style="color: #5E5E5E;">## Visualization code</span></span>
<span id="cb10-65">        <span class="cf" style="color: #003B4F;">if</span> cur_step <span class="op" style="color: #5E5E5E;">%</span> display_step <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span> <span class="kw" style="color: #003B4F;">and</span> cur_step <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb10-66">            <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, step </span><span class="sc" style="color: #5E5E5E;">{</span>cur_step<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">: Generator loss: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_generator_loss<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, Discriminator loss: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_discriminator_loss<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb10-67">            tensor_show(fake)</span>
<span id="cb10-68">            tensor_show(real)</span>
<span id="cb10-69">            mean_generator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-70">            mean_discriminator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-71">        cur_step <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
</section>
</section>
<section id="results" class="level1">
<h1>Results:</h1>
<p>After training for 100 epochs with the settings, I received the final results. <img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/Screenshot%202023-01-23%20175133.png" class="img-fluid" alt="The above were the generated images, and the below were the real images."></p>
<p>One definitely can still discern which kanji characters were real and which were fake. However, one must admit that the model did manage to learn some of the intricate features of Japanese calligraphy. Then, after another 100 epochs… <img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/Screenshot%202023-01-23%20183057.png" class="img-fluid"></p>
<p>While it is still not yet indistinguishables, the Generator has gotten very close. The third image in the first row or the one at bottom left corner could definitely be passed off as real ones.</p>
</section>
<section id="end-of-part-2" class="level1">
<h1>End of part 2</h1>
<p>In this part, I have walked you through the training process and the building blocks of GANS. We have trained and witness good results from a simple model on the KMNIST dataset. In the next part, we will continue with the development of GANs, namely different ways to make training more stable. Up to now, it seemed really easy to achieve relatively good result with GANs (to be honest, we haven’t tried anything too complicated, too), but it will much harder when we touch upon larger models. See you then.</p>
<p><em>All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the <a href="https://ar5iv.labs.arxiv.org/">tool</a>. To change to the abstract page, follow this example:</em> <code>https://ar5iv.labs.arxiv.org/html/1409.1556</code> → <code>https://arxiv.org/abs/1409.1556</code>.</p>


</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan_p2/index.html</guid>
  <pubDate>Sun, 22 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-1.png" medium="image" type="image/png" height="44" width="144"/>
</item>
<item>
  <title>A Primer on Generative Adversarial Networks (GANs)</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan_p1/index.html</link>
  <description><![CDATA[ 




<p>If you have studied deep learning before, you will notice that we will encounter classification many times. To be honest, it is fun in a way, having your own model to classify anime characters. Alas, it is a bit dry to me. Intelligence, for me, is creativity, the ability to create something <em>new</em>. I want a model that can create, especially work of art. That led me right to GANs, not so much a model but an elegant way of thinking.</p>
<section id="a-brief-history-of-gans" class="level1">
<h1>A brief history of GANs</h1>
<p><em>For a fuller account, check out the <a href="https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/">MIT Technology Review article</a>.</em></p>
<p>Back in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from <a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a> to <a href="https://ar5iv.labs.arxiv.org/html/1409.1556">VGG</a>. (Not to mention <a href="https://ar5iv.labs.arxiv.org/html/1512.03385">ResNet</a> in 2015, an architecture with so interesting an idea that I had to <a href="https://github.com/HangenYuu/vision_learner/tree/main/ARCHITECTURE/CNN/Paper">make a project</a> for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, <a href="https://ar5iv.labs.arxiv.org/html/1406.2661">Generative Adversarial Nets</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/Screenshot%202023-01-20%20at%2019-53-43%20Generative%20Dog%20Images%20Kaggle.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The image was not from the era, but was representative of what you got from the model at that time (and still now with GANs, if your model was trained poorly or prematurely). <a href="https://www.kaggle.com/c/generative-dog-images/discussion/97753">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>Now I wanted to make two quick detours before going into the inside of GANs:</p>
<ol type="1">
<li>At its core sense, a <em>function</em> is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is <em>trying to update its parameters such that the model will approximate the optimal function as closely as possible</em>. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.</li>
<li>Advances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.</li>
</ol>
</section>
<section id="the-gans-game" class="level1">
<h1>The GANs game:</h1>
<p><strong>Note:</strong> I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.</p>
<p>The word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called <em>Generator</em>, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called <em>Discriminator</em>, (or <em>Critic</em>, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Generator</strong></th>
<th><strong>Discriminator</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>Random numbers</td>
<td>Images (real &amp; generated)</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Images</td>
<td>Class of image (binary)</td>
</tr>
<tr class="odd">
<td><strong>Role</strong></td>
<td>Forger</td>
<td>Appraiser</td>
</tr>
</tbody>
</table>
<p><strong>Quick detour:</strong> the GAN concept advances generative AI the same way backpropagation does so. The approach of trying to know the distribution of the image features was right, but the method was wrong a.k.a too complex and computationally expensive. With GAN, we have an elegant way to start with any random distribution while moving towards the optimal distribution incrementally. No need to know everything any more.</p>
<p>Our loss function will be the good ol’ binary cross-entropy: <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)%20=%20-%5Cfrac%7B1%7D%7Bm%7D*%5By%5E%7B(i)%7Dlog(h(x%5E%7B(i)%7D,%20%5Ctheta))%20+%20(1%20-%20y%5E%7B(i)%7D)log(1%20-%20(h(x%5E%7B(i)%7D,%20%5Ctheta)))%5D"></p>
<p>That surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know <img src="https://latex.codecogs.com/png.latex?y%5E%7B(i)%7D"> is the true label of the i<sup>th</sup> example (0 or 1), <img src="https://latex.codecogs.com/png.latex?h(x%5E%7B(i)%7D,%20%5Ctheta)"> is the predicted label for the i<sup>th</sup> example with input <img src="https://latex.codecogs.com/png.latex?x%5E%7B(i)%7D"> and parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Define the BCE function</span></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;">def</span> bce(y_true, y_pred):</span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;">return</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">*</span>(y_true<span class="op" style="color: #5E5E5E;">*</span>torch.log(y_pred) <span class="op" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>y_true)<span class="op" style="color: #5E5E5E;">*</span>torch.log(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>y_pred))</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">y_true <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-2">y_pred <span class="op" style="color: #5E5E5E;">=</span> torch.linspace(<span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">plt.figure()</span>
<span id="cb2-5">plt.plot(y_pred, bce(y_true, y_pred), <span class="st" style="color: #20794D;">"o--"</span>)</span>
<span id="cb2-6">plt.xlabel(<span class="st" style="color: #20794D;">"prediction"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb2-7">plt.ylabel(<span class="st" style="color: #20794D;">"loss"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb2-8">plt.grid()</span>
<span id="cb2-9">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-0" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/index_files/figure-html/fig-0-output-1.png" width="605" height="439" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: BCE loss when y = 0</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y_true <span class="op" style="color: #5E5E5E;">=</span> torch.ones(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb3-2">y_pred <span class="op" style="color: #5E5E5E;">=</span> torch.linspace(<span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb3-3">criterion <span class="op" style="color: #5E5E5E;">=</span> torch.nn.BCELoss(reduction<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'none'</span>)</span>
<span id="cb3-4"></span>
<span id="cb3-5">plt.figure()</span>
<span id="cb3-6">plt.plot(y_pred, bce(y_true, y_pred), <span class="st" style="color: #20794D;">"o--"</span>)</span>
<span id="cb3-7">plt.xlabel(<span class="st" style="color: #20794D;">"prediction"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb3-8">plt.ylabel(<span class="st" style="color: #20794D;">"loss"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb3-9">plt.grid()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/index_files/figure-html/fig-1-output-1.png" width="605" height="439" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: BCE loss when y = 1</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>I mentioned that this is a conflict between Generator and Discriminator. For Discriminator, it wants to classify correctly i.e.&nbsp;catch the Generator every time while approve the value of the real images. In other words, it wants to <em>minimize</em> its loss function. For Generator, it wants the reverse i.e.&nbsp;pass a fake as a real to the Discriminator every single time. In other words, it wants to <em>maximize</em> the loss function (of the Discriminator). This leads to the ter <em>minimax game</em> that you may hear some people use to describe GAN.</p>
<p>The game can be considered complete when the Discriminator’s accuracy drops to 50% i.e.&nbsp;it can no longer discern, and essentially has to guess at random for each image. At this, our Generator will become potent enough to fool even us with its <a href="https://thispersondoesnotexist.com/">humans</a> and <a href="https://thiscatdoesnotexist.com/">cats</a>.</p>
</section>
<section id="end-of-part-1" class="level1">
<h1>End of part 1:</h1>
<p>As a primer this is far enough. I will continue on the subject, describing each model’s simplest architecture possible, the process of training one, as well as the difficulty in training GANs. (Training a model is hard enough, now we have two.)</p>
<p><em>All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the <a href="https://ar5iv.labs.arxiv.org/">tool</a>. To change to the abstract page, follow this example:</em> <code>https://ar5iv.labs.arxiv.org/html/1409.1556</code> → <code>https://arxiv.org/abs/1409.1556</code>.</p>


</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan_p1/index.html</guid>
  <pubDate>Fri, 20 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://www.kaggle.com/c/generative-dog-images/discussion/97753" medium="image"/>
</item>
<item>
  <title>Hello World</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/hello_world/index.html</link>
  <description><![CDATA[ 




<section id="up-and-running-with-quarto" class="level2">
<h2 class="anchored" data-anchor-id="up-and-running-with-quarto">Up and running with Quarto!</h2>
<div id="hello-world" class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Hello World!"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Hello World!</code></pre>
</div>
</div>


</section>

 ]]></description>
  <category>new</category>
  <category>code</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/hello_world/index.html</guid>
  <pubDate>Fri, 21 Oct 2022 16:00:00 GMT</pubDate>
</item>
</channel>
</rss>
