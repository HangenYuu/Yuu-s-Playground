<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>H&#39;s notes</title>
<link>https://hangenyuu.github.io/h_notes/index.html</link>
<atom:link href="https://hangenyuu.github.io/h_notes/index.xml" rel="self" type="application/rss+xml"/>
<description>H&#39;s Notes on Deep Learning</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Mon, 27 Feb 2023 16:00:00 GMT</lastBuildDate>
<item>
  <title>A micro AI tool</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/hackathon_report/index.html</link>
  <description><![CDATA[ 




<p>On the weekends of 25-26/02 I had the pleasure(?) of attending the <a href="https://intuition.ieeentu.com/">Intuition</a> hackathon hosted by the NTU branch of IEEE with <a href="https://www.linkedin.com/in/hoang-phan-nhat-8a3892191/">Phan Nhat Hoang</a> a.k.a John Phan. We did not win any prize this time (yes, there was a last time that we won, which deserved a post of it all, but not today). Consider this post the debrief for the two days.</p>
<p>First, here is the <a href="https://intuition-v9-0.devpost.com/project-gallery">link</a> to the gallery of the hackathon. Take some time to browse through it and you will notice that at least half of them mentioned GPT-3. Our project, <a href="https://devpost.com/software/summed-is-all-you-need">SumMed</a>, did, too. And we were not alone. After OpenAI released the APIs for their GPT 3.5 (<code>davinci</code>) and DALL·E 2 model, there swiftly spawned a generation of pico AI start-ups that made use of the platform to build products that bring in good income. This was mentioned in Data Machina’s <a href="https://datamachina.substack.com/p/data-machina-190">Newsletter 190</a>, together with a bag of tools termed “<em>Modern AI Stack</em>” by Carlos.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/hackathon_report/Data Machina 190.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Here is the full list for those who wonder. <a href="https://datamachina.substack.com/p/data-machina-190">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>It was amazing how quickly people in tech caught on to something interesting. Or perhaps it was the ability to turn almost everything into interesting stuff. Anyway, I want to mention the newsletter first because it was our first mistake. We were not up with the news. Had only we known more about the trend in the field, we could have utilized more tools to save the work. As we were about to see, the biggest regret would be the front-end, which Hoang spent most of his time to write with React.js, while <a href="https://devpost.com/software/archmed">another team</a> accomplished nearly the same thing and some more with <a href="https://streamlit.io/">Streamlit</a>. And it was also worth mentioning that neither of us know how to use Streamlit - Hoang fell into React.js out of habit. And we just straight up focused on OpenAI technology instead of considering others, with two worth mentioning being <a href="https://huggingface.co/">HuggingFace</a> and <a href="https://www.hpc-ai.tech/blog/colossal-ai-chatgpt">Colossal-AI</a>. There was no time, and we were not knowledgeable enough to utilize the tools.</p>
<p>Before moving on, it is worth mentioning that the “mistake” I wrote above needs reading as “mistake in the context of a hackathon”. When you are in such a rush (&lt;24 hours) and you are not a master learner who can acquire tools and integrate in the project at will (yet), you will need to prepare everything way before the event. I did not do that, because these skills were not the highest in my long-term priority yet (guess so for Hoang). A hackathon seemed big and important on the resume (especially when you are deep into it and do not have any sleep for the past 24 hours), but the long-term vision is always more important and always comes first.</p>
<p>Now that is enough rambling. Onto the actual stuff.</p>
<section id="before-summed" class="level1">
<h1>Before SumMed</h1>
<p>The hackathon was divided into 3 <a href="https://intuition.ieeentu.com/#tracks">tracks</a>: Healthcare track, FinTech track, and an Open track that also cover the two. We chose Healthcare track, with the lengthier problem statement.</p>
<p><img src="https://hangenyuu.github.io/h_notes/posts/hackathon_report/iNTUition v9.0.png" class="img-fluid"></p>
<p>The requirement was clear: zoom onto “an AI tool that can automatically convert research articles into multimodal formats such as PowerPoint, blogs, and infographic posters.” Anybody who caught wind of GPT-3 would think about calling an API together with the paper content to retrieve various summaries for the parts of the paper and create stuff (slides, infographic, or blog post) from them. Well, such was the majority of the submissions. For some reason, we got tunnel vision, did not realize this, and got stuck with a project that resembled everybody else. Eventually, the selection for winner became the selection for the prototype that was closer to the requirements (<a href="https://github.com/jiawen3131/Hacknwhack">here</a>).</p>
<p>Back to our product, it all started some time ago when Hoang introduced me to the concept of DocumentQA. This started with the discovery of the model to reason <em>in-context</em>. This is something that is unique to large language models (LLMs). Simply put, if we feed the model a context i.e.&nbsp;background information <em>that it has never seen before</em> together with a format of conversation that we desire, the model can immediately adapt to the format we want, and use the background information as the main source of knowledge to answer our prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/hackathon_report/GPT 3 SQuADv2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>An example from the <a href="https://arxiv.org/pdf/2005.14165.pdf">GPT-3 research paper</a></em></figcaption><p></p>
</figure>
</div>
<p>This means that we can take a pre-trained LLM, which will act as a <em>document reader</em> and augment it with a <em>document retriever</em> to form a DocumentQA pipeline. You ask, the retriever performs preliminary search and takes out the relevant one to feed into the reader together with your question, and the reader answers after reading the document. A most notable example is <a href="https://github.com/facebookresearch/DrQA">DrQA</a>. For the particular case of OpenAI <a href="https://platform.openai.com/docs/model-index-for-researchers">GPT-3.5</a> (<code>text-davinci-003</code>), there exists two applications available as retriever for the model: <a href="https://langchain.readthedocs.io/en/latest/">LangChain</a> and <a href="https://gpt-index.readthedocs.io/en/latest/">LlamaIndex (GPT Index)</a>. We started simply with a Discord chatbot that used LlamaIndex to read an attachment (PDF, HTML, etc.) and answer a question that you send. I have not created a GitHub repo for it, but here is the <a href="https://replit.com/@HangenYuu/PoliteWavyReciprocal">Repl</a>.</p>
<p>Because of this toy project, we got tunnel vision into creating a chatbot for QA over a research paper, which was far from the point. We shifted gear after a Dr.&nbsp;from MSD set me straight about the project, and came up with SumMed.</p>
</section>
<section id="enter-summed" class="level1">
<h1>Enter SumMed</h1>
<p>SumMed supported 3 features:</p>
<ol type="1">
<li>Extract and display key information about a research paper.</li>
<li>Extract and display all tables, figures, and charts from a research paper.</li>
<li>Of course, a chatbot for QA over a researcher paper.</li>
</ol>
<p>The diagram of the application is simple</p>
<p><img src="https://hangenyuu.github.io/h_notes/posts/hackathon_report/SumMed diagram.png" class="img-fluid"></p>
<p>Hoang was in charge of the intricate detail of the front-end and Flask app, which could be viewed in client folder of repo (again, <a href="https://github.com/JohnToro-CZAF/MedSum/tree/main/client">here</a>). I was in charge of the model part of the back-end, and I was not a React.js pro, would not try to show you what Hoang had done. Instead, I will walk you through the back-end models: GPT-3.5 and Detectron2.</p>
<blockquote class="blockquote">
<p>Note: Apparently, catching wind of this blog post, Hoang has refactored the codes. The codes in the post are <code>DocLayout.py</code>, <code>DocReader.py</code>, <code>DocSummarizer.py</code> in the <code>server</code> folder.</p>
</blockquote>
<section id="doclayout.py" class="level2">
<h2 class="anchored" data-anchor-id="doclayout.py"><code>DocLayout.py</code></h2>
<p>First, the whole file:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pdf2image</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> layoutparser <span class="im" style="color: #00769E;">as</span> lp</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> collections <span class="im" style="color: #00769E;">import</span> defaultdict</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="kw" style="color: #003B4F;">class</span> DocLayout(<span class="bu" style="color: null;">object</span>):</span>
<span id="cb1-7">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb1-8">        <span class="va" style="color: #111111;">self</span>.model <span class="op" style="color: #5E5E5E;">=</span> lp.Detectron2LayoutModel(<span class="st" style="color: #20794D;">'lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config'</span>,</span>
<span id="cb1-9">                                    extra_config<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"MODEL.ROI_HEADS.SCORE_THRESH_TEST"</span>, <span class="fl" style="color: #AD0000;">0.5</span>],</span>
<span id="cb1-10">                                    label_map<span class="op" style="color: #5E5E5E;">=</span>{<span class="dv" style="color: #AD0000;">0</span>: <span class="st" style="color: #20794D;">"Text"</span>, <span class="dv" style="color: #AD0000;">1</span>: <span class="st" style="color: #20794D;">"Title"</span>, <span class="dv" style="color: #AD0000;">2</span>: <span class="st" style="color: #20794D;">"List"</span>, <span class="dv" style="color: #AD0000;">3</span>:<span class="st" style="color: #20794D;">"Table"</span>, <span class="dv" style="color: #AD0000;">4</span>:<span class="st" style="color: #20794D;">"Figure"</span>})</span>
<span id="cb1-11">        <span class="va" style="color: #111111;">self</span>.ocr_agent <span class="op" style="color: #5E5E5E;">=</span> lp.TesseractAgent(languages<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'eng'</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13">    <span class="kw" style="color: #003B4F;">def</span> extract_pdf(<span class="va" style="color: #111111;">self</span>, file_name: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb1-14">        <span class="co" style="color: #5E5E5E;">""" From a local file pdf file, extract the title, text, tables and figures</span></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;">        Args:</span></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;">            file_name (str): path to the pdf file</span></span>
<span id="cb1-17"><span class="co" style="color: #5E5E5E;">        Returns:</span></span>
<span id="cb1-18"><span class="co" style="color: #5E5E5E;">            title (str): title of the paper</span></span>
<span id="cb1-19"><span class="co" style="color: #5E5E5E;">            Paper (str): text of the paper</span></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;">            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array</span></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;">            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array</span></span>
<span id="cb1-22"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb1-23">        list_of_pages <span class="op" style="color: #5E5E5E;">=</span> pdf2image.convert_from_path(file_name)</span>
<span id="cb1-24">        images <span class="op" style="color: #5E5E5E;">=</span> [np.asarray(page) <span class="cf" style="color: #003B4F;">for</span> page <span class="kw" style="color: #003B4F;">in</span> list_of_pages]</span>
<span id="cb1-25">        image_width <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(images[<span class="dv" style="color: #AD0000;">0</span>][<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb1-26"></span>
<span id="cb1-27">        header_blocks, text_blocks, table_blocks, figure_blocks <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._detect_element(images)</span>
<span id="cb1-28"></span>
<span id="cb1-29">        title <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_title(image_width, images, header_blocks)</span>
<span id="cb1-30">        Paper <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_text_info(image_width, images, text_blocks)</span>
<span id="cb1-31">        table_by_page, figure_by_page <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)</span>
<span id="cb1-32">        <span class="co" style="color: #5E5E5E;"># Currently we dont care about the order of the figures or tables returned</span></span>
<span id="cb1-33">        tables <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._general_by_table_to_list(table_by_page)</span>
<span id="cb1-34">        figures <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._general_by_table_to_list(figure_by_page)</span>
<span id="cb1-35">        <span class="cf" style="color: #003B4F;">return</span> title, Paper, tables, figures</span>
<span id="cb1-36">    </span>
<span id="cb1-37">    <span class="kw" style="color: #003B4F;">def</span> _general_by_table_to_list(<span class="va" style="color: #111111;">self</span>, general_by_page: <span class="bu" style="color: null;">dict</span>):</span>
<span id="cb1-38">        <span class="cf" style="color: #003B4F;">return</span> [general <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> general_by_page.keys() <span class="cf" style="color: #003B4F;">for</span> general <span class="kw" style="color: #003B4F;">in</span> general_by_page[i]]</span>
<span id="cb1-39">    </span>
<span id="cb1-40">    <span class="kw" style="color: #003B4F;">def</span> _detect_element(<span class="va" style="color: #111111;">self</span>, images):</span>
<span id="cb1-41">        types <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'Title'</span>, <span class="st" style="color: #20794D;">'Text'</span>, <span class="st" style="color: #20794D;">'Table'</span>, <span class="st" style="color: #20794D;">'Figure'</span>]</span>
<span id="cb1-42">        type_blocks <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb1-43">            t: defaultdict(<span class="bu" style="color: null;">list</span>) <span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> types</span>
<span id="cb1-44">        }</span>
<span id="cb1-45">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(images)):</span>
<span id="cb1-46">            layout_result <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.model.detect(images[i])</span>
<span id="cb1-47">            <span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> types:</span>
<span id="cb1-48">                type_block <span class="op" style="color: #5E5E5E;">=</span> lp.Layout([b <span class="cf" style="color: #003B4F;">for</span> b <span class="kw" style="color: #003B4F;">in</span> layout_result <span class="cf" style="color: #003B4F;">if</span> b.<span class="bu" style="color: null;">type</span><span class="op" style="color: #5E5E5E;">==</span>t])</span>
<span id="cb1-49">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(type_block) <span class="op" style="color: #5E5E5E;">!=</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb1-50">                    type_blocks[t][i] <span class="op" style="color: #5E5E5E;">=</span> type_block</span>
<span id="cb1-51">        <span class="cf" style="color: #003B4F;">return</span> type_blocks.values()</span>
<span id="cb1-52">    </span>
<span id="cb1-53">    </span>
<span id="cb1-54">    <span class="kw" style="color: #003B4F;">def</span> _extract_title(<span class="va" style="color: #111111;">self</span>, image_width, images, header_blocks):</span>
<span id="cb1-55">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb1-56"><span class="co" style="color: #5E5E5E;">        Extract the title of the article from several headers</span></span>
<span id="cb1-57"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb1-58">        first_page <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">min</span>(header_blocks.keys())</span>
<span id="cb1-59">        segment_title <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_page(first_page, image_width, images, header_blocks)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb1-60">        title <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.ocr_agent.detect(segment_title)</span>
<span id="cb1-61">        <span class="cf" style="color: #003B4F;">return</span> title</span>
<span id="cb1-62">    </span>
<span id="cb1-63">    <span class="kw" style="color: #003B4F;">def</span> _extract_text_info(<span class="va" style="color: #111111;">self</span>, image_width, images, text_blocks):</span>
<span id="cb1-64">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb1-65"><span class="co" style="color: #5E5E5E;">        Returns all the text in the article</span></span>
<span id="cb1-66"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb1-67">        Paper <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb1-68">        <span class="cf" style="color: #003B4F;">for</span> page_id <span class="kw" style="color: #003B4F;">in</span> text_blocks:</span>
<span id="cb1-69">            text_block_images <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_page(page_id, image_width, images, text_blocks)</span>
<span id="cb1-70">            <span class="cf" style="color: #003B4F;">for</span> block <span class="kw" style="color: #003B4F;">in</span> text_block_images:</span>
<span id="cb1-71">                text <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.ocr_agent.detect(block).strip()</span>
<span id="cb1-72">                Paper <span class="op" style="color: #5E5E5E;">+=</span> text <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">" "</span></span>
<span id="cb1-73">        <span class="cf" style="color: #003B4F;">return</span> Paper</span>
<span id="cb1-74"></span>
<span id="cb1-75">    <span class="kw" style="color: #003B4F;">def</span> _extract_table_n_figure(<span class="va" style="color: #111111;">self</span>, image_width, images, table_blocks, figure_blocks):</span>
<span id="cb1-76">        <span class="co" style="color: #5E5E5E;">"""Extract 3D numpy array of tables and figures from deteced layout</span></span>
<span id="cb1-77"><span class="co" style="color: #5E5E5E;">        Args:</span></span>
<span id="cb1-78"><span class="co" style="color: #5E5E5E;">            image_width (int): width of image</span></span>
<span id="cb1-79"><span class="co" style="color: #5E5E5E;">            images (_type_): _description_</span></span>
<span id="cb1-80"><span class="co" style="color: #5E5E5E;">            table_blocks (_type_): _description_</span></span>
<span id="cb1-81"><span class="co" style="color: #5E5E5E;">            figure_blocks (_type_): _description_</span></span>
<span id="cb1-82"><span class="co" style="color: #5E5E5E;">        Returns:</span></span>
<span id="cb1-83"><span class="co" style="color: #5E5E5E;">            table_by_page, figure_by_page (dict(list)): 3D numpy array of tables and figures by page</span></span>
<span id="cb1-84"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb1-85">        </span>
<span id="cb1-86">        table_by_page, figure_by_page <span class="op" style="color: #5E5E5E;">=</span> defaultdict(<span class="bu" style="color: null;">list</span>), defaultdict(<span class="bu" style="color: null;">list</span>)</span>
<span id="cb1-87">        <span class="cf" style="color: #003B4F;">for</span> page_id <span class="kw" style="color: #003B4F;">in</span> table_blocks:</span>
<span id="cb1-88">            results <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_page(page_id, image_width, images, table_blocks )</span>
<span id="cb1-89">            table_by_page[page_id] <span class="op" style="color: #5E5E5E;">=</span> results</span>
<span id="cb1-90">        </span>
<span id="cb1-91">        <span class="cf" style="color: #003B4F;">for</span> page_id <span class="kw" style="color: #003B4F;">in</span> figure_blocks:</span>
<span id="cb1-92">            results <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_page(page_id, image_width, images, figure_blocks)</span>
<span id="cb1-93">            figure_by_page[page_id] <span class="op" style="color: #5E5E5E;">=</span> results</span>
<span id="cb1-94">        </span>
<span id="cb1-95">        <span class="cf" style="color: #003B4F;">return</span> table_by_page, figure_by_page</span>
<span id="cb1-96"></span>
<span id="cb1-97">    <span class="kw" style="color: #003B4F;">def</span> _extract_page(<span class="va" style="color: #111111;">self</span>, page_id, image_width, images, general_blocks):</span>
<span id="cb1-98">        <span class="co" style="color: #5E5E5E;">""" </span></span>
<span id="cb1-99"><span class="co" style="color: #5E5E5E;">        Get a list of 3D array numpy image of tables and figures, or text from a page</span></span>
<span id="cb1-100"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb1-101">        results <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb1-102">        left_interval <span class="op" style="color: #5E5E5E;">=</span> lp.Interval(<span class="dv" style="color: #AD0000;">0</span>, image_width<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span>, axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'x'</span>).put_on_canvas(images[page_id])</span>
<span id="cb1-103">        left_blocks <span class="op" style="color: #5E5E5E;">=</span> general_blocks[page_id].filter_by(left_interval, center<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)._blocks</span>
<span id="cb1-104">        left_blocks.sort(key <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> b: b.coordinates[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb1-105"></span>
<span id="cb1-106">        <span class="co" style="color: #5E5E5E;"># Sort element ID of the right column based on y1 coordinate</span></span>
<span id="cb1-107">        right_blocks <span class="op" style="color: #5E5E5E;">=</span> [b <span class="cf" style="color: #003B4F;">for</span> b <span class="kw" style="color: #003B4F;">in</span> general_blocks[page_id] <span class="cf" style="color: #003B4F;">if</span> b <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> left_blocks]</span>
<span id="cb1-108">        right_blocks.sort(key <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> b: b.coordinates[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb1-109"></span>
<span id="cb1-110">        <span class="co" style="color: #5E5E5E;"># Sort the overall element ID starts from left column</span></span>
<span id="cb1-111">        general_block <span class="op" style="color: #5E5E5E;">=</span> lp.Layout([b.<span class="bu" style="color: null;">set</span>(<span class="bu" style="color: null;">id</span> <span class="op" style="color: #5E5E5E;">=</span> idx) <span class="cf" style="color: #003B4F;">for</span> idx, b <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(left_blocks <span class="op" style="color: #5E5E5E;">+</span> right_blocks)])</span>
<span id="cb1-112"></span>
<span id="cb1-113">        <span class="co" style="color: #5E5E5E;"># Crop image around the detected layout</span></span>
<span id="cb1-114">        <span class="cf" style="color: #003B4F;">for</span> block <span class="kw" style="color: #003B4F;">in</span> general_block:</span>
<span id="cb1-115">            segment_image <span class="op" style="color: #5E5E5E;">=</span> (block</span>
<span id="cb1-116">                                .pad(left<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>, right<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">15</span>, top<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, bottom<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb1-117">                                .crop_image(images[page_id]))</span>
<span id="cb1-118">            results.append(segment_image)</span>
<span id="cb1-119"></span>
<span id="cb1-120">        <span class="cf" style="color: #003B4F;">return</span> results</span></code></pre></div>
<p>Let’s dissect the codes.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> pdf2image</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb2-3"><span class="im" style="color: #00769E;">import</span> layoutparser <span class="im" style="color: #00769E;">as</span> lp</span>
<span id="cb2-4"><span class="im" style="color: #00769E;">from</span> collections <span class="im" style="color: #00769E;">import</span> defaultdict</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;">class</span> DocLayout(<span class="bu" style="color: null;">object</span>):</span>
<span id="cb2-7">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb2-8">        <span class="va" style="color: #111111;">self</span>.model <span class="op" style="color: #5E5E5E;">=</span> lp.Detectron2LayoutModel(<span class="st" style="color: #20794D;">'lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config'</span>,</span>
<span id="cb2-9">                                    extra_config<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"MODEL.ROI_HEADS.SCORE_THRESH_TEST"</span>, <span class="fl" style="color: #AD0000;">0.5</span>],</span>
<span id="cb2-10">                                    label_map<span class="op" style="color: #5E5E5E;">=</span>{<span class="dv" style="color: #AD0000;">0</span>: <span class="st" style="color: #20794D;">"Text"</span>, <span class="dv" style="color: #AD0000;">1</span>: <span class="st" style="color: #20794D;">"Title"</span>, <span class="dv" style="color: #AD0000;">2</span>: <span class="st" style="color: #20794D;">"List"</span>, <span class="dv" style="color: #AD0000;">3</span>:<span class="st" style="color: #20794D;">"Table"</span>, <span class="dv" style="color: #AD0000;">4</span>:<span class="st" style="color: #20794D;">"Figure"</span>})</span>
<span id="cb2-11">        <span class="va" style="color: #111111;">self</span>.ocr_agent <span class="op" style="color: #5E5E5E;">=</span> lp.TesseractAgent(languages<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'eng'</span>)</span></code></pre></div>
<p>The star of the show is <a href="https://layout-parser.readthedocs.io/en/latest/">LayoutParser</a> module, which employs a host of models from the <a href="https://github.com/facebookresearch/detectron2">Detectron2 platform</a> for the task of document layout parsing. We used the best configuration suggested by the <a href="https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html">docs</a> of <a href="https://arxiv.org/pdf/1703.06870.pdf">Mark RCNN</a> trained on the <a href="https://arxiv.org/pdf/1908.07836.pdf">PubLayNet</a> dataset of document layout analysis. As you can see, the model in this case can detect 5 elements in the dictionary <code>{0: "Text", 1: "Title", 2: "List", 3:"Table", 4:"Figure"}</code>. <code>pdf2image</code> and an <code>ocr_agent</code> needs importing and creating respectively because the model works on images, so we need to convert the PDF file to image(s) i.e.&nbsp;NumPy array(s) before doing anything.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">class</span> DocLayout(<span class="bu" style="color: null;">object</span>):</span>
<span id="cb3-2">    <span class="kw" style="color: #003B4F;">def</span> extract_pdf(<span class="va" style="color: #111111;">self</span>, file_name: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb3-3">        <span class="co" style="color: #5E5E5E;">""" From a local file pdf file, extract the title, text, tables and figures</span></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;">        Args:</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;">            file_name (str): path to the pdf file</span></span>
<span id="cb3-6"><span class="co" style="color: #5E5E5E;">        Returns:</span></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;">            title (str): title of the paper</span></span>
<span id="cb3-8"><span class="co" style="color: #5E5E5E;">            Paper (str): text of the paper</span></span>
<span id="cb3-9"><span class="co" style="color: #5E5E5E;">            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array</span></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;">            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array</span></span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb3-12">        list_of_pages <span class="op" style="color: #5E5E5E;">=</span> pdf2image.convert_from_path(file_name)</span>
<span id="cb3-13">        images <span class="op" style="color: #5E5E5E;">=</span> [np.asarray(page) <span class="cf" style="color: #003B4F;">for</span> page <span class="kw" style="color: #003B4F;">in</span> list_of_pages]</span>
<span id="cb3-14">        image_width <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(images[<span class="dv" style="color: #AD0000;">0</span>][<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb3-15"></span>
<span id="cb3-16">        header_blocks, text_blocks, table_blocks, figure_blocks <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._detect_element(images)</span>
<span id="cb3-17"></span>
<span id="cb3-18">        title <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_title(image_width, images, header_blocks)</span>
<span id="cb3-19">        Paper <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_text_info(image_width, images, text_blocks)</span>
<span id="cb3-20">        table_by_page, figure_by_page <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)</span>
<span id="cb3-21">        <span class="co" style="color: #5E5E5E;"># Currently we dont care about the order of the figures or tables returned</span></span>
<span id="cb3-22">        tables <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._general_by_table_to_list(table_by_page)</span>
<span id="cb3-23">        figures <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._general_by_table_to_list(figure_by_page)</span>
<span id="cb3-24">        <span class="cf" style="color: #003B4F;">return</span> title, Paper, tables, figures</span>
<span id="cb3-25"></span>
<span id="cb3-26">    <span class="kw" style="color: #003B4F;">def</span> _detect_element(<span class="va" style="color: #111111;">self</span>, images):</span>
<span id="cb3-27">        types <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'Title'</span>, <span class="st" style="color: #20794D;">'Text'</span>, <span class="st" style="color: #20794D;">'Table'</span>, <span class="st" style="color: #20794D;">'Figure'</span>]</span>
<span id="cb3-28">        type_blocks <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb3-29">            t: defaultdict(<span class="bu" style="color: null;">list</span>) <span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> types</span>
<span id="cb3-30">        }</span>
<span id="cb3-31">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(images)):</span>
<span id="cb3-32">            layout_result <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.model.detect(images[i])</span>
<span id="cb3-33">            <span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> types:</span>
<span id="cb3-34">                type_block <span class="op" style="color: #5E5E5E;">=</span> lp.Layout([b <span class="cf" style="color: #003B4F;">for</span> b <span class="kw" style="color: #003B4F;">in</span> layout_result <span class="cf" style="color: #003B4F;">if</span> b.<span class="bu" style="color: null;">type</span><span class="op" style="color: #5E5E5E;">==</span>t])</span>
<span id="cb3-35">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">len</span>(type_block) <span class="op" style="color: #5E5E5E;">!=</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb3-36">                    type_blocks[t][i] <span class="op" style="color: #5E5E5E;">=</span> type_block</span>
<span id="cb3-37">        <span class="cf" style="color: #003B4F;">return</span> type_blocks.values()</span></code></pre></div>
<p><code>pdf2image.convert_from_path()</code> returns a list of Pillow image, which needs converting to a list of NumPy arrays before work. Afterwards, in <code>_detect_element()</code> method, call <code>sel.model.detect()</code> to return a list of bounding boxes (represent by the top-left and right-bottom coordinates with respect to the particular page) with element type. The list of blocks returned will be processed accordingly.</p>
</section>
<section id="docreader.py" class="level2">
<h2 class="anchored" data-anchor-id="docreader.py"><code>DocReader.py</code></h2>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># There are minor differences (by the time of post) from the file in the repo</span></span>
<span id="cb4-2"><span class="im" style="color: #00769E;">from</span> llama_index  <span class="im" style="color: #00769E;">import</span> SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper</span>
<span id="cb4-3"><span class="im" style="color: #00769E;">from</span> langchain <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="kw" style="color: #003B4F;">class</span> DocReader(<span class="bu" style="color: null;">object</span>):</span>
<span id="cb4-6">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, directory_path, index_path):</span>
<span id="cb4-7">        <span class="va" style="color: #111111;">self</span>.index_path <span class="op" style="color: #5E5E5E;">=</span> index_path</span>
<span id="cb4-8">        <span class="va" style="color: #111111;">self</span>.directory_path <span class="op" style="color: #5E5E5E;">=</span> directory_path</span>
<span id="cb4-9">        <span class="va" style="color: #111111;">self</span>.max_input_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4096</span></span>
<span id="cb4-10">        <span class="va" style="color: #111111;">self</span>.num_outputs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">256</span></span>
<span id="cb4-11">        <span class="va" style="color: #111111;">self</span>.max_chunk_overlap <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">20</span></span>
<span id="cb4-12">        <span class="va" style="color: #111111;">self</span>.chunk_size_limit <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">600</span></span>
<span id="cb4-13">        <span class="va" style="color: #111111;">self</span>.llm_predictor <span class="op" style="color: #5E5E5E;">=</span> LLMPredictor(llm<span class="op" style="color: #5E5E5E;">=</span>OpenAI(temperature<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>, model_name<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"text-davinci-003"</span>, max_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.num_outputs))</span>
<span id="cb4-14">        <span class="va" style="color: #111111;">self</span>.prompt_helper <span class="op" style="color: #5E5E5E;">=</span> PromptHelper(<span class="va" style="color: #111111;">self</span>.max_input_size, <span class="va" style="color: #111111;">self</span>.num_outputs, <span class="va" style="color: #111111;">self</span>.max_chunk_overlap, chunk_size_limit<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.chunk_size_limit)</span>
<span id="cb4-15"></span>
<span id="cb4-16">    <span class="kw" style="color: #003B4F;">def</span> construct_index(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb4-17">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb4-18"><span class="co" style="color: #5E5E5E;">        Reconstruct the index, and save it to the database</span></span>
<span id="cb4-19"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb4-20">        documents <span class="op" style="color: #5E5E5E;">=</span> SimpleDirectoryReader(<span class="va" style="color: #111111;">self</span>.directory_path).load_data()        </span>
<span id="cb4-21">        index <span class="op" style="color: #5E5E5E;">=</span> GPTSimpleVectorIndex(</span>
<span id="cb4-22">            documents, llm_predictor<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.llm_predictor, prompt_helper<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.prompt_helper</span>
<span id="cb4-23">        )</span>
<span id="cb4-24">        index.save_to_disk(<span class="va" style="color: #111111;">self</span>.index_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/index.json'</span>)</span>
<span id="cb4-25"></span>
<span id="cb4-26">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, query):</span>
<span id="cb4-27">        index <span class="op" style="color: #5E5E5E;">=</span> GPTSimpleVectorIndex.load_from_disk(<span class="va" style="color: #111111;">self</span>.index_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/index.json'</span>)</span>
<span id="cb4-28">        response <span class="op" style="color: #5E5E5E;">=</span> index.query(query, response_mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"default"</span>)</span>
<span id="cb4-29">        <span class="cf" style="color: #003B4F;">return</span> response.response</span></code></pre></div>
<p>A LlamaIndex workflow consists of 4 steps:</p>
<ol type="1">
<li>Initialize an <code>LLMPredictor()</code> instance (based on LangChain <a href="https://langchain.readthedocs.io/en/latest/modules/llms.html"><code>LLM</code> and <code>LLMChain</code></a>, which supports many other model hubs besides OpenAI). <code>LLMPredictor()</code> is a wrapper outside the model we use.</li>
<li>Initialize a <a href="https://gpt-index.readthedocs.io/en/latest/reference/prompt_helper.html"><code>PromptHelper</code></a> instance that helps to define various parameters for the prompt.</li>
<li>Index the document. There are many ways to achieve this, but the most simple way is calling <code>SimpleDirectoryReader()</code> to get the documents and <code>GPTSimpleVectorIndex()</code> to get the index that can be saved as a .json file.</li>
<li>Query over the index. There are different, pre-defined response mode in LlamaIndex. Explore the docs for more.</li>
</ol>
<p>And that’s it! Short and simple, yes powerful.</p>
</section>
<section id="docsummarizer.py" class="level2">
<h2 class="anchored" data-anchor-id="docsummarizer.py"><code>DocSummarizer.py</code></h2>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb5-2"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb5-3"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb5-4"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb5-5"><span class="im" style="color: #00769E;">from</span> DocLayout <span class="im" style="color: #00769E;">import</span> DocLayout</span>
<span id="cb5-6"><span class="im" style="color: #00769E;">from</span> collections <span class="im" style="color: #00769E;">import</span> defaultdict</span>
<span id="cb5-7"></span>
<span id="cb5-8"><span class="kw" style="color: #003B4F;">class</span> DocSummarizer(<span class="bu" style="color: null;">object</span>):</span>
<span id="cb5-9">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, documents_path: <span class="bu" style="color: null;">str</span>, resources_path: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb5-10">        <span class="va" style="color: #111111;">self</span>.documents_path <span class="op" style="color: #5E5E5E;">=</span> documents_path</span>
<span id="cb5-11">        <span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">=</span> resources_path</span>
<span id="cb5-12">        <span class="va" style="color: #111111;">self</span>.prompt_tail <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb5-13">            <span class="st" style="color: #20794D;">'authors'</span>: <span class="st" style="color: #20794D;">'Who are the authors of this paper'</span>,</span>
<span id="cb5-14">            <span class="st" style="color: #20794D;">'summary'</span>:<span class="st" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">Summarize the above text, focus on key insights"</span>,</span>
<span id="cb5-15">            <span class="st" style="color: #20794D;">'keyresults'</span>:<span class="st" style="color: #20794D;">'''</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">Give me three key results in the format of "Key results:</span></span>
<span id="cb5-16"><span class="st" style="color: #20794D;">                1.  Key result 1</span></span>
<span id="cb5-17"><span class="st" style="color: #20794D;">                2. Key result 2</span></span>
<span id="cb5-18"><span class="st" style="color: #20794D;">                3. Key result 3"'''</span>,</span>
<span id="cb5-19">            <span class="st" style="color: #20794D;">'keyword'</span>:<span class="st" style="color: #20794D;">'</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">Give me keywords in the format of "Keywords:  Keyword 1, Keyword 2, Keyword 3"'</span>,</span>
<span id="cb5-20">            <span class="st" style="color: #20794D;">'limitations'</span>:<span class="st" style="color: #20794D;">'</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">Give me 3 sentences describing the limitations of the text above.'</span></span>
<span id="cb5-21">        }</span>
<span id="cb5-22">        <span class="va" style="color: #111111;">self</span>.layout_model <span class="op" style="color: #5E5E5E;">=</span> DocLayout()</span>
<span id="cb5-23">        </span>
<span id="cb5-24">    <span class="kw" style="color: #003B4F;">def</span> get_summary(<span class="va" style="color: #111111;">self</span>, file_name: <span class="bu" style="color: null;">str</span>, reader):</span>
<span id="cb5-25">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb5-26"><span class="co" style="color: #5E5E5E;">        Returns a summary of the document, this document is a pdf file that has been uploaded to the server.</span></span>
<span id="cb5-27"><span class="co" style="color: #5E5E5E;">        And save the summary to the database/resources.</span></span>
<span id="cb5-28"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb5-29">        title, Paper, tables, figures <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.layout_model.extract_pdf(<span class="va" style="color: #111111;">self</span>.documents_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name)</span>
<span id="cb5-30">        authors, summary, keywords, keyresults, limitations <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._read(Paper, reader)</span>
<span id="cb5-31">        response <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb5-32">          <span class="st" style="color: #20794D;">'title'</span>: title,</span>
<span id="cb5-33">          <span class="st" style="color: #20794D;">'authors'</span>: authors,</span>
<span id="cb5-34">          <span class="st" style="color: #20794D;">'summary'</span>: summary,</span>
<span id="cb5-35">          <span class="st" style="color: #20794D;">'key_concepts'</span>: keywords,</span>
<span id="cb5-36">          <span class="st" style="color: #20794D;">'highlights'</span>: keyresults,</span>
<span id="cb5-37">          <span class="st" style="color: #20794D;">'limitations'</span>: limitations,</span>
<span id="cb5-38">          <span class="st" style="color: #20794D;">'figures'</span>: [],</span>
<span id="cb5-39">          <span class="st" style="color: #20794D;">'tables'</span>: [],</span>
<span id="cb5-40">        }</span>
<span id="cb5-41">        </span>
<span id="cb5-42">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> os.path.exists(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>]):</span>
<span id="cb5-43">            os.mkdir(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>])</span>
<span id="cb5-44">        </span>
<span id="cb5-45">        <span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/info.json'</span>, <span class="st" style="color: #20794D;">'w'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb5-46">            json.dump(response, f)</span>
<span id="cb5-47">        </span>
<span id="cb5-48">        <span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/title.txt'</span>, <span class="st" style="color: #20794D;">'w'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb5-49">            f.write(title)</span>
<span id="cb5-50">        </span>
<span id="cb5-51">        <span class="cf" style="color: #003B4F;">for</span> idx, table <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(tables):</span>
<span id="cb5-52">            im <span class="op" style="color: #5E5E5E;">=</span> Image.fromarray(table)</span>
<span id="cb5-53">            local_fn <span class="op" style="color: #5E5E5E;">=</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'*'</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="bu" style="color: null;">str</span>(idx) <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'_table.png'</span></span>
<span id="cb5-54">            table_fn <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="bu" style="color: null;">str</span>(idx) <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'_table.png'</span></span>
<span id="cb5-55">            im.save(table_fn)</span>
<span id="cb5-56">            response[<span class="st" style="color: #20794D;">'tables'</span>].append(local_fn)</span>
<span id="cb5-57">        </span>
<span id="cb5-58">        <span class="cf" style="color: #003B4F;">for</span> idx, fig <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(figures):</span>
<span id="cb5-59">            im <span class="op" style="color: #5E5E5E;">=</span> Image.fromarray(fig)</span>
<span id="cb5-60">            local_fn <span class="op" style="color: #5E5E5E;">=</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'*'</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="bu" style="color: null;">str</span>(idx) <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'_fig.png'</span></span>
<span id="cb5-61">            fig_fn <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="bu" style="color: null;">str</span>(idx) <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'_fig.png'</span></span>
<span id="cb5-62">            im.save(fig_fn)</span>
<span id="cb5-63">            response[<span class="st" style="color: #20794D;">'figures'</span>].append(local_fn)</span>
<span id="cb5-64">        </span>
<span id="cb5-65">        <span class="cf" style="color: #003B4F;">return</span> response</span>
<span id="cb5-66"></span>
<span id="cb5-67">    <span class="kw" style="color: #003B4F;">def</span> retrieve_summary(<span class="va" style="color: #111111;">self</span>, file_name: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb5-68">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb5-69"><span class="co" style="color: #5E5E5E;">        Returns a summary of the document (retrieve from resources), this document is a pdf file that already in the server.</span></span>
<span id="cb5-70"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb5-71">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> os.path.exists(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>]):</span>
<span id="cb5-72">            <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">Exception</span>(<span class="st" style="color: #20794D;">'File not found'</span>)</span>
<span id="cb5-73">        </span>
<span id="cb5-74">        response <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb5-75">          <span class="st" style="color: #20794D;">'title'</span>: <span class="va" style="color: #111111;">None</span>,</span>
<span id="cb5-76">          <span class="st" style="color: #20794D;">'authors'</span>: <span class="va" style="color: #111111;">None</span>,</span>
<span id="cb5-77">          <span class="st" style="color: #20794D;">'summary'</span>: <span class="va" style="color: #111111;">None</span>,</span>
<span id="cb5-78">          <span class="st" style="color: #20794D;">'key_concepts'</span>: <span class="va" style="color: #111111;">None</span>,</span>
<span id="cb5-79">          <span class="st" style="color: #20794D;">'highlights'</span>: <span class="va" style="color: #111111;">None</span>,</span>
<span id="cb5-80">          <span class="st" style="color: #20794D;">'limitations'</span>: <span class="va" style="color: #111111;">None</span>,</span>
<span id="cb5-81">          <span class="st" style="color: #20794D;">'figures'</span>: [],</span>
<span id="cb5-82">          <span class="st" style="color: #20794D;">'tables'</span>: [],</span>
<span id="cb5-83">        }</span>
<span id="cb5-84">        </span>
<span id="cb5-85">        <span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/title.txt'</span>, <span class="st" style="color: #20794D;">'r'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb5-86">            response[<span class="st" style="color: #20794D;">'title'</span>] <span class="op" style="color: #5E5E5E;">=</span> f.read()</span>
<span id="cb5-87">        </span>
<span id="cb5-88">        response_js <span class="op" style="color: #5E5E5E;">=</span> json.load(<span class="bu" style="color: null;">open</span>(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/info.json'</span>))</span>
<span id="cb5-89">        response[<span class="st" style="color: #20794D;">'authors'</span>] <span class="op" style="color: #5E5E5E;">=</span> response_js[<span class="st" style="color: #20794D;">'authors'</span>]</span>
<span id="cb5-90">        response[<span class="st" style="color: #20794D;">'summary'</span>] <span class="op" style="color: #5E5E5E;">=</span> response_js[<span class="st" style="color: #20794D;">'summary'</span>]</span>
<span id="cb5-91">        response[<span class="st" style="color: #20794D;">'key_concepts'</span>] <span class="op" style="color: #5E5E5E;">=</span> response_js[<span class="st" style="color: #20794D;">'key_concepts'</span>]</span>
<span id="cb5-92">        response[<span class="st" style="color: #20794D;">'highlights'</span>] <span class="op" style="color: #5E5E5E;">=</span> response_js[<span class="st" style="color: #20794D;">'highlights'</span>]</span>
<span id="cb5-93">        response[<span class="st" style="color: #20794D;">'limitations'</span>] <span class="op" style="color: #5E5E5E;">=</span> response_js[<span class="st" style="color: #20794D;">'limitations'</span>]</span>
<span id="cb5-94">          </span>
<span id="cb5-95">        <span class="cf" style="color: #003B4F;">for</span> fn <span class="kw" style="color: #003B4F;">in</span> os.listdir(<span class="va" style="color: #111111;">self</span>.resources_path <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'/'</span> <span class="op" style="color: #5E5E5E;">+</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>]):</span>
<span id="cb5-96">            fn <span class="op" style="color: #5E5E5E;">=</span> file_name[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>] <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">'*'</span> <span class="op" style="color: #5E5E5E;">+</span> fn</span>
<span id="cb5-97">            <span class="cf" style="color: #003B4F;">if</span> <span class="st" style="color: #20794D;">'fig'</span> <span class="kw" style="color: #003B4F;">in</span> fn:</span>
<span id="cb5-98">                response[<span class="st" style="color: #20794D;">'figures'</span>].append(fn)</span>
<span id="cb5-99">            <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb5-100">                response[<span class="st" style="color: #20794D;">'tables'</span>].append(fn)</span>
<span id="cb5-101">        <span class="cf" style="color: #003B4F;">return</span> response</span>
<span id="cb5-102">    </span>
<span id="cb5-103">    <span class="kw" style="color: #003B4F;">def</span> _read(<span class="va" style="color: #111111;">self</span>, Paper, reader):</span>
<span id="cb5-104">        <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb5-105"><span class="co" style="color: #5E5E5E;">        Read the text and returns the authors, summary, keywords, keyresults and limitations</span></span>
<span id="cb5-106"><span class="co" style="color: #5E5E5E;">        """</span></span>
<span id="cb5-107">        <span class="co" style="color: #5E5E5E;"># </span><span class="al" style="color: #AD0000;">TODO</span><span class="co" style="color: #5E5E5E;">: Currently we use the Doc Reader service to read the text, but we need to implement our own service</span></span>
<span id="cb5-108">        response <span class="op" style="color: #5E5E5E;">=</span> defaultdict(<span class="bu" style="color: null;">str</span>)</span>
<span id="cb5-109">        <span class="cf" style="color: #003B4F;">for</span> query_type, prompt <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.prompt_tail.items():</span>
<span id="cb5-110">            ans_query <span class="op" style="color: #5E5E5E;">=</span> reader.predict(prompt <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">""</span>.join(Paper[:<span class="dv" style="color: #AD0000;">500</span>].split(<span class="st" style="color: #20794D;">" "</span>)[:<span class="dv" style="color: #AD0000;">20</span>]))</span>
<span id="cb5-111">            response[query_type] <span class="op" style="color: #5E5E5E;">=</span> ans_query</span>
<span id="cb5-112">        </span>
<span id="cb5-113">        <span class="cf" style="color: #003B4F;">return</span> response[<span class="st" style="color: #20794D;">'authors'</span>], response[<span class="st" style="color: #20794D;">'summary'</span>], response[<span class="st" style="color: #20794D;">'keywords'</span>], response[<span class="st" style="color: #20794D;">'keyresults'</span>], response[<span class="st" style="color: #20794D;">'limitations'</span>]</span></code></pre></div>
<p>The <code>DocSummarizer</code> class continues where the <code>DocLayout</code> leaves. The text information retrieved from a document will be concatenated with a suitable prompt tail to send to OpenAI. Notice that each prompt tail is provided with a format for the model to follow (and it did follow!) in the response. For the graphics, they are converted from NumPy arrays to .PNG files in a folders that are accessible from the UI.</p>
</section>
<section id="app.py" class="level2">
<h2 class="anchored" data-anchor-id="app.py"><code>app.py</code></h2>
<p>Add some magic from Flask</p>
</section>
<section id="client" class="level2">
<h2 class="anchored" data-anchor-id="client"><code>client</code></h2>
<p>Add some magic from React.js</p>
</section>
<section id="result" class="level2">
<h2 class="anchored" data-anchor-id="result">Result</h2>
<p>Here is the final demo capture of SumMed</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/hackathon_report/329814980 859809608651093_535713842991494898.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Left is the navigation bar displaying the papers. Center is the key information (text and graphics) + Slide maker that is not yet implemented. Right is the chatbox with information on the paper</em></figcaption><p></p>
</figure>
</div>


</section>
</section>

 ]]></description>
  <category>code</category>
  <category>Engineering/Hacking</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/hackathon_report/index.html</guid>
  <pubDate>Mon, 27 Feb 2023 16:00:00 GMT</pubDate>
  <media:content url="https://datamachina.substack.com/p/data-machina-190" medium="image"/>
</item>
<item>
  <title>Conditional &amp; Controllable GAN</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/index.html</link>
  <description><![CDATA[ 




<p>Apologize for being late: it has been nearly 3 weeks already since the latest post. But I am back for the last week of content.</p>
<p>When I started writing, it was near the time to bed, and without a doubt, I am hungry. So let’s deal with cookies this time.</p>
<p>Up until now, our GAN has managed to do some interesting stuff (“writing” Kanji characters, or numbers, if you used the traditional MNIST dataset). However, one thing you must notice is that we have <em>no</em> control over what the Generator will give us. It can be a “na”, it can be a “tsu”, it can be a “ki” - no control whatsoever. In our cookie analogy, our current Generator is like a goodwilled roomie who bakes for us every day, but each day we will receive a random cookie type.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>You know it is cookie, but you have no idea what type is it</em></figcaption><p></p>
</figure>
</div>
<p>Now, if you love randomness and can tolerate the taste as well as the sugar, fine. But we usually want our model to be <em>controllable</em>, that is, we get to decide (to some extent) what will be included in the output.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>It’s much nicer to control that you have matcha on Monday, chocochip on Tuesday, and so on.</em></figcaption><p></p>
</figure>
</div>
<p>With the objective set, let’s explore way to implement controllable GAN a.k.a way to make sure we have the correct cookie each day.</p>
<section id="limiting-to-just-one-category" class="level1">
<h1>Limiting to just one category:</h1>
<p>This is a no-brainer solution. To prevent random category generation (and mode collapse as well), who don’t just feed in data of a single class only? It is like always mixing matcha powder into the dough to make the cookies, ensuring that every day we will get matcha cookies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>A matcha cookie junkie’s dream.</em></figcaption><p></p>
</figure>
</div>
<p>Obviously this solution is for when you want to generate examples of one class only. One example would be augmenting data for brain EMR of a certain disease at a certain region. The other <del>trolled</del> example is <a href="https://www.kaggle.com/datasets/andy8744/ganyu-genshin-impact-anime-faces-gan-training">GANyu</a>, a dataset and models fine-tuned on it for the task of generating faces of the Genshin Impact character Ganyu (Check out the <a href="https://www.kaggle.com/datasets/andy8744/rezero-rem-anime-faces-for-gan-training">GA(N)Rem</a> as well).</p>
<blockquote class="blockquote">
<p>I don’t know what is the thing for animes and GANs, but the moment I discovered GANs, I instantly thought of generating anime girls’ faces. Is is the same phenomenon as researchers in the 90’s instantly thought of classifying cat from everything else the moment they got a decent classifier… - A certain unfamous author on the web</p>
</blockquote>
<p>Moving to more general (and sensible) solution, we must take note of a crucial principle: we cannot generate something that the model has not ever seen before. It’s like we need to give matcha powder to our dear friend if we expect him to bake us some matcha cookies. This principle is handy in exploring the two solutions. The two approaches will both involve tampering with the input noise vector <img src="https://latex.codecogs.com/png.latex?z">. While one focuses on the <em>class/label</em> of the generated, the other focuses on the <em>features</em> of the generated.</p>
</section>
<section id="conditional-generation-control-the-class" class="level1">
<h1>Conditional Generation (Control the class):</h1>
<p><a href="https://arxiv.org/pdf/1411.1784.pdf">Conditional Generative Adversarial Nets</a> was a solution to make GAN more controllable, by passing some extra information <img src="https://latex.codecogs.com/png.latex?y"> (the easiest is class labels, as one-hot vector) with the data fed to Generator and Discriminator. Here is an illustrated example and implementation:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>An input vector to Generator now is made up of two components: noise inputs (to ensure that each generation will be unique) &amp; one-hot class vector (to ensure that the generate example will be of the class we want)</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Similarly, an input to the Discriminator now is an image together with the an one-hot class vector. For an example to be regarded as real, not only it needs to look realistic (reflected by a low binary corss-entropy or Wasserstein loss) but it also needs to look like real examples from the same class. Here, an original matcha cookie definitely looks like a matcha cookie.</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Here is the earlier generated matcha cookie. Let’s say that our Discriminator is a bit better than the Generator. It means that it will detect this looks rather like a matcha cookie, but not too alike.</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Now suppose that the required class is black chocochip, but our Generator gives a matcha cookie. The Discriminator will recognize in this case and gives a low chance that the example is real.</em></figcaption><p></p>
</figure>
</div>
<p>The question now is how do we go on implementing this? From the descriptions, it seems that we need to do 2 things: 1) figure a way to pass the additional information into our two models and 2) update the loss function. 2) is trivial, as the same loss function (binary cross-entropy) can be used and we just need to make sure that the class of the examples are included in the output as well. For 1), in the case of Generator, you just need to concatenate it with the noise vector above. For Discriminator, it is a bit trickier. We feed the images in by passing values of three channels, so the simplest way will be to create n channels more for n classes. This way works for dataset such as the good ol’ MNIST, whether we flatten out images before concatentating or we keep the same matrix and just call <code>torch.cat()</code> (which will create 10 more channels, each of size 28*28, with one of them full of 1 and the rest full of 0). For larger images or ones we do not want to flatten, this simple approach will create a huge memory issue. We will want to pass class information in differently, such as via a different head of input layer, but that is outside of this post scope. Here are the code snippets for the easy case.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;">def</span> get_one_hot_labels(labels: torch.Tensor, n_classes: <span class="bu" style="color: null;">int</span>):</span>
<span id="cb1-5">    <span class="co" style="color: #5E5E5E;">'''</span></span>
<span id="cb1-6"><span class="co" style="color: #5E5E5E;">    Function for creating one-hot vectors for the data.</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;">    :param labels: a vector containing the labels of all examples in a batch.</span></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;">                   Get from each DataLoader. Have shape (n_samples, 1)</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;">    :param n_classes: an integer for number of classes in the data.</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;">                      Get from the dataset created</span></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;">    :return: the one-hot vector for a batch of data</span></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb1-14">    <span class="cf" style="color: #003B4F;">return</span> F.one_hot(labels,n_classes)</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="kw" style="color: #003B4F;">def</span> combine_vectors(x, y):</span>
<span id="cb1-17">    <span class="co" style="color: #5E5E5E;">'''</span></span>
<span id="cb1-18"><span class="co" style="color: #5E5E5E;">    Generic function for combining two 2-D maxtrices with the same 0-shape</span></span>
<span id="cb1-19"><span class="co" style="color: #5E5E5E;">    In our case, they will be (n_samples, x_1) and (n_samples, y_1).</span></span>
<span id="cb1-20"></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;">    :param x: the first matrix, shape (n_samples, x_1)</span></span>
<span id="cb1-22"><span class="co" style="color: #5E5E5E;">    :param y: the second matrix, shape (n_samples, y_1)</span></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;">    :return: the concatenated matrix of shape (n_samples, x_1 + y_1)</span></span>
<span id="cb1-24"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb1-25">    <span class="co" style="color: #5E5E5E;"># To ensure unity of data types, we want the return matrix to have float</span></span>
<span id="cb1-26">    <span class="co" style="color: #5E5E5E;"># type.</span></span>
<span id="cb1-27">    combined <span class="op" style="color: #5E5E5E;">=</span> torch.cat((x.<span class="bu" style="color: null;">float</span>(),y.<span class="bu" style="color: null;">float</span>()), <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-28">    <span class="cf" style="color: #003B4F;">return</span> combined</span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Code will not run if just copy-paste</span></span>
<span id="cb2-2"><span class="co" style="color: #5E5E5E;"># Pre-training</span></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;"># Just the basic part.</span></span>
<span id="cb2-4">kmnist_shape <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">28</span>, <span class="dv" style="color: #AD0000;">28</span>)</span>
<span id="cb2-5">n_classes <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb2-6"></span>
<span id="cb2-7">device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'cuda'</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cpu'</span></span>
<span id="cb2-8">criterion <span class="op" style="color: #5E5E5E;">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb2-9">z_dim <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">64</span> <span class="co" style="color: #5E5E5E;"># Size of the noise vector</span></span>
<span id="cb2-10">gen <span class="op" style="color: #5E5E5E;">=</span> Generator(input_dim<span class="op" style="color: #5E5E5E;">=</span>generator_input_dim).to(device)</span>
<span id="cb2-11">gen_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(gen.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr)</span>
<span id="cb2-12">disc <span class="op" style="color: #5E5E5E;">=</span> Discriminator(im_chan<span class="op" style="color: #5E5E5E;">=</span>discriminator_im_chan).to(device)</span>
<span id="cb2-13">disc_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(disc.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr)</span>
<span id="cb2-14">epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">200</span></span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="kw" style="color: #003B4F;">def</span> weights_init(m):</span>
<span id="cb2-17">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.Conv2d) <span class="kw" style="color: #003B4F;">or</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.ConvTranspose2d):</span>
<span id="cb2-18">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb2-19">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.BatchNorm2d):</span>
<span id="cb2-20">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb2-21">        torch.nn.init.constant_(m.bias, <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb2-22">gen <span class="op" style="color: #5E5E5E;">=</span> gen.<span class="bu" style="color: null;">apply</span>(weights_init)</span>
<span id="cb2-23">disc <span class="op" style="color: #5E5E5E;">=</span> disc.<span class="bu" style="color: null;">apply</span>(weights_init)</span>
<span id="cb2-24"></span>
<span id="cb2-25"><span class="co" style="color: #5E5E5E;"># Training loop</span></span>
<span id="cb2-26">cur_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb2-27">generator_losses <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb2-28">discriminator_losses <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb2-29"></span>
<span id="cb2-30"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(epochs):</span>
<span id="cb2-31">    <span class="cf" style="color: #003B4F;">for</span> real, labels <span class="kw" style="color: #003B4F;">in</span> tqdm(dataloader):</span>
<span id="cb2-32">        n_samples <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(real)</span>
<span id="cb2-33">        real <span class="op" style="color: #5E5E5E;">=</span> real.to(device)</span>
<span id="cb2-34"></span>
<span id="cb2-35">        <span class="co" style="color: #5E5E5E;"># Get image one-hot labels for this batch</span></span>
<span id="cb2-36">        one_hot_labels <span class="op" style="color: #5E5E5E;">=</span> get_one_hot_labels(labels.to(device), n_classes)</span>
<span id="cb2-37">        <span class="co" style="color: #5E5E5E;"># Remember that the DataLoader is in size (n_samples, 1, 28, 28) while the one hot label matrix </span></span>
<span id="cb2-38">        <span class="co" style="color: #5E5E5E;"># has size (n_samples, 1). We need to extend 2 more dimensions if we want to concatenate the two.</span></span>
<span id="cb2-39">        image_one_hot_labels <span class="op" style="color: #5E5E5E;">=</span> one_hot_labels[:, :, <span class="va" style="color: #111111;">None</span>, <span class="va" style="color: #111111;">None</span>]</span>
<span id="cb2-40">        <span class="co" style="color: #5E5E5E;"># Now the one-hot labels matrix has size (n_samples, 1, 1, 1). We need to turn it into</span></span>
<span id="cb2-41">        <span class="co" style="color: #5E5E5E;"># (n_samples, 1, 28, 28) to pass into the input layer.</span></span>
<span id="cb2-42">        image_one_hot_labels <span class="op" style="color: #5E5E5E;">=</span> image_one_hot_labels.repeat(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, kmnist_shape[<span class="dv" style="color: #AD0000;">1</span>], kmnist_shape[<span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb2-43"></span>
<span id="cb2-44">        <span class="co" style="color: #5E5E5E;">### Update discriminator</span></span>
<span id="cb2-45">        <span class="co" style="color: #5E5E5E;"># Zero out the discriminator gradients</span></span>
<span id="cb2-46">        disc_opt.zero_grad()</span>
<span id="cb2-47"></span>
<span id="cb2-48">        <span class="co" style="color: #5E5E5E;"># Get noise corresponding to the current batch_size </span></span>
<span id="cb2-49">        fake_noise <span class="op" style="color: #5E5E5E;">=</span> get_noise(n_samples, z_dim, device<span class="op" style="color: #5E5E5E;">=</span>device)</span>
<span id="cb2-50">        </span>
<span id="cb2-51">        <span class="co" style="color: #5E5E5E;"># Combine the label and the noise and generate fake examples</span></span>
<span id="cb2-52">        noise_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(fake_noise, one_hot_labels)</span>
<span id="cb2-53">        fake <span class="op" style="color: #5E5E5E;">=</span> gen(noise_and_labels)</span>
<span id="cb2-54"></span>
<span id="cb2-55">        <span class="co" style="color: #5E5E5E;"># Get Discriminator's predictiopn on the real and the fake examples</span></span>
<span id="cb2-56">        fake_image_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(fake, image_one_hot_labels)</span>
<span id="cb2-57">        real_image_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(real, image_one_hot_labels)</span>
<span id="cb2-58">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake_image_and_labels.detach()) <span class="co" style="color: #5E5E5E;"># do not update the Generator</span></span>
<span id="cb2-59">        disc_real_pred <span class="op" style="color: #5E5E5E;">=</span> disc(real_image_and_labels)</span>
<span id="cb2-60"></span>
<span id="cb2-61">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb2-62">        disc_fake_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span>
<span id="cb2-63">        disc_real_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span>
<span id="cb2-64">        disc_loss <span class="op" style="color: #5E5E5E;">=</span> (disc_fake_loss <span class="op" style="color: #5E5E5E;">+</span> disc_real_loss) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb2-65"></span>
<span id="cb2-66">        <span class="co" style="color: #5E5E5E;"># Backpropagation</span></span>
<span id="cb2-67">        disc_loss.backward(retain_graph<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-68"></span>
<span id="cb2-69">        <span class="co" style="color: #5E5E5E;"># Update the parameters</span></span>
<span id="cb2-70">        disc_opt.step()</span>
<span id="cb2-71"></span>
<span id="cb2-72">        <span class="co" style="color: #5E5E5E;"># Keep track of the average discriminator loss for visualization</span></span>
<span id="cb2-73">        discriminator_losses <span class="op" style="color: #5E5E5E;">+=</span> [disc_loss.item()]</span>
<span id="cb2-74"></span>
<span id="cb2-75">        <span class="co" style="color: #5E5E5E;">### Update generator</span></span>
<span id="cb2-76">        <span class="co" style="color: #5E5E5E;"># Zero out the generator gradients</span></span>
<span id="cb2-77">        gen_opt.zero_grad()</span>
<span id="cb2-78"></span>
<span id="cb2-79">        <span class="co" style="color: #5E5E5E;"># Regenerate the fake examples with gradients to update</span></span>
<span id="cb2-80">        fake_image_and_labels <span class="op" style="color: #5E5E5E;">=</span> combine_vectors(fake, image_one_hot_labels)</span>
<span id="cb2-81">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake_image_and_labels)</span>
<span id="cb2-82"></span>
<span id="cb2-83">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb2-84">        gen_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span>
<span id="cb2-85"></span>
<span id="cb2-86">        <span class="co" style="color: #5E5E5E;"># Backpropgation</span></span>
<span id="cb2-87">        gen_loss.backward()</span>
<span id="cb2-88"></span>
<span id="cb2-89">        <span class="co" style="color: #5E5E5E;"># Update the parameters</span></span>
<span id="cb2-90">        gen_opt.step()</span>
<span id="cb2-91"></span>
<span id="cb2-92">        <span class="co" style="color: #5E5E5E;"># Keep track of the generator losses for visualization</span></span>
<span id="cb2-93">        generator_losses <span class="op" style="color: #5E5E5E;">+=</span> [gen_loss.item()]</span></code></pre></div>
</section>
<section id="controllable-generation-control-the-feature" class="level1">
<h1>Controllable Generation (Control the feature):</h1>
<p>Up until now, the noise vector fed into GANs is just “noise” - meaningless. However, the numbers in the noise vector do mean something. You can think that each number represent one feature that is recognized by the Generator. The combinations of all these features form a <em>latent space</em> - a space containing a simpler but hidden (to humans) representation of generated examples. This is best understood with the example of amino acid.</p>
<p>The basic of every bodily function is protein, which is a chain of amino acids (you don’t need to know what they are). Each amino acid in the chain is encoded as a sequence of 3 nucleotides, which have 4 in total (there are <img src="https://latex.codecogs.com/png.latex?4%5E%7B3%7D=64"> total combinations, but several combinations encoding the same amino acid, and there are special ones called <em>ending combinations</em> that signify the end but do not encode).</p>
<p>All the 64 combinations can thought of as the <em>latent space</em> of the amino acid. It’s like we have a well-trained Generator on 22 classes that output the exact amino acid or ending signal we want by passing into it certain 3 nucleotides. This is a latent space because the information has been simplified, but it is not quite latent because we now know the exact encoding of the information.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/genetic code.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em><a href="https://openstax.org/books/biology/pages/15-1-the-genetic-code#fig-ch15_01_04">“The genetic code”</a> by OpenStax College, Biology</em></figcaption><p></p>
</figure>
</div>
<p>In our KMNIST example, each image can be represented as a 28 by 28 matrix where each position stores the intensity of the pixel. It can be visualized as below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/KMNIST eg.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>A certain Japanese letter</em></figcaption><p></p>
</figure>
</div>
<p>In the noise vector above, we try to compress this information down to a vector of 64 numbers (recalling the <code>z_dim</code> above), hoping that this is sufficient to store the information to construct all 10 classes of handwritten kanji characters. But let’s fall back to our cookies for a more easily visualizable example.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 5.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Before, we have been generating random numbers in the noise vector. Let’s say that we have been able to decode that the second number in our noise vector encodes information for color of the cookie, with 2.1 signifies the green matcha color we wants.</em></figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cgan 6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Now, after training, we now know that 1.4 corresponds to the brown color of chochip cookies. We can now pass the number to get a brown cookie.</em></figcaption><p></p>
</figure>
</div>
<p>In reality, there are multiple things to note in implementation. One unfortunate thing was the DNA analogy extends to the noise space. A feature is often not influenced by a single value of the noise vector alone but depends on many ones. This is called <em>entanglement</em>, which mostly arises from having a noise vector with dimension smaller than the number of features we want to control. Entanglemnt affects our controllability: if two or more features’ values significantly depend on the same noise value, then changing it will shift all of them while we may want one to change. Therefore, we want to encourage <em>disentanglement</em> of features in two ways:</p>
<ol type="1">
<li>Ensure noise vector has enough data slots. You cannot expect disentanglement of 10 features if your noise vector only has 9 slots. Always have a noise vector with dimension at least the number of modifiable features you want</li>
<li>As a regularization.</li>
</ol>
<p>In practice, we will not learn the exact encoding (such as 2.1 for green or 1.4 for brown as above) but how the feature change with varying number (say, from green to brown by decreasing the <img src="https://latex.codecogs.com/png.latex?2%5E%7Bnd%7D"> number of the noise vector). You do this with, well, a classifier and label. First, you freeze the weight of the Generator. Then you classify the generated examples based on whether they have the feature(s) or not. Afterwards, you <em>update the noise vector</em> based on the loss function with backpropagation. That is the most simple (and laziest) way to update the noise vector, making it the greatest way (for we always want to do the most work with the least effort). Of course, now we need a pre-trained classifier on the feature(s) that we are trying to detect. If we do not, then we will need to train one on our own i.e.&nbsp;more work to do. You can observe the gradual change in the demo video below for the famous paper on the subject <a href="https://arxiv.org/pdf/1907.10786.pdf">Interpreting the Latent Space of GANs for Semantic Face Editing</a>.</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/uoftpl3Bj6w" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Here’s the implementation</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Again, this is not a full implementation</span></span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;"># The images we will work with now are RGB</span></span>
<span id="cb3-3"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb3-4"></span>
<span id="cb3-5">z_dim <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">64</span></span>
<span id="cb3-6">batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">128</span></span>
<span id="cb3-7">device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'cuda'</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cpu'</span></span>
<span id="cb3-8">n_classes <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">40</span></span>
<span id="cb3-9"></span>
<span id="cb3-10">gen <span class="op" style="color: #5E5E5E;">=</span> Generator(z_dim).to(device)</span>
<span id="cb3-11"><span class="co" style="color: #5E5E5E;"># Magically train the model or load a pretrained one</span></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;"># Put the pretrained model on evaluation mode</span></span>
<span id="cb3-13">gen.<span class="bu" style="color: null;">eval</span>()</span>
<span id="cb3-14"></span>
<span id="cb3-15"><span class="co" style="color: #5E5E5E;"># Defined class Classifier above</span></span>
<span id="cb3-16">classifier <span class="op" style="color: #5E5E5E;">=</span> Classifier(n_classes<span class="op" style="color: #5E5E5E;">=</span>n_classes).to(device)</span>
<span id="cb3-17"><span class="co" style="color: #5E5E5E;"># Really load a pretrained model. Look for details at https://pytorch.org/tutorials/beginner/saving_loading_models.html</span></span>
<span id="cb3-18">class_dict <span class="op" style="color: #5E5E5E;">=</span> torch.load(<span class="st" style="color: #20794D;">"pretrained_classifier.pth"</span>, map_location<span class="op" style="color: #5E5E5E;">=</span>torch.device(device))[<span class="st" style="color: #20794D;">"classifier"</span>]</span>
<span id="cb3-19">classifier.load_state_dict(class_dict)</span>
<span id="cb3-20">classifier.<span class="bu" style="color: null;">eval</span>()</span>
<span id="cb3-21"></span>
<span id="cb3-22"><span class="co" style="color: #5E5E5E;"># Here is the optimizer. We have frozen the weight of the classifier with .eval()</span></span>
<span id="cb3-23"><span class="co" style="color: #5E5E5E;"># so only the noise gets updated.</span></span>
<span id="cb3-24">opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(classifier.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb3-25"></span>
<span id="cb3-26"><span class="co" style="color: #5E5E5E;"># Gradient ascent for the noise</span></span>
<span id="cb3-27"><span class="kw" style="color: #003B4F;">def</span> calculate_updated_noise(noise, weight):</span>
<span id="cb3-28">    <span class="co" style="color: #5E5E5E;">'''</span></span>
<span id="cb3-29"><span class="co" style="color: #5E5E5E;">    Update and return the noise vector with gradient ascent</span></span>
<span id="cb3-30"><span class="co" style="color: #5E5E5E;">    :param noise: the old noise vector </span></span>
<span id="cb3-31"><span class="co" style="color: #5E5E5E;">    :param weight: the weights to update each noise value. An analogy</span></span>
<span id="cb3-32"><span class="co" style="color: #5E5E5E;">                   to the learning rate, but for each noise value</span></span>
<span id="cb3-33"><span class="co" style="color: #5E5E5E;">    :return: the updated noise vector</span></span>
<span id="cb3-34"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb3-35">    <span class="cf" style="color: #003B4F;">return</span> noise <span class="op" style="color: #5E5E5E;">+</span> ( noise.grad <span class="op" style="color: #5E5E5E;">*</span> weight)</span>
<span id="cb3-36"></span>
<span id="cb3-37"><span class="co" style="color: #5E5E5E;"># Regularization for disentanglement - and also the scoring function to update noise</span></span>
<span id="cb3-38"><span class="kw" style="color: #003B4F;">def</span> get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):</span>
<span id="cb3-39">    <span class="co" style="color: #5E5E5E;">'''</span></span>
<span id="cb3-40"><span class="co" style="color: #5E5E5E;">    Function to get the score of the update. Reward change in the target feature(s) to</span></span>
<span id="cb3-41"><span class="co" style="color: #5E5E5E;">    change and penalize changes in other features.</span></span>
<span id="cb3-42"><span class="co" style="color: #5E5E5E;">    :param current_classifications: the classifications associated with the current noise</span></span>
<span id="cb3-43"><span class="co" style="color: #5E5E5E;">    :param original_classifications: the classifications associated with the original noise     </span></span>
<span id="cb3-44"><span class="co" style="color: #5E5E5E;">    :param target_indices: the index of the target feature</span></span>
<span id="cb3-45"><span class="co" style="color: #5E5E5E;">    :param other_indices: the indices of the other features</span></span>
<span id="cb3-46"><span class="co" style="color: #5E5E5E;">    :param penalty_weight: the amount that the penalty should be weighted in the overall score</span></span>
<span id="cb3-47"></span>
<span id="cb3-48"><span class="co" style="color: #5E5E5E;">    :return: the score for the current update. </span></span>
<span id="cb3-49"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb3-50">    <span class="co" style="color: #5E5E5E;"># Penalize change in other features</span></span>
<span id="cb3-51">    other_distances <span class="op" style="color: #5E5E5E;">=</span> current_classifications[:,other_indices] <span class="op" style="color: #5E5E5E;">-</span> original_classifications[:,other_indices]</span>
<span id="cb3-52">    other_class_penalty <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>torch.norm(other_distances, dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>).mean() <span class="op" style="color: #5E5E5E;">*</span> penalty_weight</span>
<span id="cb3-53">    </span>
<span id="cb3-54">    <span class="co" style="color: #5E5E5E;"># Reward change in the target feature(s)</span></span>
<span id="cb3-55">    target_score <span class="op" style="color: #5E5E5E;">=</span> current_classifications[:, target_indices].mean()</span>
<span id="cb3-56">    <span class="cf" style="color: #003B4F;">return</span> target_score <span class="op" style="color: #5E5E5E;">+</span> other_class_penalty</span>
<span id="cb3-57"></span>
<span id="cb3-58"><span class="co" style="color: #5E5E5E;">### Generation time!</span></span>
<span id="cb3-59"><span class="co" style="color: #5E5E5E;"># The dataset of choice was CelebA, and here's the list of feature</span></span>
<span id="cb3-60">feature_names <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"5oClockShadow"</span>, <span class="st" style="color: #20794D;">"ArchedEyebrows"</span>, <span class="st" style="color: #20794D;">"Attractive"</span>, <span class="st" style="color: #20794D;">"BagsUnderEyes"</span>, <span class="st" style="color: #20794D;">"Bald"</span>, <span class="st" style="color: #20794D;">"Bangs"</span>,</span>
<span id="cb3-61"><span class="st" style="color: #20794D;">"BigLips"</span>, <span class="st" style="color: #20794D;">"BigNose"</span>, <span class="st" style="color: #20794D;">"BlackHair"</span>, <span class="st" style="color: #20794D;">"BlondHair"</span>, <span class="st" style="color: #20794D;">"Blurry"</span>, <span class="st" style="color: #20794D;">"BrownHair"</span>, <span class="st" style="color: #20794D;">"BushyEyebrows"</span>, <span class="st" style="color: #20794D;">"Chubby"</span>,</span>
<span id="cb3-62"><span class="st" style="color: #20794D;">"DoubleChin"</span>, <span class="st" style="color: #20794D;">"Eyeglasses"</span>, <span class="st" style="color: #20794D;">"Goatee"</span>, <span class="st" style="color: #20794D;">"GrayHair"</span>, <span class="st" style="color: #20794D;">"HeavyMakeup"</span>, <span class="st" style="color: #20794D;">"HighCheekbones"</span>, <span class="st" style="color: #20794D;">"Male"</span>, </span>
<span id="cb3-63"><span class="st" style="color: #20794D;">"MouthSlightlyOpen"</span>, <span class="st" style="color: #20794D;">"Mustache"</span>, <span class="st" style="color: #20794D;">"NarrowEyes"</span>, <span class="st" style="color: #20794D;">"NoBeard"</span>, <span class="st" style="color: #20794D;">"OvalFace"</span>, <span class="st" style="color: #20794D;">"PaleSkin"</span>, <span class="st" style="color: #20794D;">"PointyNose"</span>, </span>
<span id="cb3-64"><span class="st" style="color: #20794D;">"RecedingHairline"</span>, <span class="st" style="color: #20794D;">"RosyCheeks"</span>, <span class="st" style="color: #20794D;">"Sideburn"</span>, <span class="st" style="color: #20794D;">"Smiling"</span>, <span class="st" style="color: #20794D;">"StraightHair"</span>, <span class="st" style="color: #20794D;">"WavyHair"</span>, <span class="st" style="color: #20794D;">"WearingEarrings"</span>, </span>
<span id="cb3-65"><span class="st" style="color: #20794D;">"WearingHat"</span>, <span class="st" style="color: #20794D;">"WearingLipstick"</span>, <span class="st" style="color: #20794D;">"WearingNecklace"</span>, <span class="st" style="color: #20794D;">"WearingNecktie"</span>, <span class="st" style="color: #20794D;">"Young"</span>]</span>
<span id="cb3-66"></span>
<span id="cb3-67">grad_steps <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb3-68">fake_image_history <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb3-69"></span>
<span id="cb3-70">target_indices <span class="op" style="color: #5E5E5E;">=</span> feature_names.index(<span class="st" style="color: #20794D;">"Smiling"</span>) <span class="co" style="color: #5E5E5E;"># Feel free to change this value</span></span>
<span id="cb3-71">other_indices <span class="op" style="color: #5E5E5E;">=</span> [cur_idx <span class="op" style="color: #5E5E5E;">!=</span> target_indices <span class="cf" style="color: #003B4F;">for</span> cur_idx, _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(feature_names)]</span>
<span id="cb3-72">noise <span class="op" style="color: #5E5E5E;">=</span> get_noise(n_images, z_dim).to(device).requires_grad_() <span class="co" style="color: #5E5E5E;"># Must have grad for gradient ascent</span></span>
<span id="cb3-73">original_classifications <span class="op" style="color: #5E5E5E;">=</span> classifier(gen(noise)).detach() <span class="co" style="color: #5E5E5E;"># But we don't need gradients for classifier</span></span>
<span id="cb3-74"></span>
<span id="cb3-75"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(grad_steps):</span>
<span id="cb3-76">    <span class="co" style="color: #5E5E5E;"># Empty the optimizer</span></span>
<span id="cb3-77">    opt.zero_grad()</span>
<span id="cb3-78"></span>
<span id="cb3-79">    <span class="co" style="color: #5E5E5E;"># Generate a batch of fake examples</span></span>
<span id="cb3-80">    <span class="co" style="color: #5E5E5E;"># and add to history</span></span>
<span id="cb3-81">    fake <span class="op" style="color: #5E5E5E;">=</span> gen(noise)</span>
<span id="cb3-82">    fake_image_history <span class="op" style="color: #5E5E5E;">+=</span> [fake]</span>
<span id="cb3-83"></span>
<span id="cb3-84">    <span class="co" style="color: #5E5E5E;"># Calculate scoring function</span></span>
<span id="cb3-85">    fake_score <span class="op" style="color: #5E5E5E;">=</span> get_score(</span>
<span id="cb3-86">        classifier(fake), </span>
<span id="cb3-87">        original_classifications,</span>
<span id="cb3-88">        target_indices,</span>
<span id="cb3-89">        other_indices,</span>
<span id="cb3-90">        penalty_weight<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span></span>
<span id="cb3-91">    )</span>
<span id="cb3-92">    fake_score.backward() <span class="co" style="color: #5E5E5E;"># Automatically calculate noise gradients</span></span>
<span id="cb3-93">    noise.data <span class="op" style="color: #5E5E5E;">=</span> calculate_updated_noise(noise, <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">/</span> grad_steps)</span></code></pre></div>
<p>Here is a <code>fake_image_history</code> for training for feature <code>"Smiling"</code>:</p>
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/celeb example.png" class="img-fluid"></p>
<p>The faces looked distorted. There’s no denying it. The reason may be correlation between features. For example, the model may not be able to generate a smiling face without creating one with a slightly open mouth. Moreover, the model may modify <em>unlabeled</em> features while modifying the target feature(s) and we cannot penalize them with this method.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Let’s compare Conditional and Controllable Generation. For Conditional Generation, we want <em>a specific class</em>, while for Controllable Generation, we want <em>a specific feature</em> for the output. Secondly, if data for Conditional Generation must contain class information, data for Controllable Generation must contain the feature(s) that we want (we cannot get matcha cookie from if we don’t have matcha powder); however, the data now do not need labeling. In both ways, we influence the input by the noise vector <img src="https://latex.codecogs.com/png.latex?z">, but if we concatenate it with class information before, now we try to directly modify the noise vector.</p>


</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <category>From scratch</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/index.html</guid>
  <pubDate>Tue, 21 Feb 2023 16:00:00 GMT</pubDate>
  <media:content url="https://hangenyuu.github.io/h_notes/posts/gan-p4-conditional/cookie 1.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>That Unstable GAN</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan-p3/index.html</link>
  <description><![CDATA[ 




<p>In the last two posts, I have gone over the concept and process of training GANs. I made it seem so straightforward, but reality is harsher. Recall that for GANs, we are training two neural networks at once, and these two compete with each other to mutually improve. Much like real life, if one competitor is too good, the other will not learn anything, if the at point before the end the Generator or the Discriminator becomes too good (or the other becomes too bad), then training breaks down. Thus, training GANs is highly unstable, and any way to ensure smoother training is encouraged.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/test 17.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>I tried to find an xkcd comic for training GANs, but found none. Instead I found this <a href="https://github.com/generic-github-user/xkcd-Generator/">repo</a> about using GANs to generate xkcd comic. It is not even close for a substitute, but you can defintely see that training has broken down: the loss of Generator is way much more than the loss of the Discriminator, and the difference between THIS and an <a href="https://xkcd.com/1838/">xkcd comic</a> is obvious</em></figcaption><p></p>
</figure>
</div>
<section id="general-methods" class="level1">
<h1>General methods</h1>
<section id="activation-function" class="level2">
<h2 class="anchored" data-anchor-id="activation-function">Activation function</h2>
<p>Activation function is a requirement for neural networks’ ability to approximate complex function. Without it, a neural network will become just another linear function.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> torch.nn.functional <span class="im" style="color: #00769E;">as</span> F</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sb</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-8"></span>
<span id="cb1-9">np.random.seed(<span class="dv" style="color: #AD0000;">17</span>)</span>
<span id="cb1-10">torch.manual_seed(<span class="dv" style="color: #AD0000;">17</span>)</span>
<span id="cb1-11"><span class="kw" style="color: #003B4F;">def</span> linear(a, b, x):</span>
<span id="cb1-12">    <span class="cf" style="color: #003B4F;">return</span> a<span class="op" style="color: #5E5E5E;">*</span>x <span class="op" style="color: #5E5E5E;">+</span> b</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">x <span class="op" style="color: #5E5E5E;">=</span> torch.randn(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-2"></span>
<span id="cb2-3">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">9</span>,<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb2-4">ax1 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">121</span>)</span>
<span id="cb2-5">ax2 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">122</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7">ax1.plot(x, linear(<span class="fl" style="color: #AD0000;">.5</span>, <span class="dv" style="color: #AD0000;">4</span>, x) <span class="op" style="color: #5E5E5E;">+</span> linear(<span class="fl" style="color: #AD0000;">3.56</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">5.32</span>, x) <span class="op" style="color: #5E5E5E;">+</span> linear(<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.86</span>, <span class="fl" style="color: #AD0000;">3.74</span>, x), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb2-8">ax2.plot(x, torch.relu(<span class="fl" style="color: #AD0000;">0.5</span><span class="op" style="color: #5E5E5E;">*</span>x) <span class="op" style="color: #5E5E5E;">+</span> torch.relu(<span class="fl" style="color: #AD0000;">3.56</span><span class="op" style="color: #5E5E5E;">*</span>x) <span class="op" style="color: #5E5E5E;">+</span> torch.relu(<span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.86</span><span class="op" style="color: #5E5E5E;">*</span>x), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb2-9"></span>
<span id="cb2-10">ax1.grid()</span>
<span id="cb2-11">ax2.grid()</span>
<span id="cb2-12">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/index_files/figure-html/fig-1-output-1.png" width="707" height="259" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Stacking linear functions on top of each other is just a linear function. Meanwhile, stacking ReLU functions on top of each other create a piecemeal linear function that approximates a curve.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We all starts with the sigmoid function in a binary cross-entropy problem. However, sigmoid, together with tanh, leads to the “vanishing gradient” problem. When the output value of gets close to 0 or 1 for sigmoid (or -1 or 1 for tanh), the gradient gets close to 0, so the weights either are updated very slowly or stop learning altogether. That was when ReLU came into play: the function has a clear, positive gradient when output value is greater than 0, while the bend makes sure that ReLU stacking on each other can produce a curve.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/1 KKjPz4KaEERCpvI04D6Bng.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Each neural network had three hidden layers with three units in each one. The only difference was the activation function. Learning rate: 0.03, regularization: L2. <a href="https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>However, the joy ReLU brought came to halt when “dying ReLU” problem was reported. Suppose we have an output smaller or equal 0, then our derivative will be 0. The 0 derivative on the node means that it will not get updated, and that’s the end for it. Worse, the previous components connected to the node are affected as well, so the whole structure of our neural network will be “dead”. To fix, we have the variation: LeakyReLU. For LeakyReLU, the output value below 0 is not set at 0, but is multiplied by a constant (such as 0.2). Gradient for such value will still be non-zero, provide information to update the weights.</p>
<p>Another, more advanced variation is <a href="https://ar5iv.labs.arxiv.org/html/1606.08415v4">GeLU</a>, where the output is multiplied with i.e.&nbsp;weighted by its percentile. Sounds too complicated? Look at the formula: <img src="https://latex.codecogs.com/png.latex?GELU(x)=x*P(X%3Cx)=x*%5CPhi(x)"> for <img src="https://latex.codecogs.com/png.latex?X"> ~ <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BN%7D(0,%201)"></p>
<p>GELU has been successfully applied in Transformer models such as <a href="https://ar5iv.labs.arxiv.org/html/1810.04805v2">BERT</a>, <a href="https://ar5iv.labs.arxiv.org/html/2005.14165v4">GPT-3</a>, and especially in CNN such as <a href="https://ar5iv.labs.arxiv.org/html/2201.03545">ConvNeXts</a>. (Yeah, look at ConvNeXts - it started with a ResNet, the great ConvNet architecture, then the authors slowly introduced all the modern training tricks, until the result surpassed the Swin Transformer in the cheer of CNN-backer/Transformer-haters. Okay, that was eaxaggerating, but still…)</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">9</span>,<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb3-2">ax1 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">121</span>)</span>
<span id="cb3-3">ax2 <span class="op" style="color: #5E5E5E;">=</span> fig.add_subplot(<span class="dv" style="color: #AD0000;">122</span>)</span>
<span id="cb3-4"></span>
<span id="cb3-5">ax1.plot(x, F.leaky_relu(x, negative_slope<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb3-6">ax2.plot(x, F.gelu(x), <span class="st" style="color: #20794D;">'o--'</span>)</span>
<span id="cb3-7"></span>
<span id="cb3-8">ax1.grid()</span>
<span id="cb3-9">ax2.grid()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/index_files/figure-html/fig-2-output-1.png" width="719" height="259" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: LeakyReLU and GELU</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Now let’s move on to the second general trick that we have already done: batch normalization.</p>
</section>
<section id="batch-normalization" class="level2">
<h2 class="anchored" data-anchor-id="batch-normalization">Batch normalization</h2>
<p>We all know that neural netowrk is trying to appromixate a certain way of mapping inputs i.e.&nbsp;data to outputs. The parameters of a neural network therefore depend on the data we receive, characteristically the <em>distribution of the data</em>. Here I have this example of an HDR image, which captures a farther range of color and exposure than a compressed format such as JPG or PNG. I found the original image from the Internet <a href="https://blog.gregzaal.com/2014/03/29/pano-golden-gate/">here</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Screenshot 2023-01-24 205502.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The curve at the bottom that may remind you of a bell curve is the curve for the distribution of pixel values a.k.a colors</em></figcaption><p></p>
</figure>
</div>
<p>Now, we train a neural network on data having similar color distribution such as this image, possibly for the task of recognizing grass. The model was trained well. Alas, the testing image contains one such as this</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Screenshot 2023-01-24 205554.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>This was the exact same image, but compressed at a differen color distribution (shifted to the right)</em></figcaption><p></p>
</figure>
</div>
<p>Here we say that the data distribution <em>has shifted between training data and testing data</em>. This generally will cause model problems (decrease accuracy, etc.). Data distribution shift (or covariate shift) can also happen between batches of training data, leading to slow convergence (imagine the model has to take a zig-zag path instead of a straight one). This can be dealt with by <em>normalization</em>, where make sure that the distributions of the training set and the testing set are similar e.g.&nbsp;centered around a mean of 0 and a standard deviation of 1. This could be done by taking the mean and standard deviation for each training batch of image and normalize the inputs of each training batch, then take the accumulated statistics to normalize the testing set during testing. This will smooth out the cost function and increases model performance (you might not need to do this if your training set and testing set are already similar to each other).</p>
<p>However, model is susceptible to <em>internal covariate shift</em> as well, where the activation output distributions shift between each layer. This can happen due to the change in the weights of each layer. Batch normalization came into play here by normalizing the inputs to each layer (“batch” means that we do so for each batch of image). For example, supposed are at nueron <img src="https://latex.codecogs.com/png.latex?i"> of non-last layer <img src="https://latex.codecogs.com/png.latex?l">, with activated output from the last layer to this neuron being <img src="https://latex.codecogs.com/png.latex?a_%7Bi%7D%5E%7B%5Bl-1%5D%7D">. The logit out of this neuron will be <img src="https://latex.codecogs.com/png.latex?z_%7Bi%7D%5E%7B%5Bl%5D%7D=%5CSigma%20W_%7Bi%7D%5E%7B%5Bl%5D%7Da_%7Bi%7D%5E%7B%5Bl-1%5D%7D"></p>
<p>Without batch normalization, the logit will be passed into activation to output <img src="https://latex.codecogs.com/png.latex?a_%7Bi%7D%5E%7B%5Bl%5D%7D">. But here, we will perform batch normalization:</p>
<ol type="1">
<li>We get the statistics mean <img src="https://latex.codecogs.com/png.latex?%5Cmu%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D%20%5E%7B2%7D"> for the batch.</li>
<li>We use them in the formula <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bz%7D_%7Bi%7D%5E%7B%5Bl%5D%7D=%5Cfrac%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D-%5Cmu%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D%7D%7B%5Csqrt%7B%5Csigma%20_%7Bz_%7Bi%7D%5E%7B%5Bl%5D%7D%7D%20%5E%7B2%7D%20+%20%5Cepsilon%7D%7D"> Nothing too fancy - it’s just the normalization formula that you encounter in any statistics course/textbook: substract the value by the mean, then divide it by the square root of variance a.k.a the standard deviation. The <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> term is a positive constant there to make sure that the denominator is always positive.</li>
<li>We map the normalized value <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bz%7D_%7Bi%7D%5E%7B%5Bl%5D%7D"> to a new distribution with the formula <img src="https://latex.codecogs.com/png.latex?y_%7Bi%7D%5E%7B%5Bl%5D%7D=%5Cgamma*%5Chat%7Bz%7D_%7Bi%7D%5E%7B%5Bl%5D%7D%20+%20%5Cbeta"> where <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is <em>scale factor</em> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> the <em>shift factor</em>. These two are learnable inputs in the batch normalization layer, and will be tuned to figure out the best distribution for the task at hand.</li>
<li>We pass <img src="https://latex.codecogs.com/png.latex?y_%7Bi%7D%5E%7B%5Bl%5D%7D"> through the activation function to the output <img src="https://latex.codecogs.com/png.latex?a_%7Bi%7D%5E%7B%5Bl%5D%7D">.</li>
</ol>
<p>The batch normalization layer seems complicated, but we usually does not need to all the things. As backpropagation is reduced to just calling <a href="https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop"><code>loss.backward</code></a> in PyTorch, the <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html"><code>nn.BatchNorm2d()</code></a> (for images) will take care of this during training.</p>
<p>There is another normalization method called <em>layer normalization</em>. I will not go into details here, though I very much want to because it was used in the training of ConvNeXts as well (seriously, I want to make a blog post just about the tricks used in pushing this CNN to surpass Swin). Here is a <a href="https://www.pinecone.io/learn/batch-layer-normalization/">post</a> about the two normalizations that also have great images. In PyTorch, this is implemented in <a href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"><code>nn.LayerNorm()</code></a>.</p>
</section>
</section>
<section id="gans-specific-method" class="level1">
<h1>GAN’s specific method</h1>
<p>To be honest, there should be tens of tricks for GANs. But I will only cover one this post: Wasserstein GAN (WGAN) and the accompanied Gradient Penalty.</p>
<section id="wgan" class="level2">
<h2 class="anchored" data-anchor-id="wgan">WGAN:</h2>
<p>First, we need to talk about <em>mode collapse</em>. Now, a mode in statistical term is the value that we are most likely to get from a distribution (not too correct for continuous distribution, but still great for understanding). This will be represented by a peak in the data distribution, such as the mean in a normal distribution. A distribution can have just one mode, like the normal distribution, or multiple modes like below.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">sample1 <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(loc<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">20</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">300</span>)</span>
<span id="cb4-2">sample2 <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(loc<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">40</span>, scale<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">700</span>)</span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;"># Concatenating the two sample along the second axis</span></span>
<span id="cb4-4">sample <span class="op" style="color: #5E5E5E;">=</span> np.hstack((sample1, sample2))</span>
<span id="cb4-5"></span>
<span id="cb4-6">sb.kdeplot(sample)</span>
<span id="cb4-7">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/index_files/figure-html/fig-3-output-1.png" width="606" height="404" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: A bimodal distribution created by merging two normal distributions</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The outputs have their modes alright. For example, in our KMNIST dataset, there are 10 modes for the output, corresponding to 10 characters. So we have a new way to think about training: we are trying to make the model learn to shift the distribution of the outputs to approximate the one we want. For illustration, suppose initially our model is outputing each value for each pixel randomly, leading to an output distribution like this.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Copy of GAN-p2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>Initial output distribution</em></figcaption><p></p>
</figure>
</div>
<p>We want to change the output to this kind of distribution</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan-p3/Copy of GAN-p3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>We want to shift from the circle to just 10 peaks i.e.&nbsp;just outputting 1 from 10 classes at a time</em></figcaption><p></p>
</figure>
</div>
<p>In the ideal scenario, our model will be guided by the loss function to make the right shift. However, notice a lack in the BCE loss: it only promotes the model generating images close to real images, regardless of the class of the image. This means that there exists a quick n’ dirty way for the Generator to reduce the loss by <em>only generating images from 1 class that the Discriminator is most fooled by</em>. So there exists a case where we end up with a Generator that outputs very realistic kanji character, only that it generates <em>kanji character</em>, say, only “tsu”. That’s boring.</p>
<p>My last paragraph gives hint to the source of the problem: our loss function. BCE loss works to push the Generator forward, but it cannot capture the information of class within the image. We need something else. And that something else is <a href="https://ar5iv.labs.arxiv.org/html/1701.07875">Wasserstein GAN</a>, introducing a new kind of loss function: the Wasserstein Loss (no surprise) a.k.a the <em>Earth Mover’s distance</em>, or EM distance. It measures the difference between two distributions, and can be informally defined as the least amount of energy required to move and mold a earth pile in the shape of one distribution to the shape of another distribution (hence earth mover).</p>
<p>In mathematical form, if we have the noise vector <img src="https://latex.codecogs.com/png.latex?z">, the fake image data <img src="https://latex.codecogs.com/png.latex?x">, the Generator model <img src="https://latex.codecogs.com/png.latex?g()">, the Discriminator who becomes the Critic <img src="https://latex.codecogs.com/png.latex?c()">, then the Wasserstein Loss is the difference between the expected value i.e.&nbsp;the mean of the Critic outputs for real images and the mean of the Critic outputs for generated images. <img src="https://latex.codecogs.com/png.latex?WLoss=E(c(x))-E(c(g(z)))"></p>
<p>We still have a minimax game here, with the Generator’s goal being minimize the above difference and the Critic’s goal being maximize the above difference. Notice also that the Critic now is no longer a classifier: it can give any real value possible e.g.&nbsp;higher score for real(istic) examples. This means that the Generator will get useful feedback for all classes of examples, and is less prone to mode collapse. Getting rid of the classifier i.e.&nbsp;the sigmoid function in the output also means that vanishing gradient is also less likely. Two birds, one stone.</p>
<p>W-Loss has one condition: the function of the Critic should be 1-Lipschitz Continuity. That looks intimidating but it just means that the norm of the gradient (the value in 2D math, the <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bx%5E%7B2%7D+y%5E%7B2%7D+...%7D"> as example in higher dimensions) for the Critic can be at most 1 at any point. In other words, the output of the Critic cannot increase/decrease more than linearly at any point. To achieve this, the first proposed (and terrible way, according to the original author) was <em>weight clipping</em> - forcing the weights to a fixed interval, say, [0,1]. Any negative value will be set to 0, and any value more than 1 will be set to 1. This was terrible (I have to say it again) because it limits the potential of the Critic. Another less strict way is <a href="https://arxiv.org/abs/1704.00028">gradient penalty</a> (the first dead ar5iv link), where you add a regularization term in the loss function to <em>promote</em> the Critic to be 1-Lipschitz Continuity, as oppposed to forcing it. Formula is <img src="https://latex.codecogs.com/png.latex?WLoss=E(c(x))-E(c(g(z)))%20+%20%5Clambda%20*%20pen"> with <img src="https://latex.codecogs.com/png.latex?pen=E((%7C%7C%20%5Cnabla%20c(%5Chat%7Bx%7D)%7C%7C_%7B2%7D-1)%5E%7B2%7D)">, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D=%5Cepsilon%20x%20+%20(1-%5Cepsilon)g(z)"></p>
<p>For completeness, the Critic gradient needs checking at every point in the feature space, which is impractical. What we do is sampling some points from real examples, and then some points from generated examples with weights for each, and then we calculate the penalty for the interpolated examples. An example for the code of WGAN can be found here (nothing for now). For a more technical review of WGAN, check out this <a href="https://ar5iv.labs.arxiv.org/html/1904.08994">paper</a>, also available as a <a href="https://lilianweng.github.io/posts/2017-08-20-gan/">blog post</a>.</p>
<p>Next in line: Conditional GANs.</p>
<p><em>All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the <a href="https://ar5iv.labs.arxiv.org/">tool</a>. To change to the abstract page, follow this example:</em> <code>https://ar5iv.labs.arxiv.org/html/1409.1556</code> → <code>https://arxiv.org/abs/1409.1556</code>.</p>


</section>
</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <category>From scratch</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan-p3/index.html</guid>
  <pubDate>Mon, 23 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://github.com/generic-github-user/xkcd-Generator/" medium="image"/>
</item>
<item>
  <title>Building a simple GAN</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan_p2/index.html</link>
  <description><![CDATA[ 




<p>In the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, <a href="https://github.com/rois-codh/kmnist">KMNIST</a>.</p>
<section id="concept---training-gans" class="level1">
<h1>Concept - Training GANs:</h1>
<p>Like any training process, the process GANs can be decomposed into feed-forward and backpropagation, though with distinct features from, say, training a classifier. The parameters updated after backpropagation can also be divided into two steps, for Discriminator and Generator. These are summed up in the images below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">First, in the feed-forward, we pass some random noise (denoted by <img src="https://latex.codecogs.com/png.latex?%5Cxi">) into the Generator, which outputs some fake examples (denoted by <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">). The fake examples are then merged with a dataset of real examples (just <img src="https://latex.codecogs.com/png.latex?X">) and feed separately into the Discriminator, and we receive the outputs as a vector containing the possibilities of each example being real (between 0 and 1).</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Second, for training the Discriminator. We will calculate the loss as binary cross-entropy (BCE) loss for two components: how closely to 0 the Discriminator predicted the fake examples, and how closely to 1 the Discriminator predicted the real examples. Here, we need to detach the Generator from the gradient flow as we want to update the Discriminator’s parameters only</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Third, for training the Generator. From the predictions for the fake examples, we calculate the BCE loss as how closely the Discriminator predicted them to 1. We then update the Generator’s parameters.</figcaption><p></p>
</figure>
</div>
<p>Hopefully the ideas are not too complicated. If they are so, hopefully things will make more sense when we look at the codes.</p>
</section>
<section id="hands-on---creating-gans" class="level1">
<h1>Hands-on - Creating GANs:</h1>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">The dataset:</h2>
<p>First rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/kmnist examples.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The 10 classes of Kuzushiji-MNIST, with the first column showing each character’s modern hiragana counterpart. <a href="https://github.com/rois-codh/kmnist#the-dataset">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>Similar to MNIST, a KMNIST image has only one channel. Let’s visualize one.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> torch <span class="im" style="color: #00769E;">import</span> nn</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> tqdm.auto <span class="im" style="color: #00769E;">import</span> tqdm</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> torchvision <span class="im" style="color: #00769E;">import</span> datasets, transforms</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> torch.utils.data <span class="im" style="color: #00769E;">import</span> DataLoader</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> torchvision.utils <span class="im" style="color: #00769E;">import</span> make_grid</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Function learnt from GAN's Specialization Course 1 Week 1</span></span>
<span id="cb2-2"><span class="kw" style="color: #003B4F;">def</span> tensor_show(image_tensor, num_images<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">25</span>, size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">28</span>, <span class="dv" style="color: #AD0000;">28</span>)):</span>
<span id="cb2-3">    <span class="co" style="color: #5E5E5E;"># The original image tensor could be stored on GPU and </span></span>
<span id="cb2-4">    <span class="co" style="color: #5E5E5E;"># have been flattened out for training, so we restore it</span></span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;"># first.</span></span>
<span id="cb2-6">    image_unflat <span class="op" style="color: #5E5E5E;">=</span> image_tensor.detach().cpu().view(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="op" style="color: #5E5E5E;">*</span>size)</span>
<span id="cb2-7">    image_grid <span class="op" style="color: #5E5E5E;">=</span> make_grid(image_unflat[:num_images], nrow<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb2-8">    <span class="co" style="color: #5E5E5E;"># torch uses (color channel, height, width) while </span></span>
<span id="cb2-9">    <span class="co" style="color: #5E5E5E;"># matplotlib used (height, width, color channel)</span></span>
<span id="cb2-10">    <span class="co" style="color: #5E5E5E;"># so we fix it here</span></span>
<span id="cb2-11">    plt.imshow(image_grid.permute(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>).squeeze())</span>
<span id="cb2-12">    plt.show()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Download needs to be set to True the first time you run it.</span></span>
<span id="cb3-2">batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">32</span></span>
<span id="cb3-3">dataloader <span class="op" style="color: #5E5E5E;">=</span> DataLoader(</span>
<span id="cb3-4">    datasets.KMNIST(<span class="st" style="color: #20794D;">'data'</span>, download<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, transform<span class="op" style="color: #5E5E5E;">=</span>transforms.ToTensor()),</span>
<span id="cb3-5">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb3-6">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">image_tensor <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">next</span>(<span class="bu" style="color: null;">iter</span>(dataloader))[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-2">tensor_show(image_tensor)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/index_files/figure-html/cell-5-output-1.png" width="416" height="408"></p>
</div>
</div>
</section>
<section id="the-discriminator" class="level2">
<h2 class="anchored" data-anchor-id="the-discriminator">The Discriminator:</h2>
<blockquote class="blockquote">
<p>The architecture for each block of Discriminator and Generator follows the suggestions from the <a href="https://ar5iv.labs.arxiv.org/html/1511.06434">Deep Convolutional GAN</a> paper.</p>
</blockquote>
<p>The Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol’ linear model, but I will skip a bit to the year 2015, when DCGAN was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A non-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN’s training. We will touch upon tricks to stabilize GAN’s training in the next post).</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">class</span> Discriminator(nn.Module):</span>
<span id="cb5-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, image_channel<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, hidden_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">56</span>):</span>
<span id="cb5-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb5-4">        <span class="va" style="color: #111111;">self</span>.disc <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb5-5">            <span class="va" style="color: #111111;">self</span>.make_disc_block(image_channel, hidden_dim),</span>
<span id="cb5-6">            <span class="va" style="color: #111111;">self</span>.make_disc_block(hidden_dim, hidden_dim),</span>
<span id="cb5-7">            <span class="va" style="color: #111111;">self</span>.make_disc_block(hidden_dim, <span class="dv" style="color: #AD0000;">1</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>),</span>
<span id="cb5-8">        )</span>
<span id="cb5-9"></span>
<span id="cb5-10">    <span class="kw" style="color: #003B4F;">def</span> make_disc_block(<span class="va" style="color: #111111;">self</span>, input_channels, output_channels, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb5-11">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> final_layer:</span>
<span id="cb5-12">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb5-13">                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),</span>
<span id="cb5-14">                    nn.BatchNorm2d(output_channels),</span>
<span id="cb5-15">                    nn.LeakyReLU(negative_slope<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.25</span>)</span>
<span id="cb5-16">            )</span>
<span id="cb5-17">        <span class="cf" style="color: #003B4F;">else</span>: <span class="co" style="color: #5E5E5E;"># Final Layer</span></span>
<span id="cb5-18">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb5-19">                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)</span>
<span id="cb5-20">            )</span>
<span id="cb5-21">    </span>
<span id="cb5-22">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, x: torch.Tensor):</span>
<span id="cb5-23">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.disc(x)</span>
<span id="cb5-24">        <span class="co" style="color: #5E5E5E;"># The input can be a tensor of multiple images</span></span>
<span id="cb5-25">        <span class="co" style="color: #5E5E5E;"># We want to return a tensor with the possibility</span></span>
<span id="cb5-26">        <span class="co" style="color: #5E5E5E;"># of real/fake for each image.</span></span>
<span id="cb5-27">        <span class="cf" style="color: #003B4F;">return</span> x.view(<span class="bu" style="color: null;">len</span>(x), <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
</section>
<section id="the-generator" class="level2">
<h2 class="anchored" data-anchor-id="the-generator">The Generator:</h2>
<p>A point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially <em>distilling</em> the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the <em>transposed convolution</em> operation, which <em>increases</em> the dimensions of data, essentially <em>magnifying</em> the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this <a href="https://github.com/HangenYuu/vision_learner/blob/main/ARCHITECTURE/CNN/Tiny/TinyCNN.ipynb">notebook</a> as my draft.)</p>
<p>First, we need a function to generate noise. Basically, we need some tensor containing random numbers, and we can conveniently return a tensor filled with random numbers from a normal distribution with <code>torch.randn()</code>. For the dimensions, we define argument <code>z_dim</code> as the dimension of the noise input, and <code>n_samples</code> as the number of samples we need.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">def</span> generate_noise(n_samples, z_dim, device<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'cpu'</span>):</span>
<span id="cb6-2">    <span class="cf" style="color: #003B4F;">return</span> torch.randn((n_samples, z_dim), device<span class="op" style="color: #5E5E5E;">=</span>device)</span></code></pre></div>
</div>
<p>For the <code>Generator</code> class, I will also create a function to construct each layer. A non-last layer will have a transposed convolution, followed by batch normalization and ReLU activation. The final layer does not have batch normalization but will have Tanh activation to squish the pixels in range.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;">class</span> Generator(nn.Module):</span>
<span id="cb7-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, z_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">14</span>, image_channel<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>, hidden_dim<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">56</span>):</span>
<span id="cb7-3">        <span class="co" style="color: #5E5E5E;"># z_dim is the dimension of the input noise vector</span></span>
<span id="cb7-4">        <span class="va" style="color: #111111;">self</span>.z_dim <span class="op" style="color: #5E5E5E;">=</span> z_dim</span>
<span id="cb7-5">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb7-6">        <span class="va" style="color: #111111;">self</span>.gen <span class="op" style="color: #5E5E5E;">=</span> nn.Sequential(</span>
<span id="cb7-7">            <span class="va" style="color: #111111;">self</span>.make_gen_block(z_dim, hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">4</span>),</span>
<span id="cb7-8">            <span class="va" style="color: #111111;">self</span>.make_gen_block(hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">4</span>, hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">2</span>, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb7-9">            <span class="va" style="color: #111111;">self</span>.make_gen_block(hidden_dim <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">2</span>, hidden_dim),</span>
<span id="cb7-10">            <span class="va" style="color: #111111;">self</span>.make_gen_block(hidden_dim, image_channel, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>),</span>
<span id="cb7-11">        )</span>
<span id="cb7-12">    </span>
<span id="cb7-13">    <span class="kw" style="color: #003B4F;">def</span> make_gen_block(<span class="va" style="color: #111111;">self</span>, input_channels, output_channels, kernel_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">3</span>, stride<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, final_layer<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb7-14">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> final_layer:</span>
<span id="cb7-15">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb7-16">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span>
<span id="cb7-17">                nn.BatchNorm2d(output_channels),</span>
<span id="cb7-18">                nn.ReLU()</span>
<span id="cb7-19">            )</span>
<span id="cb7-20">        <span class="cf" style="color: #003B4F;">else</span>: <span class="co" style="color: #5E5E5E;"># Final Layer</span></span>
<span id="cb7-21">            <span class="cf" style="color: #003B4F;">return</span> nn.Sequential(</span>
<span id="cb7-22">                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),</span>
<span id="cb7-23">                nn.Tanh()</span>
<span id="cb7-24">            )</span>
<span id="cb7-25">    <span class="co" style="color: #5E5E5E;"># Recall torch expect an image to be in the form (color channel, height, width).</span></span>
<span id="cb7-26">    <span class="co" style="color: #5E5E5E;"># In a batch, torch expects it to be (no. of images in batch, color channel, height, width)</span></span>
<span id="cb7-27">    <span class="co" style="color: #5E5E5E;"># So we need to transform the noise, originally in (no. of images in batch, input dimension)</span></span>
<span id="cb7-28">    <span class="co" style="color: #5E5E5E;"># to (no. of images in batch, input dimension, 1, 1)</span></span>
<span id="cb7-29">    <span class="co" style="color: #5E5E5E;"># See more here:</span></span>
<span id="cb7-30">    <span class="co" style="color: #5E5E5E;"># https://pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images</span></span>
<span id="cb7-31">    <span class="kw" style="color: #003B4F;">def</span> unsqueeze_noise(<span class="va" style="color: #111111;">self</span>, noise):</span>
<span id="cb7-32">        <span class="cf" style="color: #003B4F;">return</span> noise.view(<span class="bu" style="color: null;">len</span>(noise), <span class="va" style="color: #111111;">self</span>.z_dim, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb7-33"></span>
<span id="cb7-34">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, noise):</span>
<span id="cb7-35">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.unsqueeze_noise(noise)</span>
<span id="cb7-36">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.gen(x)</span></code></pre></div>
</div>
</section>
<section id="optimizers-and-criterion" class="level2">
<h2 class="anchored" data-anchor-id="optimizers-and-criterion">Optimizers and Criterion</h2>
<p>Next, we want to define our optimizers (one for each model) and our criterion.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;"># We do not have activation at the output for Discriminator, so the outputs</span></span>
<span id="cb8-2"><span class="co" style="color: #5E5E5E;"># are raw (logits).</span></span>
<span id="cb8-3">criterion <span class="op" style="color: #5E5E5E;">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb8-4">z_dim <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">64</span></span>
<span id="cb8-5">display_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">500</span></span>
<span id="cb8-6">batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1000</span></span>
<span id="cb8-7"><span class="co" style="color: #5E5E5E;"># Learning rate of 0.0002 and beta_1 (momentum term for Adam optimizer) of </span></span>
<span id="cb8-8"><span class="co" style="color: #5E5E5E;"># 0.5 works well for DCGAN, according to the paper (yes, I seriously searched</span></span>
<span id="cb8-9"><span class="co" style="color: #5E5E5E;"># for keyword "learning rate" in the paper)</span></span>
<span id="cb8-10">lr <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.0002</span></span>
<span id="cb8-11">beta_1 <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span> </span>
<span id="cb8-12">beta_2 <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.999</span></span>
<span id="cb8-13"><span class="co" style="color: #5E5E5E;"># Device-agnostic code</span></span>
<span id="cb8-14">device <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'cuda'</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cpu'</span></span>
<span id="cb8-15"></span>
<span id="cb8-16"><span class="co" style="color: #5E5E5E;"># You can tranform the image values to be between -1 and 1 (the range of the Tanh activation)</span></span>
<span id="cb8-17">transform <span class="op" style="color: #5E5E5E;">=</span> transforms.Compose([</span>
<span id="cb8-18">    transforms.ToTensor(),</span>
<span id="cb8-19">    transforms.Normalize((<span class="fl" style="color: #AD0000;">0.5</span>,), (<span class="fl" style="color: #AD0000;">0.5</span>,)),</span>
<span id="cb8-20">])</span>
<span id="cb8-21"></span>
<span id="cb8-22">dataloader <span class="op" style="color: #5E5E5E;">=</span> DataLoader(</span>
<span id="cb8-23">    datasets.KMNIST(<span class="st" style="color: #20794D;">'data'</span>, download<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, transform<span class="op" style="color: #5E5E5E;">=</span>transform),</span>
<span id="cb8-24">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb8-25">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">gen <span class="op" style="color: #5E5E5E;">=</span> Generator(z_dim).to(device)</span>
<span id="cb9-2">gen_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(gen.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr, betas<span class="op" style="color: #5E5E5E;">=</span>(beta_1, beta_2))</span>
<span id="cb9-3">disc <span class="op" style="color: #5E5E5E;">=</span> Discriminator().to(device) </span>
<span id="cb9-4">disc_opt <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(disc.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span>lr, betas<span class="op" style="color: #5E5E5E;">=</span>(beta_1, beta_2))</span>
<span id="cb9-5"></span>
<span id="cb9-6"><span class="co" style="color: #5E5E5E;"># You initialize the weights to the normal distribution</span></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;"># with mean 0 and standard deviation 0.02</span></span>
<span id="cb9-8"><span class="co" style="color: #5E5E5E;"># (Yes, the paper said so.)</span></span>
<span id="cb9-9"><span class="kw" style="color: #003B4F;">def</span> weights_init(m):</span>
<span id="cb9-10">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.Conv2d) <span class="kw" style="color: #003B4F;">or</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.ConvTranspose2d):</span>
<span id="cb9-11">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb9-12">    <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(m, nn.BatchNorm2d):</span>
<span id="cb9-13">        torch.nn.init.normal_(m.weight, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.02</span>)</span>
<span id="cb9-14">        torch.nn.init.constant_(m.bias, <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb9-15"><span class="co" style="color: #5E5E5E;"># Apply recursively weights_init() according to the docs:</span></span>
<span id="cb9-16"><span class="co" style="color: #5E5E5E;"># https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply</span></span>
<span id="cb9-17">gen <span class="op" style="color: #5E5E5E;">=</span> gen.<span class="bu" style="color: null;">apply</span>(weights_init)</span>
<span id="cb9-18">disc <span class="op" style="color: #5E5E5E;">=</span> disc.<span class="bu" style="color: null;">apply</span>(weights_init)</span></code></pre></div>
</div>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>Okay, now onto training!</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">n_epochs <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100</span></span>
<span id="cb10-2">cur_step <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="co" style="color: #5E5E5E;"># For visualization purpose</span></span>
<span id="cb10-3">mean_generator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-4">mean_discriminator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-5"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n_epochs):</span>
<span id="cb10-6">    <span class="cf" style="color: #003B4F;">for</span> real, _ <span class="kw" style="color: #003B4F;">in</span> tqdm(dataloader):</span>
<span id="cb10-7">        cur_batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(real)</span>
<span id="cb10-8">        real <span class="op" style="color: #5E5E5E;">=</span> real.to(device)</span>
<span id="cb10-9"></span>
<span id="cb10-10">        <span class="co" style="color: #5E5E5E;">## Update Discriminator</span></span>
<span id="cb10-11"></span>
<span id="cb10-12">        <span class="co" style="color: #5E5E5E;"># Empty the optimizer</span></span>
<span id="cb10-13">        disc_opt.zero_grad()</span>
<span id="cb10-14"></span>
<span id="cb10-15">        <span class="co" style="color: #5E5E5E;"># Generate noise and pass through Discriminator for fake examples</span></span>
<span id="cb10-16">        fake_noise <span class="op" style="color: #5E5E5E;">=</span> generate_noise(cur_batch_size, z_dim, device<span class="op" style="color: #5E5E5E;">=</span>device)</span>
<span id="cb10-17">        fake <span class="op" style="color: #5E5E5E;">=</span> gen(fake_noise)</span>
<span id="cb10-18">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake.detach())</span>
<span id="cb10-19"></span>
<span id="cb10-20">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb10-21">        disc_fake_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))</span>
<span id="cb10-22"></span>
<span id="cb10-23">        <span class="co" style="color: #5E5E5E;"># Same for real examples</span></span>
<span id="cb10-24">        disc_real_pred <span class="op" style="color: #5E5E5E;">=</span> disc(real)</span>
<span id="cb10-25">        disc_real_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_real_pred, torch.ones_like(disc_real_pred))</span>
<span id="cb10-26"></span>
<span id="cb10-27">        <span class="co" style="color: #5E5E5E;"># The Discriminator's loss is the average of the two</span></span>
<span id="cb10-28">        disc_loss <span class="op" style="color: #5E5E5E;">=</span> (disc_fake_loss <span class="op" style="color: #5E5E5E;">+</span> disc_real_loss) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb10-29"></span>
<span id="cb10-30">        <span class="co" style="color: #5E5E5E;"># Keep track of the average Discriminator loss</span></span>
<span id="cb10-31">        mean_discriminator_loss <span class="op" style="color: #5E5E5E;">+=</span> disc_loss.item() <span class="op" style="color: #5E5E5E;">/</span> display_step</span>
<span id="cb10-32"></span>
<span id="cb10-33">        <span class="co" style="color: #5E5E5E;"># Update Discriminator's gradients a.k.a backpropagation</span></span>
<span id="cb10-34">        <span class="co" style="color: #5E5E5E;"># Normally don't set retain_graph=True, but we do so for GAN</span></span>
<span id="cb10-35">        <span class="co" style="color: #5E5E5E;"># as we need to propagate through the graph a second time</span></span>
<span id="cb10-36">        <span class="co" style="color: #5E5E5E;"># when updating the Generator.</span></span>
<span id="cb10-37">        disc_loss.backward(retain_graph<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb10-38"></span>
<span id="cb10-39">        <span class="co" style="color: #5E5E5E;"># Update Discriminator's optimizer</span></span>
<span id="cb10-40">        disc_opt.step()</span>
<span id="cb10-41"></span>
<span id="cb10-42">        <span class="co" style="color: #5E5E5E;">## Update Generator</span></span>
<span id="cb10-43"></span>
<span id="cb10-44">        <span class="co" style="color: #5E5E5E;"># Empty the optimizer</span></span>
<span id="cb10-45">        gen_opt.zero_grad()</span>
<span id="cb10-46"></span>
<span id="cb10-47">        <span class="co" style="color: #5E5E5E;"># Generate noise and pass through Discriminator for fake examples</span></span>
<span id="cb10-48">        fake_noise_2 <span class="op" style="color: #5E5E5E;">=</span> generate_noise(cur_batch_size, z_dim, device<span class="op" style="color: #5E5E5E;">=</span>device)</span>
<span id="cb10-49">        fake_2 <span class="op" style="color: #5E5E5E;">=</span> gen(fake_noise_2)</span>
<span id="cb10-50">        disc_fake_pred <span class="op" style="color: #5E5E5E;">=</span> disc(fake_2)</span>
<span id="cb10-51"></span>
<span id="cb10-52">        <span class="co" style="color: #5E5E5E;"># Calculate loss</span></span>
<span id="cb10-53">        gen_loss <span class="op" style="color: #5E5E5E;">=</span> criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))</span>
<span id="cb10-54"></span>
<span id="cb10-55">        <span class="co" style="color: #5E5E5E;"># Backpropagation for Generator's loss</span></span>
<span id="cb10-56">        gen_loss.backward()</span>
<span id="cb10-57"></span>
<span id="cb10-58">        <span class="co" style="color: #5E5E5E;"># Update Generator's optimizer</span></span>
<span id="cb10-59">        gen_opt.step()</span>
<span id="cb10-60"></span>
<span id="cb10-61">        <span class="co" style="color: #5E5E5E;"># Keep track of the average Generator loss</span></span>
<span id="cb10-62">        mean_generator_loss <span class="op" style="color: #5E5E5E;">+=</span> gen_loss.item() <span class="op" style="color: #5E5E5E;">/</span> display_step</span>
<span id="cb10-63"></span>
<span id="cb10-64">        <span class="co" style="color: #5E5E5E;">## Visualization code</span></span>
<span id="cb10-65">        <span class="cf" style="color: #003B4F;">if</span> cur_step <span class="op" style="color: #5E5E5E;">%</span> display_step <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span> <span class="kw" style="color: #003B4F;">and</span> cur_step <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb10-66">            <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Epoch </span><span class="sc" style="color: #5E5E5E;">{</span>epoch<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, step </span><span class="sc" style="color: #5E5E5E;">{</span>cur_step<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">: Generator loss: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_generator_loss<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, Discriminator loss: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_discriminator_loss<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb10-67">            tensor_show(fake)</span>
<span id="cb10-68">            tensor_show(real)</span>
<span id="cb10-69">            mean_generator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-70">            mean_discriminator_loss <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb10-71">        cur_step <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
</section>
</section>
<section id="results" class="level1">
<h1>Results:</h1>
<p>After training for 100 epochs with the settings, I received the final results. <img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/Screenshot 2023-01-23 175133.png" class="img-fluid" alt="The above were the generated images, and the below were the real images."></p>
<p>One definitely can still discern which kanji characters were real and which were fake. However, one must admit that the model did manage to learn some of the intricate features of Japanese calligraphy. Then, after another 100 epochs… <img src="https://hangenyuu.github.io/h_notes/posts/gan_p2/Screenshot 2023-01-23 183057.png" class="img-fluid"></p>
<p>While it is still not yet indistinguishables, the Generator has gotten very close. The third image in the first row or the one at bottom left corner could definitely be passed off as real ones.</p>
</section>
<section id="end-of-part-2" class="level1">
<h1>End of part 2</h1>
<p>In this part, I have walked you through the training process and the building blocks of GANS. We have trained and witness good results from a simple model on the KMNIST dataset. In the next part, we will continue with the development of GANs, namely different ways to make training more stable. Up to now, it seemed really easy to achieve relatively good result with GANs (to be honest, we haven’t tried anything too complicated, too), but it will much harder when we touch upon larger models. See you then.</p>
<p><em>All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the <a href="https://ar5iv.labs.arxiv.org/">tool</a>. To change to the abstract page, follow this example:</em> <code>https://ar5iv.labs.arxiv.org/html/1409.1556</code> → <code>https://arxiv.org/abs/1409.1556</code>.</p>


</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <category>From scratch</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan_p2/index.html</guid>
  <pubDate>Sun, 22 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://hangenyuu.github.io/h_notes/posts/gan_p2/GAN-p2-1.png" medium="image" type="image/png" height="44" width="144"/>
</item>
<item>
  <title>A Primer on Generative Adversarial Networks (GANs)</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/gan_p1/index.html</link>
  <description><![CDATA[ 




<p>If you have studied deep learning before, you will notice that we will encounter classification many times. To be honest, it is fun in a way, having your own model to classify anime characters. Alas, it is a bit dry to me. Intelligence, for me, is creativity, the ability to create something <em>new</em>. I want a model that can create, especially work of art. That led me right to GANs, not so much a model but an elegant way of thinking.</p>
<section id="a-brief-history-of-gans" class="level1">
<h1>A brief history of GANs</h1>
<p><em>For a fuller account, check out the <a href="https://www.technologyreview.com/2018/02/21/145289/the-ganfather-the-man-whos-given-machines-the-gift-of-imagination/">MIT Technology Review article</a>.</em></p>
<p>Back in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from <a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a> to <a href="https://ar5iv.labs.arxiv.org/html/1409.1556">VGG</a>. (Not to mention <a href="https://ar5iv.labs.arxiv.org/html/1512.03385">ResNet</a> in 2015, an architecture with so interesting an idea that I had to <a href="https://github.com/HangenYuu/vision_learner/tree/main/ARCHITECTURE/CNN/Paper">make a project</a> for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, <a href="https://ar5iv.labs.arxiv.org/html/1406.2661">Generative Adversarial Nets</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/Screenshot 2023-01-20 at 19-53-43 Generative Dog Images Kaggle.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><em>The image was not from the era, but was representative of what you got from the model at that time (and still now with GANs, if your model was trained poorly or prematurely). <a href="https://www.kaggle.com/c/generative-dog-images/discussion/97753">Source</a></em></figcaption><p></p>
</figure>
</div>
<p>Now I wanted to make two quick detours before going into the inside of GANs:</p>
<ol type="1">
<li>At its core sense, a <em>function</em> is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is <em>trying to update its parameters such that the model will approximate the optimal function as closely as possible</em>. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.</li>
<li>Advances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.</li>
</ol>
</section>
<section id="the-gans-game" class="level1">
<h1>The GANs game:</h1>
<p><strong>Note:</strong> I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.</p>
<p>The word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called <em>Generator</em>, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called <em>Discriminator</em>, (or <em>Critic</em>, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Generator</strong></th>
<th><strong>Discriminator</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>Random numbers</td>
<td>Images (real &amp; generated)</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Images</td>
<td>Class of image (binary)</td>
</tr>
<tr class="odd">
<td><strong>Role</strong></td>
<td>Forger</td>
<td>Appraiser</td>
</tr>
</tbody>
</table>
<p><strong>Quick detour:</strong> the GAN concept advances generative AI the same way backpropagation does so. The approach of trying to know the distribution of the image features was right, but the method was wrong a.k.a too complex and computationally expensive. With GAN, we have an elegant way to start with any random distribution while moving towards the optimal distribution incrementally. No need to know everything any more.</p>
<p>Our loss function will be the good ol’ binary cross-entropy: <img src="https://latex.codecogs.com/png.latex?J(%5Ctheta)%20=%20-%5Cfrac%7B1%7D%7Bm%7D*%5By%5E%7B(i)%7Dlog(h(x%5E%7B(i)%7D,%20%5Ctheta))%20+%20(1%20-%20y%5E%7B(i)%7D)log(1%20-%20(h(x%5E%7B(i)%7D,%20%5Ctheta)))%5D"></p>
<p>That surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know <img src="https://latex.codecogs.com/png.latex?y%5E%7B(i)%7D"> is the true label of the i<sup>th</sup> example (0 or 1), <img src="https://latex.codecogs.com/png.latex?h(x%5E%7B(i)%7D,%20%5Ctheta)"> is the predicted label for the i<sup>th</sup> example with input <img src="https://latex.codecogs.com/png.latex?x%5E%7B(i)%7D"> and parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Define the BCE function</span></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;">def</span> bce(y_true, y_pred):</span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;">return</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">*</span>(y_true<span class="op" style="color: #5E5E5E;">*</span>torch.log(y_pred) <span class="op" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>y_true)<span class="op" style="color: #5E5E5E;">*</span>torch.log(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>y_pred))</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">y_true <span class="op" style="color: #5E5E5E;">=</span> torch.zeros(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-2">y_pred <span class="op" style="color: #5E5E5E;">=</span> torch.linspace(<span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-3"></span>
<span id="cb2-4">plt.figure()</span>
<span id="cb2-5">plt.plot(y_pred, bce(y_true, y_pred), <span class="st" style="color: #20794D;">"o--"</span>)</span>
<span id="cb2-6">plt.xlabel(<span class="st" style="color: #20794D;">"prediction"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb2-7">plt.ylabel(<span class="st" style="color: #20794D;">"loss"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb2-8">plt.grid()</span>
<span id="cb2-9">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-0" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/index_files/figure-html/fig-0-output-1.png" width="605" height="439" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: BCE loss when y = 0</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">y_true <span class="op" style="color: #5E5E5E;">=</span> torch.ones(<span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb3-2">y_pred <span class="op" style="color: #5E5E5E;">=</span> torch.linspace(<span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, <span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb3-3">criterion <span class="op" style="color: #5E5E5E;">=</span> torch.nn.BCELoss(reduction<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'none'</span>)</span>
<span id="cb3-4"></span>
<span id="cb3-5">plt.figure()</span>
<span id="cb3-6">plt.plot(y_pred, bce(y_true, y_pred), <span class="st" style="color: #20794D;">"o--"</span>)</span>
<span id="cb3-7">plt.xlabel(<span class="st" style="color: #20794D;">"prediction"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb3-8">plt.ylabel(<span class="st" style="color: #20794D;">"loss"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">23</span>)</span>
<span id="cb3-9">plt.grid()</span>
<span id="cb3-10">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://hangenyuu.github.io/h_notes/posts/gan_p1/index_files/figure-html/fig-1-output-1.png" width="605" height="439" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: BCE loss when y = 1</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>I mentioned that this is a conflict between Generator and Discriminator. For Discriminator, it wants to classify correctly i.e.&nbsp;catch the Generator every time while approve the value of the real images. In other words, it wants to <em>minimize</em> its loss function. For Generator, it wants the reverse i.e.&nbsp;pass a fake as a real to the Discriminator every single time. In other words, it wants to <em>maximize</em> the loss function (of the Discriminator). This leads to the ter <em>minimax game</em> that you may hear some people use to describe GAN.</p>
<p>The game can be considered complete when the Discriminator’s accuracy drops to 50% i.e.&nbsp;it can no longer discern, and essentially has to guess at random for each image. At this, our Generator will become potent enough to fool even us with its <a href="https://thispersondoesnotexist.com/">humans</a> and <a href="https://thiscatdoesnotexist.com/">cats</a>.</p>
</section>
<section id="end-of-part-1" class="level1">
<h1>End of part 1:</h1>
<p>As a primer this is far enough. I will continue on the subject, describing each model’s simplest architecture possible, the process of training one, as well as the difficulty in training GANs. (Training a model is hard enough, now we have two.)</p>
<p><em>All the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the <a href="https://ar5iv.labs.arxiv.org/">tool</a>. To change to the abstract page, follow this example:</em> <code>https://ar5iv.labs.arxiv.org/html/1409.1556</code> → <code>https://arxiv.org/abs/1409.1556</code>.</p>


</section>

 ]]></description>
  <category>code</category>
  <category>GAN</category>
  <category>From scratch</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/gan_p1/index.html</guid>
  <pubDate>Fri, 20 Jan 2023 16:00:00 GMT</pubDate>
  <media:content url="https://www.kaggle.com/c/generative-dog-images/discussion/97753" medium="image"/>
</item>
<item>
  <title>Hello World</title>
  <dc:creator>Pham Nguyen Hung</dc:creator>
  <link>https://hangenyuu.github.io/h_notes/posts/hello_world/index.html</link>
  <description><![CDATA[ 




<section id="up-and-running-with-quarto" class="level2">
<h2 class="anchored" data-anchor-id="up-and-running-with-quarto">Up and running with Quarto!</h2>
<div id="hello-world" class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Hello World!"</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Hello World!</code></pre>
</div>
</div>


</section>

 ]]></description>
  <category>new</category>
  <category>code</category>
  <guid>https://hangenyuu.github.io/h_notes/posts/hello_world/index.html</guid>
  <pubDate>Fri, 21 Oct 2022 16:00:00 GMT</pubDate>
</item>
</channel>
</rss>
