[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Pham Nguyen Hung. I am a Deep Learning Practitioner. The stage I am at (student, master, etc.) is bound to change over the years, so you are encouraged to view the Now tab for my current position. I have a knack for numbers and complex concepts, and love learning them. Outside that, I have an interest in books of all types, and arts (music, drawing). The word that motivates me is “polymath”, and the figure is Nassim Nicholas Taleb."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "H’s notes",
    "section": "",
    "text": "Deep Learning from the ground up - From tensor to multi-layer perceptron\n\n\nLesson 9 - 14 of fast.ai course, part 2\n\n\n\n\ncode\n\n\nFrom scratch\n\n\n \n\n\n\n\nMay 12, 2023\n\n\nPham Nguyen Hung\n\n\n3 min\n\n\n\n\n\n\n\n\nA micro AI tool\n\n\nA writtent demo (and some reflections here and there) for the project for the Intuition hackathon by the NTU branch of IEEE\n\n\n\n\ncode\n\n\nEngineering/Hacking\n\n\n \n\n\n\n\nFeb 28, 2023\n\n\nPham Nguyen Hung\n\n\n27 min\n\n\n\n\n\n\n\n\nConditional & Controllable GAN\n\n\nMy notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\nFrom scratch\n\n\n \n\n\n\n\nFeb 22, 2023\n\n\nPham Nguyen Hung\n\n\n17 min\n\n\n\n\n\n\n\n\nThat Unstable GAN\n\n\nMy notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\nFrom scratch\n\n\n \n\n\n\n\nJan 24, 2023\n\n\nPham Nguyen Hung\n\n\n11 min\n\n\n\n\n\n\n\n\nBuilding a simple GAN\n\n\nMy notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\nFrom scratch\n\n\n \n\n\n\n\nJan 23, 2023\n\n\nPham Nguyen Hung\n\n\n14 min\n\n\n\n\n\n\n\n\nA Primer on Generative Adversarial Networks (GANs)\n\n\nMy notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\nFrom scratch\n\n\n \n\n\n\n\nJan 21, 2023\n\n\nPham Nguyen Hung\n\n\n7 min\n\n\n\n\n\n\n\n\nHello World\n\n\n\n\n\n\n\nnew\n\n\ncode\n\n\n \n\n\n\n\nOct 22, 2022\n\n\nPham Nguyen Hung\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "My major is chemical engineering, but I want to be a deep learning practitioner.\n\n\n\nI studied it on Coursera. Two courses were released at different times, so there was a mismatched in module used (TensorFlow vs PyTorch), but that means that I can learn from both worlds. I often encountered champions for PyTorch, such as Jeremy Howard, who praised the module over TensorFlow, but I feel like I need to experience both to judge.\n\n\n\nJust go to a blog post and see one of my entry.\n\n\n\nYou can see them at my Github repo. For the philosophy of project-based and just-in-time learning, check out this post or this post.\n\n\n\nIt is not an easy market that I come into, with the news about tech layoffs in Singapore this 2023. Still, I do what I can do and have to do. My deadline is 29/01/2023, when the career fair at my school starts."
  },
  {
    "objectID": "posts/anw/index.html",
    "href": "posts/anw/index.html",
    "title": "Show me your portfolio",
    "section": "",
    "text": "Nassim Nicholas Taleb told me this when he talked about listening to pundits in The Black Swan (or was it Antifragile?). Derek Sivers shared the same idea about talking versus doing. The idea reminds me to pick my words carefully, when I give advice, and when I state my priority. It also reminds to tailor my daily schedule so that the priority is always at the top. (It is a nice idea, by the way, to speak and act on priority as singular rather than plural). The next time you say something, act on it to see if you really believe your words. And the next time you do something, stating clearly how you like doing that."
  },
  {
    "objectID": "posts/dlm/index.html",
    "href": "posts/dlm/index.html",
    "title": "Daily translation of Meditations to Vietnamese",
    "section": "",
    "text": "“Don’t trust in your reputation, money, or position, but in the strength that is yours—namely, your judgements about the things that you control and don’t control. For this alone is what makes us free and unfettered, that picks us up by the neck from the depths and lifts us eye to eye with the rich and powerful.” — EPICTETUS, DISCOURSES, 3.26.34–35\n\nĐừng tin vào danh vọng, tiền bạc, hay vị trí [của bản thân], mà hãy tin vào sức mạnh mà con nắm giữ—khả năng phân biệt giữa thứ mà con có thể kiểm soát và thứ mà con không thể. Bởi đây là thứ duy nhất giúp chúng ta được tự do và không bị ràng buộc, là thứ duy nhất kéo chúng ta thoát khỏi vực thẳm và đưa chúng ta ngang hàng với những kẻ giàu có và quyền lực.\nIt is something that is expected from Epictetus - the emphasis on the dichotomy of control. What surprises me is the last phrase. The ability to exercise the dichotomy of control and act accordingly gives us power and wealth. It is all based on the same principle: to have more, you can acquire more, or you can want less."
  },
  {
    "objectID": "posts/dlm/index.html#nov---2022-1",
    "href": "posts/dlm/index.html#nov---2022-1",
    "title": "Daily translation of Meditations to Vietnamese",
    "section": "08 - Nov - 2022",
    "text": "08 - Nov - 2022\n\n“Remember that you are an actor in a play, playing a character according to the will of the playwright—if a short play, then it’s short; if long, long. If he wishes you to play the beggar, play even that role well, just as you would if it were a cripple, a honcho, or an everyday person. For this is your duty, to perform well the character assigned you. That selection belongs to another.” — EPICTETUS, ENCHIRIDION, 17\n\nHãy nhớ rằng con là một diễn viên trong một vở kịch [mang tên cuộc đời], diễn một vai theo ý muốn của người viết kịch—nếu đó là một vở kịch ngắn, thì diễn ngắn; nếu nó dài, thì diễn dài. Nếu người viết kịch muốn con đóng vai một kẻ ăn mày, vậy thì hãy diễn vai đó thật tốt, cũng tương tự như vậy nếu con nhận được vai kẻ bị thọt, người đứng đầu, hay chỉ là một người bình thường. Bởi việc diễn thật tốt vai đã được giao chính là nghĩa vụ của con. Việc chọn vai nào [cho con] thuộc về những người khác.\nNow the last sentence raised my eyebrows. What? Let’s others choose for me? Ins’t that antithetical to “Essentialism”? That I need to prioritize my life instead of letting others prioritize it for me! But be calm - Epictetus did not talk about letting others choose. The others here referred to the higher being, chance, luck i.e. anything that we cannot control. It is probable that the course of our life is led to stranger lands because of some accidents. However, we are still in control: we can choose to get back on track after being thrown off-trackor not. That alone is important, that alone is enough."
  },
  {
    "objectID": "posts/ftf/index.html#data-analyst",
    "href": "posts/ftf/index.html#data-analyst",
    "title": "First thing (1)",
    "section": "Data Analyst:",
    "text": "Data Analyst:\n\nHere is a diagram from DataCamp that sums up the job descriptions well:\n\nFrom the job descriptions, you will need to know…\nBasic:\n\n…how to use a business intelligence platforms - Looker, Power BI, Tableau and the like. One quick look at DataCamp Data Analyst in Power BI track shows that Power BI is capable of step 2-4 in the diagram. You can prepare and transform data; you can explore data from basic (mean, median, etc.) to complex operations that you define yourself; you can do all kinds of visualizations; you can publish the findings to a report, an app (possibly dashboard), or slides to aid your presentation. And compared to frameworks in languages such as R or Python, these platforms look less daunting because they have a GUI (you still often need to “program stuff”). These are the main tools for your trade.\n…how to communicate (written and oral). At the end of anything at work, you will need to “give report to stakeholders”. Being a data analyst\n\nPreferred: These are specific TikTok’s preferences:\n\n…how to do statistical analysis. This is arguably “basic”, not “preferred”. The numbers you calculate in the platform, they all have statistical meanings, so it helps to know what you calculating. Understanding statistics also helps you to avoid"
  },
  {
    "objectID": "posts/ftf/index.html#data-engineer",
    "href": "posts/ftf/index.html#data-engineer",
    "title": "First thing (1)",
    "section": "Data Engineer:",
    "text": "Data Engineer:\nAs a LinkedIn post suggested, a data engineer’s job is “being in charge of the data pipeline”, making sure that people such as data scientists can have the data to do their jobs. I will admit that I know next to nothing about data engineer, so the following came from a certain “God-tier” roadmap that I picked up. (I am weak again anything “God-tier”, so please refrain from using that against me). I will guess that the job requirements will vary. The data you need to work on may just be stored in a single AWS S3, but there are cases where the data are distributed in 23 data centers around the world, and you need Vitess on top of MySQL to handle the requests successfully (totally not Youtube). In any case, a quick rundown of the “God-tier” roadmap reviews that you need:\n\nSQL (and NoSQL)\nDatabase system knowledge (data lake, data warehouse, etc.)\nLinux a.k.a working with command line interface (CLI)\nBig data framework (Hadoop, Spark, Kafka, depends on your company)\nCloud service knowledge (Google Cloud Product (GCP), Amazon Web Services (AWS), or Microsoft Azure - depends on your company)\nContainerization technology: Docker (and Kubernetes)\n\nYou may find the rest in the video and the attached document. Now onto the confusing topic…"
  },
  {
    "objectID": "posts/ftf/index.html#data-scientistmachine-learning-engineer",
    "href": "posts/ftf/index.html#data-scientistmachine-learning-engineer",
    "title": "First thing (1)",
    "section": "Data Scientist/Machine Learning Engineer:",
    "text": "Data Scientist/Machine Learning Engineer:\nFirst, let’s start with a diagram from DataCamp.\n\n\n\nSource: DataCamp\n\n\nThe things that confuse me"
  },
  {
    "objectID": "posts/ganlec/index.html",
    "href": "posts/ganlec/index.html",
    "title": "A Primer on Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "A brief history of GANs\nFor a fuller account, check out the MIT Technology Review article.\nBack in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from AlexNet to VGG. (Not to mention ResNet in 2015, an architecture with so interesting an idea that I had to make a project for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, Generative Adversarial Nets.\n\n\n\nThe image was not from the era, but was representative of what you got from the model at that time (and still now with GANs). Source\n\n\nNow I wanted to make two quick detours before going into the inside of GANs:\n\nAt its core sense, a function is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is trying to update its parameters such that the model will approximate the optimal function as closely as possible. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.\nAdvances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.\n\n\n\nThe GALN’s game:\nNote: I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.\nThe word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called Generator, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called Discriminator, (or Critic, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.\n\n\n\n\nGenerator\nDiscriminator\n\n\n\n\nInput\nRandom numbers\nImages (real & generated)\n\n\nOutput\nImages\nClass of image (binary)\n\n\nRole\nForger\nAppraiser\n\n\n\nOur loss function will be the good ol’ binary cross-entropy: \\[J(\\theta) = -\\frac{1}{m}*[y^{(i)}log(h(x^{(i)}, \\theta)) + (1 - y^{(i)})log(1 - (h(x^{(i)}, \\theta)))]\\]\nThat surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know \\(y^{(i)}\\) is the true label of the ith example (0 or 1), \\(h(x^{(i)}, \\theta)\\) is the predicted label for the ith example with input \\(x^{(i)}\\) and parameters \\(\\theta\\). With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the BCE function\ndef bce(y_true, y_pred):\n    return -1*(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n\n\n\n\nCode\ny_true = torch.zeros(50)\ny_pred = torch.linspace(0., 1., 50)\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 1: BCE loss when y = 0\n\n\n\n\n\n\nCode\ny_true = torch.ones(50)\ny_pred = torch.linspace(0., 1., 50)\ncriterion = torch.nn.BCELoss(reduction='none')\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 2: BCE loss when y = 1\n\n\n\n\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  },
  {
    "objectID": "posts/gan-p4-conditional/index.html",
    "href": "posts/gan-p4-conditional/index.html",
    "title": "Conditional & Controllable GAN",
    "section": "",
    "text": "Apologize for being late: it has been nearly 3 weeks already since the latest post. But I am back for the last week of content.\nWhen I started writing, it was near the time to bed, and without a doubt, I am hungry. So let’s deal with cookies this time.\nUp until now, our GAN has managed to do some interesting stuff (“writing” Kanji characters, or numbers, if you used the traditional MNIST dataset). However, one thing you must notice is that we have no control over what the Generator will give us. It can be a “na”, it can be a “tsu”, it can be a “ki” - no control whatsoever. In our cookie analogy, our current Generator is like a goodwilled roomie who bakes for us every day, but each day we will receive a random cookie type.\n\n\n\nYou know it is cookie, but you have no idea what type is it\n\n\nNow, if you love randomness and can tolerate the taste as well as the sugar, fine. But we usually want our model to be controllable, that is, we get to decide (to some extent) what will be included in the output.\n\n\n\nIt’s much nicer to control that you have matcha on Monday, chocochip on Tuesday, and so on.\n\n\nWith the objective set, let’s explore way to implement controllable GAN a.k.a way to make sure we have the correct cookie each day.\n\nLimiting to just one category:\nThis is a no-brainer solution. To prevent random category generation (and mode collapse as well), who don’t just feed in data of a single class only? It is like always mixing matcha powder into the dough to make the cookies, ensuring that every day we will get matcha cookies.\n\n\n\nA matcha cookie junkie’s dream.\n\n\nObviously this solution is for when you want to generate examples of one class only. One example would be augmenting data for brain EMR of a certain disease at a certain region. The other trolled example is GANyu, a dataset and models fine-tuned on it for the task of generating faces of the Genshin Impact character Ganyu (Check out the GA(N)Rem as well).\n\nI don’t know what is the thing for animes and GANs, but the moment I discovered GANs, I instantly thought of generating anime girls’ faces. Is is the same phenomenon as researchers in the 90’s instantly thought of classifying cat from everything else the moment they got a decent classifier… - A certain unfamous author on the web\n\nMoving to more general (and sensible) solution, we must take note of a crucial principle: we cannot generate something that the model has not ever seen before. It’s like we need to give matcha powder to our dear friend if we expect him to bake us some matcha cookies. This principle is handy in exploring the two solutions. The two approaches will both involve tampering with the input noise vector \\(z\\). While one focuses on the class/label of the generated, the other focuses on the features of the generated.\n\n\nConditional Generation (Control the class):\nConditional Generative Adversarial Nets was a solution to make GAN more controllable, by passing some extra information \\(y\\) (the easiest is class labels, as one-hot vector) with the data fed to Generator and Discriminator. Here is an illustrated example and implementation:\n\n\n\nAn input vector to Generator now is made up of two components: noise inputs (to ensure that each generation will be unique) & one-hot class vector (to ensure that the generate example will be of the class we want)\n\n\n\n\n\nSimilarly, an input to the Discriminator now is an image together with the an one-hot class vector. For an example to be regarded as real, not only it needs to look realistic (reflected by a low binary corss-entropy or Wasserstein loss) but it also needs to look like real examples from the same class. Here, an original matcha cookie definitely looks like a matcha cookie.\n\n\n\n\n\nHere is the earlier generated matcha cookie. Let’s say that our Discriminator is a bit better than the Generator. It means that it will detect this looks rather like a matcha cookie, but not too alike.\n\n\n\n\n\nNow suppose that the required class is black chocochip, but our Generator gives a matcha cookie. The Discriminator will recognize in this case and gives a low chance that the example is real.\n\n\nThe question now is how do we go on implementing this? From the descriptions, it seems that we need to do 2 things: 1) figure a way to pass the additional information into our two models and 2) update the loss function. 2) is trivial, as the same loss function (binary cross-entropy) can be used and we just need to make sure that the class of the examples are included in the output as well. For 1), in the case of Generator, you just need to concatenate it with the noise vector above. For Discriminator, it is a bit trickier. We feed the images in by passing values of three channels, so the simplest way will be to create n channels more for n classes. This way works for dataset such as the good ol’ MNIST, whether we flatten out images before concatentating or we keep the same matrix and just call torch.cat() (which will create 10 more channels, each of size 28*28, with one of them full of 1 and the rest full of 0). For larger images or ones we do not want to flatten, this simple approach will create a huge memory issue. We will want to pass class information in differently, such as via a different head of input layer, but that is outside of this post scope. Here are the code snippets for the easy case.\nimport torch\nimport torch.nn.functional as F\n\ndef get_one_hot_labels(labels: torch.Tensor, n_classes: int):\n    '''\n    Function for creating one-hot vectors for the data.\n\n    :param labels: a vector containing the labels of all examples in a batch.\n                   Get from each DataLoader. Have shape (n_samples, 1)\n    :param n_classes: an integer for number of classes in the data.\n                      Get from the dataset created\n    :return: the one-hot vector for a batch of data\n    '''\n    return F.one_hot(labels,n_classes)\n\ndef combine_vectors(x, y):\n    '''\n    Generic function for combining two 2-D maxtrices with the same 0-shape\n    In our case, they will be (n_samples, x_1) and (n_samples, y_1).\n\n    :param x: the first matrix, shape (n_samples, x_1)\n    :param y: the second matrix, shape (n_samples, y_1)\n    :return: the concatenated matrix of shape (n_samples, x_1 + y_1)\n    '''\n    # To ensure unity of data types, we want the return matrix to have float\n    # type.\n    combined = torch.cat((x.float(),y.float()), 1)\n    return combined\n# Code will not run if just copy-paste\n# Pre-training\n# Just the basic part.\nkmnist_shape = (1, 28, 28)\nn_classes = 10\n\ndevice = 'cuda' if torch.cuda.available() else 'cpu'\ncriterion = nn.BCEWithLogitsLoss()\nz_dim = 64 # Size of the noise vector\ngen = Generator(input_dim=generator_input_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\ndisc = Discriminator(im_chan=discriminator_im_chan).to(device)\ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\nepochs = 200\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init)\n\n# Training loop\ncur_step = 0\ngenerator_losses = []\ndiscriminator_losses = []\n\nfor epoch in range(epochs):\n    for real, labels in tqdm(dataloader):\n        n_samples = len(real)\n        real = real.to(device)\n\n        # Get image one-hot labels for this batch\n        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n        # Remember that the DataLoader is in size (n_samples, 1, 28, 28) while the one hot label matrix \n        # has size (n_samples, 1). We need to extend 2 more dimensions if we want to concatenate the two.\n        image_one_hot_labels = one_hot_labels[:, :, None, None]\n        # Now the one-hot labels matrix has size (n_samples, 1, 1, 1). We need to turn it into\n        # (n_samples, 1, 28, 28) to pass into the input layer.\n        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, kmnist_shape[1], kmnist_shape[2])\n\n        ### Update discriminator\n        # Zero out the discriminator gradients\n        disc_opt.zero_grad()\n\n        # Get noise corresponding to the current batch_size \n        fake_noise = get_noise(n_samples, z_dim, device=device)\n        \n        # Combine the label and the noise and generate fake examples\n        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n        fake = gen(noise_and_labels)\n\n        # Get Discriminator's predictiopn on the real and the fake examples\n        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n        disc_fake_pred = disc(fake_image_and_labels.detach()) # do not update the Generator\n        disc_real_pred = disc(real_image_and_labels)\n\n        # Calculate loss\n        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n\n        # Backpropagation\n        disc_loss.backward(retain_graph=True)\n\n        # Update the parameters\n        disc_opt.step()\n\n        # Keep track of the average discriminator loss for visualization\n        discriminator_losses += [disc_loss.item()]\n\n        ### Update generator\n        # Zero out the generator gradients\n        gen_opt.zero_grad()\n\n        # Regenerate the fake examples with gradients to update\n        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n        disc_fake_pred = disc(fake_image_and_labels)\n\n        # Calculate loss\n        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n\n        # Backpropgation\n        gen_loss.backward()\n\n        # Update the parameters\n        gen_opt.step()\n\n        # Keep track of the generator losses for visualization\n        generator_losses += [gen_loss.item()]\n\n\nControllable Generation (Control the feature):\nUp until now, the noise vector fed into GANs is just “noise” - meaningless. However, the numbers in the noise vector do mean something. You can think that each number represent one feature that is recognized by the Generator. The combinations of all these features form a latent space - a space containing a simpler but hidden (to humans) representation of generated examples. This is best understood with the example of amino acid.\nThe basic of every bodily function is protein, which is a chain of amino acids (you don’t need to know what they are). Each amino acid in the chain is encoded as a sequence of 3 nucleotides, which have 4 in total (there are \\(4^{3}=64\\) total combinations, but several combinations encoding the same amino acid, and there are special ones called ending combinations that signify the end but do not encode).\nAll the 64 combinations can thought of as the latent space of the amino acid. It’s like we have a well-trained Generator on 22 classes that output the exact amino acid or ending signal we want by passing into it certain 3 nucleotides. This is a latent space because the information has been simplified, but it is not quite latent because we now know the exact encoding of the information.\n\n\n\n“The genetic code” by OpenStax College, Biology\n\n\nIn our KMNIST example, each image can be represented as a 28 by 28 matrix where each position stores the intensity of the pixel. It can be visualized as below.\n\n\n\nA certain Japanese letter\n\n\nIn the noise vector above, we try to compress this information down to a vector of 64 numbers (recalling the z_dim above), hoping that this is sufficient to store the information to construct all 10 classes of handwritten kanji characters. But let’s fall back to our cookies for a more easily visualizable example.\n\n\n\nBefore, we have been generating random numbers in the noise vector. Let’s say that we have been able to decode that the second number in our noise vector encodes information for color of the cookie, with 2.1 signifies the green matcha color we wants.\n\n\n\n\n\nNow, after training, we now know that 1.4 corresponds to the brown color of chochip cookies. We can now pass the number to get a brown cookie.\n\n\nIn reality, there are multiple things to note in implementation. One unfortunate thing was the DNA analogy extends to the noise space. A feature is often not influenced by a single value of the noise vector alone but depends on many ones. This is called entanglement, which mostly arises from having a noise vector with dimension smaller than the number of features we want to control. Entanglemnt affects our controllability: if two or more features’ values significantly depend on the same noise value, then changing it will shift all of them while we may want one to change. Therefore, we want to encourage disentanglement of features in two ways:\n\nEnsure noise vector has enough data slots. You cannot expect disentanglement of 10 features if your noise vector only has 9 slots. Always have a noise vector with dimension at least the number of modifiable features you want\nAs a regularization.\n\nIn practice, we will not learn the exact encoding (such as 2.1 for green or 1.4 for brown as above) but how the feature change with varying number (say, from green to brown by decreasing the \\(2^{nd}\\) number of the noise vector). You do this with, well, a classifier and label. First, you freeze the weight of the Generator. Then you classify the generated examples based on whether they have the feature(s) or not. Afterwards, you update the noise vector based on the loss function with backpropagation. That is the most simple (and laziest) way to update the noise vector, making it the greatest way (for we always want to do the most work with the least effort). Of course, now we need a pre-trained classifier on the feature(s) that we are trying to detect. If we do not, then we will need to train one on our own i.e. more work to do. You can observe the gradual change in the demo video below for the famous paper on the subject Interpreting the Latent Space of GANs for Semantic Face Editing.\n\nHere’s the implementation\n# Again, this is not a full implementation\n# The images we will work with now are RGB\nimport torch\n\nz_dim = 64\nbatch_size = 128\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nn_classes = 40\n\ngen = Generator(z_dim).to(device)\n# Magically train the model or load a pretrained one\n# Put the pretrained model on evaluation mode\ngen.eval()\n\n# Defined class Classifier above\nclassifier = Classifier(n_classes=n_classes).to(device)\n# Really load a pretrained model. Look for details at https://pytorch.org/tutorials/beginner/saving_loading_models.html\nclass_dict = torch.load(\"pretrained_classifier.pth\", map_location=torch.device(device))[\"classifier\"]\nclassifier.load_state_dict(class_dict)\nclassifier.eval()\n\n# Here is the optimizer. We have frozen the weight of the classifier with .eval()\n# so only the noise gets updated.\nopt = torch.optim.Adam(classifier.parameters(), lr=0.01)\n\n# Gradient ascent for the noise\ndef calculate_updated_noise(noise, weight):\n    '''\n    Update and return the noise vector with gradient ascent\n    :param noise: the old noise vector \n    :param weight: the weights to update each noise value. An analogy\n                   to the learning rate, but for each noise value\n    :return: the updated noise vector\n    '''\n    return noise + ( noise.grad * weight)\n\n# Regularization for disentanglement - and also the scoring function to update noise\ndef get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):\n    '''\n    Function to get the score of the update. Reward change in the target feature(s) to\n    change and penalize changes in other features.\n    :param current_classifications: the classifications associated with the current noise\n    :param original_classifications: the classifications associated with the original noise     \n    :param target_indices: the index of the target feature\n    :param other_indices: the indices of the other features\n    :param penalty_weight: the amount that the penalty should be weighted in the overall score\n\n    :return: the score for the current update. \n    '''\n    # Penalize change in other features\n    other_distances = current_classifications[:,other_indices] - original_classifications[:,other_indices]\n    other_class_penalty = -torch.norm(other_distances, dim=1).mean() * penalty_weight\n    \n    # Reward change in the target feature(s)\n    target_score = current_classifications[:, target_indices].mean()\n    return target_score + other_class_penalty\n\n### Generation time!\n# The dataset of choice was CelebA, and here's the list of feature\nfeature_names = [\"5oClockShadow\", \"ArchedEyebrows\", \"Attractive\", \"BagsUnderEyes\", \"Bald\", \"Bangs\",\n\"BigLips\", \"BigNose\", \"BlackHair\", \"BlondHair\", \"Blurry\", \"BrownHair\", \"BushyEyebrows\", \"Chubby\",\n\"DoubleChin\", \"Eyeglasses\", \"Goatee\", \"GrayHair\", \"HeavyMakeup\", \"HighCheekbones\", \"Male\", \n\"MouthSlightlyOpen\", \"Mustache\", \"NarrowEyes\", \"NoBeard\", \"OvalFace\", \"PaleSkin\", \"PointyNose\", \n\"RecedingHairline\", \"RosyCheeks\", \"Sideburn\", \"Smiling\", \"StraightHair\", \"WavyHair\", \"WearingEarrings\", \n\"WearingHat\", \"WearingLipstick\", \"WearingNecklace\", \"WearingNecktie\", \"Young\"]\n\ngrad_steps = 10\nfake_image_history = []\n\ntarget_indices = feature_names.index(\"Smiling\") # Feel free to change this value\nother_indices = [cur_idx != target_indices for cur_idx, _ in enumerate(feature_names)]\nnoise = get_noise(n_images, z_dim).to(device).requires_grad_() # Must have grad for gradient ascent\noriginal_classifications = classifier(gen(noise)).detach() # But we don't need gradients for classifier\n\nfor i in range(grad_steps):\n    # Empty the optimizer\n    opt.zero_grad()\n\n    # Generate a batch of fake examples\n    # and add to history\n    fake = gen(noise)\n    fake_image_history += [fake]\n\n    # Calculate scoring function\n    fake_score = get_score(\n        classifier(fake), \n        original_classifications,\n        target_indices,\n        other_indices,\n        penalty_weight=0.1\n    )\n    fake_score.backward() # Automatically calculate noise gradients\n    noise.data = calculate_updated_noise(noise, 1 / grad_steps)\nHere is a fake_image_history for training for feature \"Smiling\":\n\nThe faces looked distorted. There’s no denying it. The reason may be correlation between features. For example, the model may not be able to generate a smiling face without creating one with a slightly open mouth. Moreover, the model may modify unlabeled features while modifying the target feature(s) and we cannot penalize them with this method.\n\n\nConclusion\nLet’s compare Conditional and Controllable Generation. For Conditional Generation, we want a specific class, while for Controllable Generation, we want a specific feature for the output. Secondly, if data for Conditional Generation must contain class information, data for Controllable Generation must contain the feature(s) that we want (we cannot get matcha cookie from if we don’t have matcha powder); however, the data now do not need labeling. In both ways, we influence the input by the noise vector \\(z\\), but if we concatenate it with class information before, now we try to directly modify the noise vector."
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Code\nprint(\"Hello World!\")\n\n\nHello World!"
  },
  {
    "objectID": "posts/gan-p3/index.html",
    "href": "posts/gan-p3/index.html",
    "title": "That Unstable GAN",
    "section": "",
    "text": "In the last two posts, I have gone over the concept and process of training GANs. I made it seem so straightforward, but reality is harsher. Recall that for GANs, we are training two neural networks at once, and these two compete with each other to mutually improve. Much like real life, if one competitor is too good, the other will not learn anything, if the at point before the end the Generator or the Discriminator becomes too good (or the other becomes too bad), then training breaks down. Thus, training GANs is highly unstable, and any way to ensure smoother training is encouraged."
  },
  {
    "objectID": "posts/gan-p3/index.html#activation-function",
    "href": "posts/gan-p3/index.html#activation-function",
    "title": "That Unstable GAN",
    "section": "Activation function",
    "text": "Activation function\nActivation function is a requirement for neural networks’ ability to approximate complex function. Without it, a neural network will become just another linear function.\n\n\nCode\nimport torch\nimport torch.nn.functional as F\n\nimport numpy as np\nimport seaborn as sb\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(17)\ntorch.manual_seed(17)\ndef linear(a, b, x):\n    return a*x + b\n\n\n\n\nCode\nx = torch.randn(50)\n\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, linear(.5, 4, x) + linear(3.56, -5.32, x) + linear(-1.86, 3.74, x), 'o--')\nax2.plot(x, torch.relu(0.5*x) + torch.relu(3.56*x) + torch.relu(-1.86*x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n\n\n\n\n\nFigure 1: Stacking linear functions on top of each other is just a linear function. Meanwhile, stacking ReLU functions on top of each other create a piecemeal linear function that approximates a curve.\n\n\n\n\nWe all starts with the sigmoid function in a binary cross-entropy problem. However, sigmoid, together with tanh, leads to the “vanishing gradient” problem. When the output value of gets close to 0 or 1 for sigmoid (or -1 or 1 for tanh), the gradient gets close to 0, so the weights either are updated very slowly or stop learning altogether. That was when ReLU came into play: the function has a clear, positive gradient when output value is greater than 0, while the bend makes sure that ReLU stacking on each other can produce a curve.\n\n\n\nEach neural network had three hidden layers with three units in each one. The only difference was the activation function. Learning rate: 0.03, regularization: L2. Source\n\n\nHowever, the joy ReLU brought came to halt when “dying ReLU” problem was reported. Suppose we have an output smaller or equal 0, then our derivative will be 0. The 0 derivative on the node means that it will not get updated, and that’s the end for it. Worse, the previous components connected to the node are affected as well, so the whole structure of our neural network will be “dead”. To fix, we have the variation: LeakyReLU. For LeakyReLU, the output value below 0 is not set at 0, but is multiplied by a constant (such as 0.2). Gradient for such value will still be non-zero, provide information to update the weights.\nAnother, more advanced variation is GeLU, where the output is multiplied with i.e. weighted by its percentile. Sounds too complicated? Look at the formula: \\[GELU(x)=x*P(X<x)=x*\\Phi(x)\\] for \\(X\\) ~ \\(\\mathcal{N}(0, 1)\\)\nGELU has been successfully applied in Transformer models such as BERT, GPT-3, and especially in CNN such as ConvNeXts. (Yeah, look at ConvNeXts - it started with a ResNet, the great ConvNet architecture, then the authors slowly introduced all the modern training tricks, until the result surpassed the Swin Transformer in the cheer of CNN-backer/Transformer-haters. Okay, that was eaxaggerating, but still…)\n\n\nCode\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, F.leaky_relu(x, negative_slope=0.1), 'o--')\nax2.plot(x, F.gelu(x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n\n\n\n\n\nFigure 2: LeakyReLU and GELU\n\n\n\n\nNow let’s move on to the second general trick that we have already done: batch normalization."
  },
  {
    "objectID": "posts/gan-p3/index.html#batch-normalization",
    "href": "posts/gan-p3/index.html#batch-normalization",
    "title": "That Unstable GAN",
    "section": "Batch normalization",
    "text": "Batch normalization\nWe all know that neural netowrk is trying to appromixate a certain way of mapping inputs i.e. data to outputs. The parameters of a neural network therefore depend on the data we receive, characteristically the distribution of the data. Here I have this example of an HDR image, which captures a farther range of color and exposure than a compressed format such as JPG or PNG. I found the original image from the Internet here\n\n\n\nThe curve at the bottom that may remind you of a bell curve is the curve for the distribution of pixel values a.k.a colors\n\n\nNow, we train a neural network on data having similar color distribution such as this image, possibly for the task of recognizing grass. The model was trained well. Alas, the testing image contains one such as this\n\n\n\nThis was the exact same image, but compressed at a differen color distribution (shifted to the right)\n\n\nHere we say that the data distribution has shifted between training data and testing data. This generally will cause model problems (decrease accuracy, etc.). Data distribution shift (or covariate shift) can also happen between batches of training data, leading to slow convergence (imagine the model has to take a zig-zag path instead of a straight one). This can be dealt with by normalization, where make sure that the distributions of the training set and the testing set are similar e.g. centered around a mean of 0 and a standard deviation of 1. This could be done by taking the mean and standard deviation for each training batch of image and normalize the inputs of each training batch, then take the accumulated statistics to normalize the testing set during testing. This will smooth out the cost function and increases model performance (you might not need to do this if your training set and testing set are already similar to each other).\nHowever, model is susceptible to internal covariate shift as well, where the activation output distributions shift between each layer. This can happen due to the change in the weights of each layer. Batch normalization came into play here by normalizing the inputs to each layer (“batch” means that we do so for each batch of image). For example, supposed are at nueron \\(i\\) of non-last layer \\(l\\), with activated output from the last layer to this neuron being \\(a_{i}^{[l-1]}\\). The logit out of this neuron will be \\[z_{i}^{[l]}=\\Sigma W_{i}^{[l]}a_{i}^{[l-1]}\\]\nWithout batch normalization, the logit will be passed into activation to output \\(a_{i}^{[l]}\\). But here, we will perform batch normalization:\n\nWe get the statistics mean \\(\\mu _{z_{i}^{[l]}}\\) and variance \\(\\sigma _{z_{i}^{[l]}} ^{2}\\) for the batch.\nWe use them in the formula \\[\\hat{z}_{i}^{[l]}=\\frac{z_{i}^{[l]}-\\mu _{z_{i}^{[l]}}}{\\sqrt{\\sigma _{z_{i}^{[l]}} ^{2} + \\epsilon}}\\] Nothing too fancy - it’s just the normalization formula that you encounter in any statistics course/textbook: substract the value by the mean, then divide it by the square root of variance a.k.a the standard deviation. The \\(\\epsilon\\) term is a positive constant there to make sure that the denominator is always positive.\nWe map the normalized value \\(\\hat{z}_{i}^{[l]}\\) to a new distribution with the formula \\[y_{i}^{[l]}=\\gamma*\\hat{z}_{i}^{[l]} + \\beta\\] where \\(\\gamma\\) is scale factor and \\(\\beta\\) the shift factor. These two are learnable inputs in the batch normalization layer, and will be tuned to figure out the best distribution for the task at hand.\nWe pass \\(y_{i}^{[l]}\\) through the activation function to the output \\(a_{i}^{[l]}\\).\n\nThe batch normalization layer seems complicated, but we usually does not need to all the things. As backpropagation is reduced to just calling loss.backward in PyTorch, the nn.BatchNorm2d() (for images) will take care of this during training.\nThere is another normalization method called layer normalization. I will not go into details here, though I very much want to because it was used in the training of ConvNeXts as well (seriously, I want to make a blog post just about the tricks used in pushing this CNN to surpass Swin). Here is a post about the two normalizations that also have great images. In PyTorch, this is implemented in nn.LayerNorm()."
  },
  {
    "objectID": "posts/gan-p3/index.html#wgan",
    "href": "posts/gan-p3/index.html#wgan",
    "title": "That Unstable GAN",
    "section": "WGAN:",
    "text": "WGAN:\nFirst, we need to talk about mode collapse. Now, a mode in statistical term is the value that we are most likely to get from a distribution (not too correct for continuous distribution, but still great for understanding). This will be represented by a peak in the data distribution, such as the mean in a normal distribution. A distribution can have just one mode, like the normal distribution, or multiple modes like below.\n\n\nCode\nsample1 = np.random.normal(loc=20, scale=5, size=300)\nsample2 = np.random.normal(loc=40, scale=5, size=700)\n# Concatenating the two sample along the second axis\nsample = np.hstack((sample1, sample2))\n\nsb.kdeplot(sample)\nplt.show()\n\n\n\n\n\nFigure 3: A bimodal distribution created by merging two normal distributions\n\n\n\n\nThe outputs have their modes alright. For example, in our KMNIST dataset, there are 10 modes for the output, corresponding to 10 characters. So we have a new way to think about training: we are trying to make the model learn to shift the distribution of the outputs to approximate the one we want. For illustration, suppose initially our model is outputing each value for each pixel randomly, leading to an output distribution like this.\n\n\n\nInitial output distribution\n\n\nWe want to change the output to this kind of distribution\n\n\n\nWe want to shift from the circle to just 10 peaks i.e. just outputting 1 from 10 classes at a time\n\n\nIn the ideal scenario, our model will be guided by the loss function to make the right shift. However, notice a lack in the BCE loss: it only promotes the model generating images close to real images, regardless of the class of the image. This means that there exists a quick n’ dirty way for the Generator to reduce the loss by only generating images from 1 class that the Discriminator is most fooled by. So there exists a case where we end up with a Generator that outputs very realistic kanji character, only that it generates kanji character, say, only “tsu”. That’s boring.\nMy last paragraph gives hint to the source of the problem: our loss function. BCE loss works to push the Generator forward, but it cannot capture the information of class within the image. We need something else. And that something else is Wasserstein GAN, introducing a new kind of loss function: the Wasserstein Loss (no surprise) a.k.a the Earth Mover’s distance, or EM distance. It measures the difference between two distributions, and can be informally defined as the least amount of energy required to move and mold a earth pile in the shape of one distribution to the shape of another distribution (hence earth mover).\nIn mathematical form, if we have the noise vector \\(z\\), the fake image data \\(x\\), the Generator model \\(g()\\), the Discriminator who becomes the Critic \\(c()\\), then the Wasserstein Loss is the difference between the expected value i.e. the mean of the Critic outputs for real images and the mean of the Critic outputs for generated images. \\[WLoss=E(c(x))-E(c(g(z)))\\]\nWe still have a minimax game here, with the Generator’s goal being minimize the above difference and the Critic’s goal being maximize the above difference. Notice also that the Critic now is no longer a classifier: it can give any real value possible e.g. higher score for real(istic) examples. This means that the Generator will get useful feedback for all classes of examples, and is less prone to mode collapse. Getting rid of the classifier i.e. the sigmoid function in the output also means that vanishing gradient is also less likely. Two birds, one stone.\nW-Loss has one condition: the function of the Critic should be 1-Lipschitz Continuity. That looks intimidating but it just means that the norm of the gradient (the value in 2D math, the \\(\\sqrt{x^{2}+y^{2}+...}\\) as example in higher dimensions) for the Critic can be at most 1 at any point. In other words, the output of the Critic cannot increase/decrease more than linearly at any point. To achieve this, the first proposed (and terrible way, according to the original author) was weight clipping - forcing the weights to a fixed interval, say, [0,1]. Any negative value will be set to 0, and any value more than 1 will be set to 1. This was terrible (I have to say it again) because it limits the potential of the Critic. Another less strict way is gradient penalty (the first dead ar5iv link), where you add a regularization term in the loss function to promote the Critic to be 1-Lipschitz Continuity, as oppposed to forcing it. Formula is \\[WLoss=E(c(x))-E(c(g(z))) + \\lambda * pen\\] with \\(pen=E((|| \\nabla c(\\hat{x})||_{2}-1)^{2})\\), \\(\\hat{x}=\\epsilon x + (1-\\epsilon)g(z)\\)\nFor completeness, the Critic gradient needs checking at every point in the feature space, which is impractical. What we do is sampling some points from real examples, and then some points from generated examples with weights for each, and then we calculate the penalty for the interpolated examples. An example for the code of WGAN can be found here (nothing for now). For a more technical review of WGAN, check out this paper, also available as a blog post.\nNext in line: Conditional GANs.\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  },
  {
    "objectID": "posts/gan_p2/index.html",
    "href": "posts/gan_p2/index.html",
    "title": "Building a simple GAN",
    "section": "",
    "text": "In the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, KMNIST."
  },
  {
    "objectID": "posts/gan_p2/index.html#the-dataset",
    "href": "posts/gan_p2/index.html#the-dataset",
    "title": "Building a simple GAN",
    "section": "The dataset:",
    "text": "The dataset:\nFirst rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.\n\n\n\nThe 10 classes of Kuzushiji-MNIST, with the first column showing each character’s modern hiragana counterpart. Source\n\n\nSimilar to MNIST, a KMNIST image has only one channel. Let’s visualize one.\n\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n\n\n# Function learnt from GAN's Specialization Course 1 Week 1\ndef tensor_show(image_tensor, num_images=25, size=(1, 28, 28)):\n    # The original image tensor could be stored on GPU and \n    # have been flattened out for training, so we restore it\n    # first.\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    # torch uses (color channel, height, width) while \n    # matplotlib used (height, width, color channel)\n    # so we fix it here\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n\n\n# Download needs to be set to True the first time you run it.\nbatch_size = 32\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=False, transform=transforms.ToTensor()),\n    batch_size=batch_size,\n    shuffle=True)\n\n\nimage_tensor = next(iter(dataloader))[0]\ntensor_show(image_tensor)"
  },
  {
    "objectID": "posts/gan_p2/index.html#the-discriminator",
    "href": "posts/gan_p2/index.html#the-discriminator",
    "title": "Building a simple GAN",
    "section": "The Discriminator:",
    "text": "The Discriminator:\n\nThe architecture for each block of Discriminator and Generator follows the suggestions from the Deep Convolutional GAN paper.\n\nThe Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol’ linear model, but I will skip a bit to the year 2015, when DCGAN was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A non-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN’s training. We will touch upon tricks to stabilize GAN’s training in the next post).\n\nclass Discriminator(nn.Module):\n    def __init__(self, image_channel=1, hidden_dim=56):\n        super().__init__()\n        self.disc = nn.Sequential(\n            self.make_disc_block(image_channel, hidden_dim),\n            self.make_disc_block(hidden_dim, hidden_dim),\n            self.make_disc_block(hidden_dim, 1, final_layer=True),\n        )\n\n    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                    nn.BatchNorm2d(output_channels),\n                    nn.LeakyReLU(negative_slope=0.25)\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n            )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.disc(x)\n        # The input can be a tensor of multiple images\n        # We want to return a tensor with the possibility\n        # of real/fake for each image.\n        return x.view(len(x), -1)"
  },
  {
    "objectID": "posts/gan_p2/index.html#the-generator",
    "href": "posts/gan_p2/index.html#the-generator",
    "title": "Building a simple GAN",
    "section": "The Generator:",
    "text": "The Generator:\nA point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially distilling the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the transposed convolution operation, which increases the dimensions of data, essentially magnifying the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this notebook as my draft.)\nFirst, we need a function to generate noise. Basically, we need some tensor containing random numbers, and we can conveniently return a tensor filled with random numbers from a normal distribution with torch.randn(). For the dimensions, we define argument z_dim as the dimension of the noise input, and n_samples as the number of samples we need.\n\ndef generate_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn((n_samples, z_dim), device=device)\n\nFor the Generator class, I will also create a function to construct each layer. A non-last layer will have a transposed convolution, followed by batch normalization and ReLU activation. The final layer does not have batch normalization but will have Tanh activation to squish the pixels in range.\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim=14, image_channel=1, hidden_dim=56):\n        # z_dim is the dimension of the input noise vector\n        self.z_dim = z_dim\n        super().__init__()\n        self.gen = nn.Sequential(\n            self.make_gen_block(z_dim, hidden_dim * 4),\n            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n            self.make_gen_block(hidden_dim * 2, hidden_dim),\n            self.make_gen_block(hidden_dim, image_channel, kernel_size=4, final_layer=True),\n        )\n    \n    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.ReLU()\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh()\n            )\n    # Recall torch expect an image to be in the form (color channel, height, width).\n    # In a batch, torch expects it to be (no. of images in batch, color channel, height, width)\n    # So we need to transform the noise, originally in (no. of images in batch, input dimension)\n    # to (no. of images in batch, input dimension, 1, 1)\n    # See more here:\n    # https://pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images\n    def unsqueeze_noise(self, noise):\n        return noise.view(len(noise), self.z_dim, 1, 1)\n\n    def forward(self, noise):\n        x = self.unsqueeze_noise(noise)\n        return self.gen(x)"
  },
  {
    "objectID": "posts/gan_p2/index.html#optimizers-and-criterion",
    "href": "posts/gan_p2/index.html#optimizers-and-criterion",
    "title": "Building a simple GAN",
    "section": "Optimizers and Criterion",
    "text": "Optimizers and Criterion\nNext, we want to define our optimizers (one for each model) and our criterion.\n\n# We do not have activation at the output for Discriminator, so the outputs\n# are raw (logits).\ncriterion = nn.BCEWithLogitsLoss()\nz_dim = 64\ndisplay_step = 500\nbatch_size = 1000\n# Learning rate of 0.0002 and beta_1 (momentum term for Adam optimizer) of \n# 0.5 works well for DCGAN, according to the paper (yes, I seriously searched\n# for keyword \"learning rate\" in the paper)\nlr = 0.0002\nbeta_1 = 0.5 \nbeta_2 = 0.999\n# Device-agnostic code\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# You can tranform the image values to be between -1 and 1 (the range of the Tanh activation)\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n])\n\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=False, transform=transform),\n    batch_size=batch_size,\n    shuffle=True)\n\n\ngen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\ndisc = Discriminator().to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n\n# You initialize the weights to the normal distribution\n# with mean 0 and standard deviation 0.02\n# (Yes, the paper said so.)\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\n# Apply recursively weights_init() according to the docs:\n# https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init)"
  },
  {
    "objectID": "posts/gan_p2/index.html#training",
    "href": "posts/gan_p2/index.html#training",
    "title": "Building a simple GAN",
    "section": "Training",
    "text": "Training\nOkay, now onto training!\nn_epochs = 100\ncur_step = 1 # For visualization purpose\nmean_generator_loss = 0\nmean_discriminator_loss = 0\nfor epoch in range(n_epochs):\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n        real = real.to(device)\n\n        ## Update Discriminator\n\n        # Empty the optimizer\n        disc_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise = generate_noise(cur_batch_size, z_dim, device=device)\n        fake = gen(fake_noise)\n        disc_fake_pred = disc(fake.detach())\n\n        # Calculate loss\n        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n\n        # Same for real examples\n        disc_real_pred = disc(real)\n        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n\n        # The Discriminator's loss is the average of the two\n        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n\n        # Keep track of the average Discriminator loss\n        mean_discriminator_loss += disc_loss.item() / display_step\n\n        # Update Discriminator's gradients a.k.a backpropagation\n        # Normally don't set retain_graph=True, but we do so for GAN\n        # as we need to propagate through the graph a second time\n        # when updating the Generator.\n        disc_loss.backward(retain_graph=True)\n\n        # Update Discriminator's optimizer\n        disc_opt.step()\n\n        ## Update Generator\n\n        # Empty the optimizer\n        gen_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise_2 = generate_noise(cur_batch_size, z_dim, device=device)\n        fake_2 = gen(fake_noise_2)\n        disc_fake_pred = disc(fake_2)\n\n        # Calculate loss\n        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n\n        # Backpropagation for Generator's loss\n        gen_loss.backward()\n\n        # Update Generator's optimizer\n        gen_opt.step()\n\n        # Keep track of the average Generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ## Visualization code\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n            tensor_show(fake)\n            tensor_show(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1"
  },
  {
    "objectID": "posts/gan_p1/index.html",
    "href": "posts/gan_p1/index.html",
    "title": "A Primer on Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "If you have studied deep learning before, you will notice that we will encounter classification many times. To be honest, it is fun in a way, having your own model to classify anime characters. Alas, it is a bit dry to me. Intelligence, for me, is creativity, the ability to create something new. I want a model that can create, especially work of art. That led me right to GANs, not so much a model but an elegant way of thinking.\n\nA brief history of GANs\nFor a fuller account, check out the MIT Technology Review article.\nBack in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from AlexNet to VGG. (Not to mention ResNet in 2015, an architecture with so interesting an idea that I had to make a project for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, Generative Adversarial Nets.\n\n\n\nThe image was not from the era, but was representative of what you got from the model at that time (and still now with GANs, if your model was trained poorly or prematurely). Source\n\n\nNow I wanted to make two quick detours before going into the inside of GANs:\n\nAt its core sense, a function is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is trying to update its parameters such that the model will approximate the optimal function as closely as possible. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.\nAdvances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.\n\n\n\nThe GANs game:\nNote: I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.\nThe word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called Generator, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called Discriminator, (or Critic, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.\n\n\n\n\nGenerator\nDiscriminator\n\n\n\n\nInput\nRandom numbers\nImages (real & generated)\n\n\nOutput\nImages\nClass of image (binary)\n\n\nRole\nForger\nAppraiser\n\n\n\nQuick detour: the GAN concept advances generative AI the same way backpropagation does so. The approach of trying to know the distribution of the image features was right, but the method was wrong a.k.a too complex and computationally expensive. With GAN, we have an elegant way to start with any random distribution while moving towards the optimal distribution incrementally. No need to know everything any more.\nOur loss function will be the good ol’ binary cross-entropy: \\[J(\\theta) = -\\frac{1}{m}*[y^{(i)}log(h(x^{(i)}, \\theta)) + (1 - y^{(i)})log(1 - (h(x^{(i)}, \\theta)))]\\]\nThat surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know \\(y^{(i)}\\) is the true label of the ith example (0 or 1), \\(h(x^{(i)}, \\theta)\\) is the predicted label for the ith example with input \\(x^{(i)}\\) and parameters \\(\\theta\\). With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the BCE function\ndef bce(y_true, y_pred):\n    return -1*(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n\n\n\n\nCode\ny_true = torch.zeros(50)\ny_pred = torch.linspace(0., 1., 50)\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 1: BCE loss when y = 0\n\n\n\n\n\n\nCode\ny_true = torch.ones(50)\ny_pred = torch.linspace(0., 1., 50)\ncriterion = torch.nn.BCELoss(reduction='none')\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 2: BCE loss when y = 1\n\n\n\n\nI mentioned that this is a conflict between Generator and Discriminator. For Discriminator, it wants to classify correctly i.e. catch the Generator every time while approve the value of the real images. In other words, it wants to minimize its loss function. For Generator, it wants the reverse i.e. pass a fake as a real to the Discriminator every single time. In other words, it wants to maximize the loss function (of the Discriminator). This leads to the ter minimax game that you may hear some people use to describe GAN.\nThe game can be considered complete when the Discriminator’s accuracy drops to 50% i.e. it can no longer discern, and essentially has to guess at random for each image. At this, our Generator will become potent enough to fool even us with its humans and cats.\n\n\nEnd of part 1:\nAs a primer this is far enough. I will continue on the subject, describing each model’s simplest architecture possible, the process of training one, as well as the difficulty in training GANs. (Training a model is hard enough, now we have two.)\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  },
  {
    "objectID": "posts/hackathon_report/index.html",
    "href": "posts/hackathon_report/index.html",
    "title": "A micro AI tool",
    "section": "",
    "text": "On the weekends of 25-26/02 I had the pleasure(?) of attending the Intuition hackathon hosted by the NTU branch of IEEE with Phan Nhat Hoang a.k.a John Phan. We did not win any prize this time (yes, there was a last time that we won, which deserved a post of it all, but not today). Consider this post the debrief for the two days.\nFirst, here is the link to the gallery of the hackathon. Take some time to browse through it and you will notice that at least half of them mentioned GPT-3. Our project, SumMed, did, too. And we were not alone. After OpenAI released the APIs for their GPT 3.5 (davinci) and DALL·E 2 model, there swiftly spawned a generation of pico AI start-ups that made use of the platform to build products that bring in good income. This was mentioned in Data Machina’s Newsletter 190, together with a bag of tools termed “Modern AI Stack” by Carlos.\nIt was amazing how quickly people in tech caught on to something interesting. Or perhaps it was the ability to turn almost everything into interesting stuff. Anyway, I want to mention the newsletter first because it was our first mistake. We were not up with the news. Had only we known more about the trend in the field, we could have utilized more tools to save the work. As we were about to see, the biggest regret would be the front-end, which Hoang spent most of his time to write with React.js, while another team accomplished nearly the same thing and some more with Streamlit. And it was also worth mentioning that neither of us know how to use Streamlit - Hoang fell into React.js out of habit. And we just straight up focused on OpenAI technology instead of considering others, with two worth mentioning being HuggingFace and Colossal-AI. There was no time, and we were not knowledgeable enough to utilize the tools.\nBefore moving on, it is worth mentioning that the “mistake” I wrote above needs reading as “mistake in the context of a hackathon”. When you are in such a rush (<24 hours) and you are not a master learner who can acquire tools and integrate in the project at will (yet), you will need to prepare everything way before the event. I did not do that, because these skills were not the highest in my long-term priority yet (guess so for Hoang). A hackathon seemed big and important on the resume (especially when you are deep into it and do not have any sleep for the past 24 hours), but the long-term vision is always more important and always comes first.\nNow that is enough rambling. Onto the actual stuff."
  },
  {
    "objectID": "posts/hackathon_report/index.html#doclayout.py",
    "href": "posts/hackathon_report/index.html#doclayout.py",
    "title": "A micro AI tool",
    "section": "DocLayout.py",
    "text": "DocLayout.py\nFirst, the whole file:\nimport pdf2image\nimport numpy as np\nimport layoutparser as lp\nfrom collections import defaultdict\n\nclass DocLayout(object):\n    def __init__(self) -> None:\n        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n                                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n                                    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n        self.ocr_agent = lp.TesseractAgent(languages='eng')\n\n    def extract_pdf(self, file_name: str):\n        \"\"\" From a local file pdf file, extract the title, text, tables and figures\n        Args:\n            file_name (str): path to the pdf file\n        Returns:\n            title (str): title of the paper\n            Paper (str): text of the paper\n            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array\n            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array\n        \"\"\"\n        list_of_pages = pdf2image.convert_from_path(file_name)\n        images = [np.asarray(page) for page in list_of_pages]\n        image_width = len(images[0][0])\n\n        header_blocks, text_blocks, table_blocks, figure_blocks = self._detect_element(images)\n\n        title = self._extract_title(image_width, images, header_blocks)\n        Paper = self._extract_text_info(image_width, images, text_blocks)\n        table_by_page, figure_by_page = self._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)\n        # Currently we dont care about the order of the figures or tables returned\n        tables = self._general_by_table_to_list(table_by_page)\n        figures = self._general_by_table_to_list(figure_by_page)\n        return title, Paper, tables, figures\n    \n    def _general_by_table_to_list(self, general_by_page: dict):\n        return [general for i in general_by_page.keys() for general in general_by_page[i]]\n    \n    def _detect_element(self, images):\n        types = ['Title', 'Text', 'Table', 'Figure']\n        type_blocks = {\n            t: defaultdict(list) for t in types\n        }\n        for i in range(len(images)):\n            layout_result = self.model.detect(images[i])\n            for t in types:\n                type_block = lp.Layout([b for b in layout_result if b.type==t])\n                if len(type_block) != 0:\n                    type_blocks[t][i] = type_block\n        return type_blocks.values()\n    \n    \n    def _extract_title(self, image_width, images, header_blocks):\n        \"\"\"\n        Extract the title of the article from several headers\n        \"\"\"\n        first_page = min(header_blocks.keys())\n        segment_title = self._extract_page(first_page, image_width, images, header_blocks)[0]\n        title = self.ocr_agent.detect(segment_title)\n        return title\n    \n    def _extract_text_info(self, image_width, images, text_blocks):\n        \"\"\"\n        Returns all the text in the article\n        \"\"\"\n        Paper = \"\"\n        for page_id in text_blocks:\n            text_block_images = self._extract_page(page_id, image_width, images, text_blocks)\n            for block in text_block_images:\n                text = self.ocr_agent.detect(block).strip()\n                Paper += text + \" \"\n        return Paper\n\n    def _extract_table_n_figure(self, image_width, images, table_blocks, figure_blocks):\n        \"\"\"Extract 3D numpy array of tables and figures from deteced layout\n        Args:\n            image_width (int): width of image\n            images (_type_): _description_\n            table_blocks (_type_): _description_\n            figure_blocks (_type_): _description_\n        Returns:\n            table_by_page, figure_by_page (dict(list)): 3D numpy array of tables and figures by page\n        \"\"\"\n        \n        table_by_page, figure_by_page = defaultdict(list), defaultdict(list)\n        for page_id in table_blocks:\n            results = self._extract_page(page_id, image_width, images, table_blocks )\n            table_by_page[page_id] = results\n        \n        for page_id in figure_blocks:\n            results = self._extract_page(page_id, image_width, images, figure_blocks)\n            figure_by_page[page_id] = results\n        \n        return table_by_page, figure_by_page\n\n    def _extract_page(self, page_id, image_width, images, general_blocks):\n        \"\"\" \n        Get a list of 3D array numpy image of tables and figures, or text from a page\n        \"\"\"\n        results = []\n        left_interval = lp.Interval(0, image_width/2, axis='x').put_on_canvas(images[page_id])\n        left_blocks = general_blocks[page_id].filter_by(left_interval, center=True)._blocks\n        left_blocks.sort(key = lambda b: b.coordinates[1])\n\n        # Sort element ID of the right column based on y1 coordinate\n        right_blocks = [b for b in general_blocks[page_id] if b not in left_blocks]\n        right_blocks.sort(key = lambda b: b.coordinates[1])\n\n        # Sort the overall element ID starts from left column\n        general_block = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n\n        # Crop image around the detected layout\n        for block in general_block:\n            segment_image = (block\n                                .pad(left=15, right=15, top=5, bottom=5)\n                                .crop_image(images[page_id]))\n            results.append(segment_image)\n\n        return results\nLet’s dissect the codes.\nimport pdf2image\nimport numpy as np\nimport layoutparser as lp\nfrom collections import defaultdict\n\nclass DocLayout(object):\n    def __init__(self) -> None:\n        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n                                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n                                    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n        self.ocr_agent = lp.TesseractAgent(languages='eng')\nThe star of the show is LayoutParser module, which employs a host of models from the Detectron2 platform for the task of document layout parsing. We used the best configuration suggested by the docs of Mark RCNN trained on the PubLayNet dataset of document layout analysis. As you can see, the model in this case can detect 5 elements in the dictionary {0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}. pdf2image and an ocr_agent needs importing and creating respectively because the model works on images, so we need to convert the PDF file to image(s) i.e. NumPy array(s) before doing anything.\nclass DocLayout(object):\n    def extract_pdf(self, file_name: str):\n        \"\"\" From a local file pdf file, extract the title, text, tables and figures\n        Args:\n            file_name (str): path to the pdf file\n        Returns:\n            title (str): title of the paper\n            Paper (str): text of the paper\n            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array\n            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array\n        \"\"\"\n        list_of_pages = pdf2image.convert_from_path(file_name)\n        images = [np.asarray(page) for page in list_of_pages]\n        image_width = len(images[0][0])\n\n        header_blocks, text_blocks, table_blocks, figure_blocks = self._detect_element(images)\n\n        title = self._extract_title(image_width, images, header_blocks)\n        Paper = self._extract_text_info(image_width, images, text_blocks)\n        table_by_page, figure_by_page = self._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)\n        # Currently we dont care about the order of the figures or tables returned\n        tables = self._general_by_table_to_list(table_by_page)\n        figures = self._general_by_table_to_list(figure_by_page)\n        return title, Paper, tables, figures\n\n    def _detect_element(self, images):\n        types = ['Title', 'Text', 'Table', 'Figure']\n        type_blocks = {\n            t: defaultdict(list) for t in types\n        }\n        for i in range(len(images)):\n            layout_result = self.model.detect(images[i])\n            for t in types:\n                type_block = lp.Layout([b for b in layout_result if b.type==t])\n                if len(type_block) != 0:\n                    type_blocks[t][i] = type_block\n        return type_blocks.values()\npdf2image.convert_from_path() returns a list of Pillow image, which needs converting to a list of NumPy arrays before work. Afterwards, in _detect_element() method, call sel.model.detect() to return a list of bounding boxes (represent by the top-left and right-bottom coordinates with respect to the particular page) with element type. The list of blocks returned will be processed accordingly."
  },
  {
    "objectID": "posts/hackathon_report/index.html#docreader.py",
    "href": "posts/hackathon_report/index.html#docreader.py",
    "title": "A micro AI tool",
    "section": "DocReader.py",
    "text": "DocReader.py\n# There are minor differences (by the time of post) from the file in the repo\nfrom llama_index  import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\nfrom langchain import OpenAI\n\nclass DocReader(object):\n    def __init__(self, directory_path, index_path):\n        self.index_path = index_path\n        self.directory_path = directory_path\n        self.max_input_size = 4096\n        self.num_outputs = 256\n        self.max_chunk_overlap = 20\n        self.chunk_size_limit = 600\n        self.llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.75, model_name=\"text-davinci-003\", max_tokens=self.num_outputs))\n        self.prompt_helper = PromptHelper(self.max_input_size, self.num_outputs, self.max_chunk_overlap, chunk_size_limit=self.chunk_size_limit)\n\n    def construct_index(self):\n        \"\"\"\n        Reconstruct the index, and save it to the database\n        \"\"\"\n        documents = SimpleDirectoryReader(self.directory_path).load_data()        \n        index = GPTSimpleVectorIndex(\n            documents, llm_predictor=self.llm_predictor, prompt_helper=self.prompt_helper\n        )\n        index.save_to_disk(self.index_path + '/index.json')\n\n    def predict(self, query):\n        index = GPTSimpleVectorIndex.load_from_disk(self.index_path + '/index.json')\n        response = index.query(query, response_mode=\"default\")\n        return response.response\nA LlamaIndex workflow consists of 4 steps:\n\nInitialize an LLMPredictor() instance (based on LangChain LLM and LLMChain, which supports many other model hubs besides OpenAI). LLMPredictor() is a wrapper outside the model we use.\nInitialize a PromptHelper instance that helps to define various parameters for the prompt.\nIndex the document. There are many ways to achieve this, but the most simple way is calling SimpleDirectoryReader() to get the documents and GPTSimpleVectorIndex() to get the index that can be saved as a .json file.\nQuery over the index. There are different, pre-defined response mode in LlamaIndex. Explore the docs for more.\n\nAnd that’s it! Short and simple, yes powerful."
  },
  {
    "objectID": "posts/hackathon_report/index.html#docsummarizer.py",
    "href": "posts/hackathon_report/index.html#docsummarizer.py",
    "title": "A micro AI tool",
    "section": "DocSummarizer.py",
    "text": "DocSummarizer.py\nimport numpy as np\nfrom PIL import Image\nimport os\nimport json\nfrom DocLayout import DocLayout\nfrom collections import defaultdict\n\nclass DocSummarizer(object):\n    def __init__(self, documents_path: str, resources_path: str):\n        self.documents_path = documents_path\n        self.resources_path = resources_path\n        self.prompt_tail = {\n            'authors': 'Who are the authors of this paper',\n            'summary':\"\\n\\nSummarize the above text, focus on key insights\",\n            'keyresults':'''\\n\\nGive me three key results in the format of \"Key results:\n                1.  Key result 1\n                2. Key result 2\n                3. Key result 3\"''',\n            'keyword':'\\n\\nGive me keywords in the format of \"Keywords:  Keyword 1, Keyword 2, Keyword 3\"',\n            'limitations':'\\n\\nGive me 3 sentences describing the limitations of the text above.'\n        }\n        self.layout_model = DocLayout()\n        \n    def get_summary(self, file_name: str, reader):\n        \"\"\"\n        Returns a summary of the document, this document is a pdf file that has been uploaded to the server.\n        And save the summary to the database/resources.\n        \"\"\"\n        title, Paper, tables, figures = self.layout_model.extract_pdf(self.documents_path + '/' + file_name)\n        authors, summary, keywords, keyresults, limitations = self._read(Paper, reader)\n        response = {\n          'title': title,\n          'authors': authors,\n          'summary': summary,\n          'key_concepts': keywords,\n          'highlights': keyresults,\n          'limitations': limitations,\n          'figures': [],\n          'tables': [],\n        }\n        \n        if not os.path.exists(self.resources_path + '/' + file_name[:-4]):\n            os.mkdir(self.resources_path + '/' + file_name[:-4])\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/info.json', 'w') as f:\n            json.dump(response, f)\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/title.txt', 'w') as f:\n            f.write(title)\n        \n        for idx, table in enumerate(tables):\n            im = Image.fromarray(table)\n            local_fn = file_name[:-4] + '*' + str(idx) + '_table.png'\n            table_fn = self.resources_path + '/' + file_name[:-4] + '/' + str(idx) + '_table.png'\n            im.save(table_fn)\n            response['tables'].append(local_fn)\n        \n        for idx, fig in enumerate(figures):\n            im = Image.fromarray(fig)\n            local_fn = file_name[:-4] + '*' + str(idx) + '_fig.png'\n            fig_fn = self.resources_path + '/' + file_name[:-4] + '/' + str(idx) + '_fig.png'\n            im.save(fig_fn)\n            response['figures'].append(local_fn)\n        \n        return response\n\n    def retrieve_summary(self, file_name: str):\n        \"\"\"\n        Returns a summary of the document (retrieve from resources), this document is a pdf file that already in the server.\n        \"\"\"\n        if not os.path.exists(self.resources_path + '/' + file_name[:-4]):\n            raise Exception('File not found')\n        \n        response = {\n          'title': None,\n          'authors': None,\n          'summary': None,\n          'key_concepts': None,\n          'highlights': None,\n          'limitations': None,\n          'figures': [],\n          'tables': [],\n        }\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/title.txt', 'r') as f:\n            response['title'] = f.read()\n        \n        response_js = json.load(open(self.resources_path + '/' + file_name[:-4] + '/info.json'))\n        response['authors'] = response_js['authors']\n        response['summary'] = response_js['summary']\n        response['key_concepts'] = response_js['key_concepts']\n        response['highlights'] = response_js['highlights']\n        response['limitations'] = response_js['limitations']\n          \n        for fn in os.listdir(self.resources_path + '/' + file_name[:-4]):\n            fn = file_name[:-4] + '*' + fn\n            if 'fig' in fn:\n                response['figures'].append(fn)\n            else:\n                response['tables'].append(fn)\n        return response\n    \n    def _read(self, Paper, reader):\n        \"\"\"\n        Read the text and returns the authors, summary, keywords, keyresults and limitations\n        \"\"\"\n        # TODO: Currently we use the Doc Reader service to read the text, but we need to implement our own service\n        response = defaultdict(str)\n        for query_type, prompt in self.prompt_tail.items():\n            ans_query = reader.predict(prompt + \"\".join(Paper[:500].split(\" \")[:20]))\n            response[query_type] = ans_query\n        \n        return response['authors'], response['summary'], response['keywords'], response['keyresults'], response['limitations']\nThe DocSummarizer class continues where the DocLayout leaves. The text information retrieved from a document will be concatenated with a suitable prompt tail to send to OpenAI. Notice that each prompt tail is provided with a format for the model to follow (and it did follow!) in the response. For the graphics, they are converted from NumPy arrays to .PNG files in a folders that are accessible from the UI."
  },
  {
    "objectID": "posts/hackathon_report/index.html#app.py",
    "href": "posts/hackathon_report/index.html#app.py",
    "title": "A micro AI tool",
    "section": "app.py",
    "text": "app.py\nAdd some magic from Flask"
  },
  {
    "objectID": "posts/hackathon_report/index.html#client",
    "href": "posts/hackathon_report/index.html#client",
    "title": "A micro AI tool",
    "section": "client",
    "text": "client\nAdd some magic from React.js"
  },
  {
    "objectID": "posts/hackathon_report/index.html#result",
    "href": "posts/hackathon_report/index.html#result",
    "title": "A micro AI tool",
    "section": "Result",
    "text": "Result\nHere is the final demo capture of SumMed\n\n\n\nLeft is the navigation bar displaying the papers. Center is the key information (text and graphics) + Slide maker that is not yet implemented. Right is the chatbox with information on the paper"
  },
  {
    "objectID": "posts/hackathon_report/index.html#whats-next",
    "href": "posts/hackathon_report/index.html#whats-next",
    "title": "A micro AI tool",
    "section": "What’s next",
    "text": "What’s next\nThere are many things to improve on. I will sample three of them.\n\nRefactor code for LlamaIndex and try options out of OpenAI. We are currently using all the “simple” stuff from LlamaIndex. It works, but not optimal. There are better ways (FAISS) to perform vector search between question and document, and better index data structure (tree for summary) for each task.\nFine-tune the document layout parsing model. Microsoft offers its LayoutLM in HuggingFace Hub, which can be fine-tuned using the 🤗 Transformers module. There are mentions online about the effectiveness of the fine-tuned model.\nEnd-to-end pipeline going straight from PDF to Slides/Infographic. The real big thing. Right now, we settle at the users manually used the extracted information to create slides, but the ideal case is automatically doing so for the users. The other two are technical optimization, this one is putting things all together and finish the job."
  },
  {
    "objectID": "posts/gan-p2/index.html",
    "href": "posts/gan-p2/index.html",
    "title": "Building a simple GAN",
    "section": "",
    "text": "In the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, KMNIST."
  },
  {
    "objectID": "posts/gan-p2/index.html#the-dataset",
    "href": "posts/gan-p2/index.html#the-dataset",
    "title": "Building a simple GAN",
    "section": "The dataset:",
    "text": "The dataset:\nFirst rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.\n\n\n\nThe 10 classes of Kuzushiji-MNIST, with the first column showing each character’s modern hiragana counterpart. Source\n\n\nSimilar to MNIST, a KMNIST image has only one channel. Let’s visualize one.\n\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n\nC:\\Users\\Admin\\anaconda3\\envs\\cb0494\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning:\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n\n\n\n# Function learnt from GAN's Specialization Course 1 Week 1\ndef tensor_show(image_tensor, num_images=25, size=(1, 28, 28)):\n    # The original image tensor could be stored on GPU and \n    # have been flattened out for training, so we restore it\n    # first.\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    # torch uses (color channel, height, width) while \n    # matplotlib used (height, width, color channel)\n    # so we fix it here\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n\n\n# Download needs to be set to True the first time you run it.\nbatch_size = 32\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=True, transform=transforms.ToTensor()),\n    batch_size=batch_size,\n    shuffle=True)\n\n\nimage_tensor = next(iter(dataloader))[0]\ntensor_show(image_tensor)"
  },
  {
    "objectID": "posts/gan-p2/index.html#the-discriminator",
    "href": "posts/gan-p2/index.html#the-discriminator",
    "title": "Building a simple GAN",
    "section": "The Discriminator:",
    "text": "The Discriminator:\n\nThe architecture for each block of Discriminator and Generator follows the suggestions from the Deep Convolutional GAN paper.\n\nThe Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol’ linear model, but I will skip a bit to the year 2015, when DCGAN was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A non-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN’s training. We will touch upon tricks to stabilize GAN’s training in the next post).\n\nclass Discriminator(nn.Module):\n    def __init__(self, image_channel=1, hidden_dim=56):\n        super().__init__()\n        self.disc = nn.Sequential(\n            self.make_disc_block(image_channel, hidden_dim),\n            self.make_disc_block(hidden_dim, hidden_dim),\n            self.make_disc_block(hidden_dim, 1, final_layer=True),\n        )\n\n    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                    nn.BatchNorm2d(output_channels),\n                    nn.LeakyReLU(negative_slope=0.25)\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n            )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.disc(x)\n        # The input can be a tensor of multiple images\n        # We want to return a tensor with the possibility\n        # of real/fake for each image.\n        return x.view(len(x), -1)"
  },
  {
    "objectID": "posts/gan-p2/index.html#the-generator",
    "href": "posts/gan-p2/index.html#the-generator",
    "title": "Building a simple GAN",
    "section": "The Generator:",
    "text": "The Generator:\nA point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially distilling the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the transposed convolution operation, which increases the dimensions of data, essentially magnifying the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this notebook as my draft.)\nFirst, we need a function to generate noise. Basically, we need some tensor containing random numbers, and we can conveniently return a tensor filled with random numbers from a normal distribution with torch.randn(). For the dimensions, we define argument z_dim as the dimension of the noise input, and n_samples as the number of samples we need.\n\ndef generate_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn((n_samples, z_dim), device=device)\n\nFor the Generator class, I will also create a function to construct each layer. A non-last layer will have a transposed convolution, followed by batch normalization and ReLU activation. The final layer does not have batch normalization but will have Tanh activation to squish the pixels in range.\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim=14, image_channel=1, hidden_dim=56):\n        # z_dim is the dimension of the input noise vector\n        self.z_dim = z_dim\n        super().__init__()\n        self.gen = nn.Sequential(\n            self.make_gen_block(z_dim, hidden_dim * 4),\n            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n            self.make_gen_block(hidden_dim * 2, hidden_dim),\n            self.make_gen_block(hidden_dim, image_channel, kernel_size=4, final_layer=True),\n        )\n    \n    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.ReLU()\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh()\n            )\n    # Recall torch expect an image to be in the form (color channel, height, width).\n    # In a batch, torch expects it to be (no. of images in batch, color channel, height, width)\n    # So we need to transform the noise, originally in (no. of images in batch, input dimension)\n    # to (no. of images in batch, input dimension, 1, 1)\n    # See more here:\n    # https://pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images\n    def unsqueeze_noise(self, noise):\n        return noise.view(len(noise), self.z_dim, 1, 1)\n\n    def forward(self, noise):\n        x = self.unsqueeze_noise(noise)\n        return self.gen(x)"
  },
  {
    "objectID": "posts/gan-p2/index.html#optimizers-and-criterion",
    "href": "posts/gan-p2/index.html#optimizers-and-criterion",
    "title": "Building a simple GAN",
    "section": "Optimizers and Criterion",
    "text": "Optimizers and Criterion\nNext, we want to define our optimizers (one for each model) and our criterion.\n\n# We do not have activation at the output for Discriminator, so the outputs\n# are raw (logits).\ncriterion = nn.BCEWithLogitsLoss()\nz_dim = 64\ndisplay_step = 500\nbatch_size = 1000\n# Learning rate of 0.0002 and beta_1 (momentum term for Adam optimizer) of \n# 0.5 works well for DCGAN, according to the paper (yes, I seriously searched\n# for keyword \"learning rate\" in the paper)\nlr = 0.0002\nbeta_1 = 0.5 \nbeta_2 = 0.999\n# Device-agnostic code\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# You can tranform the image values to be between -1 and 1 (the range of the Tanh activation)\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n])\n\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=False, transform=transform),\n    batch_size=batch_size,\n    shuffle=True)\n\n\ngen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\ndisc = Discriminator().to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n\n# You initialize the weights to the normal distribution\n# with mean 0 and standard deviation 0.02\n# (Yes, the paper said so.)\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\n# Apply recursively weights_init() according to the docs:\n# https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init)"
  },
  {
    "objectID": "posts/gan-p2/index.html#training",
    "href": "posts/gan-p2/index.html#training",
    "title": "Building a simple GAN",
    "section": "Training",
    "text": "Training\nOkay, now onto training!\nn_epochs = 100\ncur_step = 1 # For visualization purpose\nmean_generator_loss = 0\nmean_discriminator_loss = 0\nfor epoch in range(n_epochs):\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n        real = real.to(device)\n\n        ## Update Discriminator\n\n        # Empty the optimizer\n        disc_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise = generate_noise(cur_batch_size, z_dim, device=device)\n        fake = gen(fake_noise)\n        disc_fake_pred = disc(fake.detach())\n\n        # Calculate loss\n        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n\n        # Same for real examples\n        disc_real_pred = disc(real)\n        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n\n        # The Discriminator's loss is the average of the two\n        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n\n        # Keep track of the average Discriminator loss\n        mean_discriminator_loss += disc_loss.item() / display_step\n\n        # Update Discriminator's gradients a.k.a backpropagation\n        # Normally don't set retain_graph=True, but we do so for GAN\n        # as we need to propagate through the graph a second time\n        # when updating the Generator.\n        disc_loss.backward(retain_graph=True)\n\n        # Update Discriminator's optimizer\n        disc_opt.step()\n\n        ## Update Generator\n\n        # Empty the optimizer\n        gen_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise_2 = generate_noise(cur_batch_size, z_dim, device=device)\n        fake_2 = gen(fake_noise_2)\n        disc_fake_pred = disc(fake_2)\n\n        # Calculate loss\n        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n\n        # Backpropagation for Generator's loss\n        gen_loss.backward()\n\n        # Update Generator's optimizer\n        gen_opt.step()\n\n        # Keep track of the average Generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ## Visualization code\n        if cur_step % display_step == 0 and cur_step &gt; 0:\n            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n            tensor_show(fake)\n            tensor_show(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1"
  },
  {
    "objectID": "posts/hackathon-report/index.html",
    "href": "posts/hackathon-report/index.html",
    "title": "A micro AI tool",
    "section": "",
    "text": "On the weekends of 25-26/02 I had the pleasure(?) of attending the Intuition hackathon hosted by the NTU branch of IEEE with Phan Nhat Hoang a.k.a John Phan. We did not win any prize this time (yes, there was a last time that we won, which deserved a post of it all, but not today). Consider this post the debrief for the two days.\nFirst, here is the link to the gallery of the hackathon. Take some time to browse through it and you will notice that at least half of them mentioned GPT-3. Our project, SumMed, did, too. And we were not alone. After OpenAI released the APIs for their GPT 3.5 (davinci) and DALL·E 2 model, there swiftly spawned a generation of pico AI start-ups that made use of the platform to build products that bring in good income. This was mentioned in Data Machina’s Newsletter 190, together with a bag of tools termed “Modern AI Stack” by Carlos.\nIt was amazing how quickly people in tech caught on to something interesting. Or perhaps it was the ability to turn almost everything into interesting stuff. Anyway, I want to mention the newsletter first because it was our first mistake. We were not up with the news. Had only we known more about the trend in the field, we could have utilized more tools to save the work. As we were about to see, the biggest regret would be the front-end, which Hoang spent most of his time to write with React.js, while another team accomplished nearly the same thing and some more with Streamlit. And it was also worth mentioning that neither of us know how to use Streamlit - Hoang fell into React.js out of habit. And we just straight up focused on OpenAI technology instead of considering others, with two worth mentioning being HuggingFace and Colossal-AI. There was no time, and we were not knowledgeable enough to utilize the tools.\nBefore moving on, it is worth mentioning that the “mistake” I wrote above needs reading as “mistake in the context of a hackathon”. When you are in such a rush (&lt;24 hours) and you are not a master learner who can acquire tools and integrate in the project at will (yet), you will need to prepare everything way before the event. I did not do that, because these skills were not the highest in my long-term priority yet (guess so for Hoang). A hackathon seemed big and important on the resume (especially when you are deep into it and do not have any sleep for the past 24 hours), but the long-term vision is always more important and always comes first.\nNow that is enough rambling. Onto the actual stuff."
  },
  {
    "objectID": "posts/hackathon-report/index.html#doclayout.py",
    "href": "posts/hackathon-report/index.html#doclayout.py",
    "title": "A micro AI tool",
    "section": "DocLayout.py",
    "text": "DocLayout.py\nFirst, the whole file:\nimport pdf2image\nimport numpy as np\nimport layoutparser as lp\nfrom collections import defaultdict\n\nclass DocLayout(object):\n    def __init__(self) -&gt; None:\n        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n                                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n                                    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n        self.ocr_agent = lp.TesseractAgent(languages='eng')\n\n    def extract_pdf(self, file_name: str):\n        \"\"\" From a local file pdf file, extract the title, text, tables and figures\n        Args:\n            file_name (str): path to the pdf file\n        Returns:\n            title (str): title of the paper\n            Paper (str): text of the paper\n            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array\n            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array\n        \"\"\"\n        list_of_pages = pdf2image.convert_from_path(file_name)\n        images = [np.asarray(page) for page in list_of_pages]\n        image_width = len(images[0][0])\n\n        header_blocks, text_blocks, table_blocks, figure_blocks = self._detect_element(images)\n\n        title = self._extract_title(image_width, images, header_blocks)\n        Paper = self._extract_text_info(image_width, images, text_blocks)\n        table_by_page, figure_by_page = self._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)\n        # Currently we dont care about the order of the figures or tables returned\n        tables = self._general_by_table_to_list(table_by_page)\n        figures = self._general_by_table_to_list(figure_by_page)\n        return title, Paper, tables, figures\n    \n    def _general_by_table_to_list(self, general_by_page: dict):\n        return [general for i in general_by_page.keys() for general in general_by_page[i]]\n    \n    def _detect_element(self, images):\n        types = ['Title', 'Text', 'Table', 'Figure']\n        type_blocks = {\n            t: defaultdict(list) for t in types\n        }\n        for i in range(len(images)):\n            layout_result = self.model.detect(images[i])\n            for t in types:\n                type_block = lp.Layout([b for b in layout_result if b.type==t])\n                if len(type_block) != 0:\n                    type_blocks[t][i] = type_block\n        return type_blocks.values()\n    \n    \n    def _extract_title(self, image_width, images, header_blocks):\n        \"\"\"\n        Extract the title of the article from several headers\n        \"\"\"\n        first_page = min(header_blocks.keys())\n        segment_title = self._extract_page(first_page, image_width, images, header_blocks)[0]\n        title = self.ocr_agent.detect(segment_title)\n        return title\n    \n    def _extract_text_info(self, image_width, images, text_blocks):\n        \"\"\"\n        Returns all the text in the article\n        \"\"\"\n        Paper = \"\"\n        for page_id in text_blocks:\n            text_block_images = self._extract_page(page_id, image_width, images, text_blocks)\n            for block in text_block_images:\n                text = self.ocr_agent.detect(block).strip()\n                Paper += text + \" \"\n        return Paper\n\n    def _extract_table_n_figure(self, image_width, images, table_blocks, figure_blocks):\n        \"\"\"Extract 3D numpy array of tables and figures from deteced layout\n        Args:\n            image_width (int): width of image\n            images (_type_): _description_\n            table_blocks (_type_): _description_\n            figure_blocks (_type_): _description_\n        Returns:\n            table_by_page, figure_by_page (dict(list)): 3D numpy array of tables and figures by page\n        \"\"\"\n        \n        table_by_page, figure_by_page = defaultdict(list), defaultdict(list)\n        for page_id in table_blocks:\n            results = self._extract_page(page_id, image_width, images, table_blocks )\n            table_by_page[page_id] = results\n        \n        for page_id in figure_blocks:\n            results = self._extract_page(page_id, image_width, images, figure_blocks)\n            figure_by_page[page_id] = results\n        \n        return table_by_page, figure_by_page\n\n    def _extract_page(self, page_id, image_width, images, general_blocks):\n        \"\"\" \n        Get a list of 3D array numpy image of tables and figures, or text from a page\n        \"\"\"\n        results = []\n        left_interval = lp.Interval(0, image_width/2, axis='x').put_on_canvas(images[page_id])\n        left_blocks = general_blocks[page_id].filter_by(left_interval, center=True)._blocks\n        left_blocks.sort(key = lambda b: b.coordinates[1])\n\n        # Sort element ID of the right column based on y1 coordinate\n        right_blocks = [b for b in general_blocks[page_id] if b not in left_blocks]\n        right_blocks.sort(key = lambda b: b.coordinates[1])\n\n        # Sort the overall element ID starts from left column\n        general_block = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n\n        # Crop image around the detected layout\n        for block in general_block:\n            segment_image = (block\n                                .pad(left=15, right=15, top=5, bottom=5)\n                                .crop_image(images[page_id]))\n            results.append(segment_image)\n\n        return results\nLet’s dissect the codes.\nimport pdf2image\nimport numpy as np\nimport layoutparser as lp\nfrom collections import defaultdict\n\nclass DocLayout(object):\n    def __init__(self) -&gt; None:\n        self.model = lp.Detectron2LayoutModel('lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config',\n                                    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5],\n                                    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\n        self.ocr_agent = lp.TesseractAgent(languages='eng')\nThe star of the show is LayoutParser module, which employs a host of models from the Detectron2 platform for the task of document layout parsing. We used the best configuration suggested by the docs of Mark RCNN trained on the PubLayNet dataset of document layout analysis. As you can see, the model in this case can detect 5 elements in the dictionary {0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}. pdf2image and an ocr_agent needs importing and creating respectively because the model works on images, so we need to convert the PDF file to image(s) i.e. NumPy array(s) before doing anything.\nclass DocLayout(object):\n    def extract_pdf(self, file_name: str):\n        \"\"\" From a local file pdf file, extract the title, text, tables and figures\n        Args:\n            file_name (str): path to the pdf file\n        Returns:\n            title (str): title of the paper\n            Paper (str): text of the paper\n            table_by_page (dict(list)): dictionary of tables by page, each page has a list of tables, represent by 3D numpy array\n            figure_by_page (dict(list)): dictionary of figures by page, each page has a list of figures, represent by 3D numpy array\n        \"\"\"\n        list_of_pages = pdf2image.convert_from_path(file_name)\n        images = [np.asarray(page) for page in list_of_pages]\n        image_width = len(images[0][0])\n\n        header_blocks, text_blocks, table_blocks, figure_blocks = self._detect_element(images)\n\n        title = self._extract_title(image_width, images, header_blocks)\n        Paper = self._extract_text_info(image_width, images, text_blocks)\n        table_by_page, figure_by_page = self._extract_table_n_figure(image_width, images, table_blocks, figure_blocks)\n        # Currently we dont care about the order of the figures or tables returned\n        tables = self._general_by_table_to_list(table_by_page)\n        figures = self._general_by_table_to_list(figure_by_page)\n        return title, Paper, tables, figures\n\n    def _detect_element(self, images):\n        types = ['Title', 'Text', 'Table', 'Figure']\n        type_blocks = {\n            t: defaultdict(list) for t in types\n        }\n        for i in range(len(images)):\n            layout_result = self.model.detect(images[i])\n            for t in types:\n                type_block = lp.Layout([b for b in layout_result if b.type==t])\n                if len(type_block) != 0:\n                    type_blocks[t][i] = type_block\n        return type_blocks.values()\npdf2image.convert_from_path() returns a list of Pillow image, which needs converting to a list of NumPy arrays before work. Afterwards, in _detect_element() method, call sel.model.detect() to return a list of bounding boxes (represent by the top-left and right-bottom coordinates with respect to the particular page) with element type. The list of blocks returned will be processed accordingly."
  },
  {
    "objectID": "posts/hackathon-report/index.html#docreader.py",
    "href": "posts/hackathon-report/index.html#docreader.py",
    "title": "A micro AI tool",
    "section": "DocReader.py",
    "text": "DocReader.py\n# There are minor differences (by the time of post) from the file in the repo\nfrom llama_index  import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\nfrom langchain import OpenAI\n\nclass DocReader(object):\n    def __init__(self, directory_path, index_path):\n        self.index_path = index_path\n        self.directory_path = directory_path\n        self.max_input_size = 4096\n        self.num_outputs = 256\n        self.max_chunk_overlap = 20\n        self.chunk_size_limit = 600\n        self.llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.75, model_name=\"text-davinci-003\", max_tokens=self.num_outputs))\n        self.prompt_helper = PromptHelper(self.max_input_size, self.num_outputs, self.max_chunk_overlap, chunk_size_limit=self.chunk_size_limit)\n\n    def construct_index(self):\n        \"\"\"\n        Reconstruct the index, and save it to the database\n        \"\"\"\n        documents = SimpleDirectoryReader(self.directory_path).load_data()        \n        index = GPTSimpleVectorIndex(\n            documents, llm_predictor=self.llm_predictor, prompt_helper=self.prompt_helper\n        )\n        index.save_to_disk(self.index_path + '/index.json')\n\n    def predict(self, query):\n        index = GPTSimpleVectorIndex.load_from_disk(self.index_path + '/index.json')\n        response = index.query(query, response_mode=\"default\")\n        return response.response\nA LlamaIndex workflow consists of 4 steps:\n\nInitialize an LLMPredictor() instance (based on LangChain LLM and LLMChain, which supports many other model hubs besides OpenAI). LLMPredictor() is a wrapper outside the model we use.\nInitialize a PromptHelper instance that helps to define various parameters for the prompt.\nIndex the document. There are many ways to achieve this, but the most simple way is calling SimpleDirectoryReader() to get the documents and GPTSimpleVectorIndex() to get the index that can be saved as a .json file.\nQuery over the index. There are different, pre-defined response mode in LlamaIndex. Explore the docs for more.\n\nAnd that’s it! Short and simple, yes powerful."
  },
  {
    "objectID": "posts/hackathon-report/index.html#docsummarizer.py",
    "href": "posts/hackathon-report/index.html#docsummarizer.py",
    "title": "A micro AI tool",
    "section": "DocSummarizer.py",
    "text": "DocSummarizer.py\nimport numpy as np\nfrom PIL import Image\nimport os\nimport json\nfrom DocLayout import DocLayout\nfrom collections import defaultdict\n\nclass DocSummarizer(object):\n    def __init__(self, documents_path: str, resources_path: str):\n        self.documents_path = documents_path\n        self.resources_path = resources_path\n        self.prompt_tail = {\n            'authors': 'Who are the authors of this paper',\n            'summary':\"\\n\\nSummarize the above text, focus on key insights\",\n            'keyresults':'''\\n\\nGive me three key results in the format of \"Key results:\n                1.  Key result 1\n                2. Key result 2\n                3. Key result 3\"''',\n            'keyword':'\\n\\nGive me keywords in the format of \"Keywords:  Keyword 1, Keyword 2, Keyword 3\"',\n            'limitations':'\\n\\nGive me 3 sentences describing the limitations of the text above.'\n        }\n        self.layout_model = DocLayout()\n        \n    def get_summary(self, file_name: str, reader):\n        \"\"\"\n        Returns a summary of the document, this document is a pdf file that has been uploaded to the server.\n        And save the summary to the database/resources.\n        \"\"\"\n        title, Paper, tables, figures = self.layout_model.extract_pdf(self.documents_path + '/' + file_name)\n        authors, summary, keywords, keyresults, limitations = self._read(Paper, reader)\n        response = {\n          'title': title,\n          'authors': authors,\n          'summary': summary,\n          'key_concepts': keywords,\n          'highlights': keyresults,\n          'limitations': limitations,\n          'figures': [],\n          'tables': [],\n        }\n        \n        if not os.path.exists(self.resources_path + '/' + file_name[:-4]):\n            os.mkdir(self.resources_path + '/' + file_name[:-4])\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/info.json', 'w') as f:\n            json.dump(response, f)\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/title.txt', 'w') as f:\n            f.write(title)\n        \n        for idx, table in enumerate(tables):\n            im = Image.fromarray(table)\n            local_fn = file_name[:-4] + '*' + str(idx) + '_table.png'\n            table_fn = self.resources_path + '/' + file_name[:-4] + '/' + str(idx) + '_table.png'\n            im.save(table_fn)\n            response['tables'].append(local_fn)\n        \n        for idx, fig in enumerate(figures):\n            im = Image.fromarray(fig)\n            local_fn = file_name[:-4] + '*' + str(idx) + '_fig.png'\n            fig_fn = self.resources_path + '/' + file_name[:-4] + '/' + str(idx) + '_fig.png'\n            im.save(fig_fn)\n            response['figures'].append(local_fn)\n        \n        return response\n\n    def retrieve_summary(self, file_name: str):\n        \"\"\"\n        Returns a summary of the document (retrieve from resources), this document is a pdf file that already in the server.\n        \"\"\"\n        if not os.path.exists(self.resources_path + '/' + file_name[:-4]):\n            raise Exception('File not found')\n        \n        response = {\n          'title': None,\n          'authors': None,\n          'summary': None,\n          'key_concepts': None,\n          'highlights': None,\n          'limitations': None,\n          'figures': [],\n          'tables': [],\n        }\n        \n        with open(self.resources_path + '/' + file_name[:-4] + '/title.txt', 'r') as f:\n            response['title'] = f.read()\n        \n        response_js = json.load(open(self.resources_path + '/' + file_name[:-4] + '/info.json'))\n        response['authors'] = response_js['authors']\n        response['summary'] = response_js['summary']\n        response['key_concepts'] = response_js['key_concepts']\n        response['highlights'] = response_js['highlights']\n        response['limitations'] = response_js['limitations']\n          \n        for fn in os.listdir(self.resources_path + '/' + file_name[:-4]):\n            fn = file_name[:-4] + '*' + fn\n            if 'fig' in fn:\n                response['figures'].append(fn)\n            else:\n                response['tables'].append(fn)\n        return response\n    \n    def _read(self, Paper, reader):\n        \"\"\"\n        Read the text and returns the authors, summary, keywords, keyresults and limitations\n        \"\"\"\n        # TODO: Currently we use the Doc Reader service to read the text, but we need to implement our own service\n        response = defaultdict(str)\n        for query_type, prompt in self.prompt_tail.items():\n            ans_query = reader.predict(prompt + \"\".join(Paper[:500].split(\" \")[:20]))\n            response[query_type] = ans_query\n        \n        return response['authors'], response['summary'], response['keywords'], response['keyresults'], response['limitations']\nThe DocSummarizer class continues where the DocLayout leaves. The text information retrieved from a document will be concatenated with a suitable prompt tail to send to OpenAI. Notice that each prompt tail is provided with a format for the model to follow (and it did follow!) in the response. For the graphics, they are converted from NumPy arrays to .PNG files in a folders that are accessible from the UI."
  },
  {
    "objectID": "posts/hackathon-report/index.html#app.py",
    "href": "posts/hackathon-report/index.html#app.py",
    "title": "A micro AI tool",
    "section": "app.py",
    "text": "app.py\nAdd some magic from Flask"
  },
  {
    "objectID": "posts/hackathon-report/index.html#client",
    "href": "posts/hackathon-report/index.html#client",
    "title": "A micro AI tool",
    "section": "client",
    "text": "client\nAdd some magic from React.js"
  },
  {
    "objectID": "posts/hackathon-report/index.html#result",
    "href": "posts/hackathon-report/index.html#result",
    "title": "A micro AI tool",
    "section": "Result",
    "text": "Result\nHere is the final demo capture of SumMed\n\n\n\nLeft is the navigation bar displaying the papers. Center is the key information (text and graphics) + Slide maker that is not yet implemented. Right is the chatbox with information on the paper"
  },
  {
    "objectID": "posts/hackathon-report/index.html#whats-next",
    "href": "posts/hackathon-report/index.html#whats-next",
    "title": "A micro AI tool",
    "section": "What’s next",
    "text": "What’s next\nThere are many things to improve on. I will sample three of them.\n\nRefactor code for LlamaIndex and try options out of OpenAI. We are currently using all the “simple” stuff from LlamaIndex. It works, but not optimal. There are better ways (FAISS) to perform vector search between question and document, and better index data structure (tree for summary) for each task.\nFine-tune the document layout parsing model. Microsoft offers its LayoutLM in HuggingFace Hub, which can be fine-tuned using the 🤗 Transformers module. There are mentions online about the effectiveness of the fine-tuned model.\nEnd-to-end pipeline going straight from PDF to Slides/Infographic. The real big thing. Right now, we settle at the users manually used the extracted information to create slides, but the ideal case is automatically doing so for the users. The other two are technical optimization, this one is putting things all together and finish the job."
  },
  {
    "objectID": "posts/fastai2-p1/index.html",
    "href": "posts/fastai2-p1/index.html",
    "title": "Deep Learning from the ground up - From tensor to multi-layer perceptron",
    "section": "",
    "text": "I have finished and fallen in love with fast.ai course 1. It has been very informative. It showed me the rope about PyTorch and two important building blocks of deep learning: Embedding and Convolution. I was excited to learn that there was a part 2. In this part, Jeremy will dive deeper into the design of a deep learning framework, and implement one from the scratch the way PyTorch was designed. Here were my (verbose) writtent version for it.\n\nRules\n\nPermitted at the beginning: Python and all standard libraries, matplotlib, Jupyter Notebook and nbdev.\nAfter deriving something, we can use the implemented version for that.\n\n\n\nGet the data\nThe first thing you need to do is getting the data and visualize it in some way. The data we use is the good ol’ MNIST, available from the Internet. Based on good practice, let’s assign the URL to a variable, and prepare a directory to store the data.\n\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\npathlib.Path is a handy object as you can do special operations with string to receive a new Path, such as the division above. It is more readable than just raw strings.\nTo get the data, let’s use urllib.request.urlretrieve\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\nThe data is compressed in the .pkl.gz format, which can be decompressed sequentially with gzip and pickle.\n\nwith gzip.open(path_gz, 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding='latin-1')\n\n\n\nVisualize the data\nGreat, we have decompressed the data, but what exactly are stored inside these variables?\n\nx_train, x_valid, x_test\n\n(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n\n\nOkay, they are numpy.ndarray. We are not allowed to use numpy yet, so we will need to convert it to list. Yep, very sorry about that. But first, uh, let’s cheat a bit by checking the shape of the arrays.\n\nx_train.shape, x_valid.shape, x_test.shape\n\n((50000, 784), (10000, 784), (10000, 784))\n\n\nOkay, it seems that the $28 $ images are flattened into 784-element arrays. Convert any of the xs to a list will yield a big list of lists. It is unnecessary, so let’s just take 1 data point.\n\nx = list(x_train[0])\nx[:10], len(x)\n\n([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 784)\n\n\nTo visualize x, we need to convert it into a list of 28 lists, each with 28 elements. Is there a way to do that in Python? I said “No”, but Jeremy showed that there are at least two ways\nFirstly, we can make use of the yield keyword, which is used to generate iterators in Python. We want to generate 28 iterators, each containing 28 elements, from x. In Python, it is as simple as\n\ndef chunks(x, size):\n    max_size = len(x)\n    for i in range(0, max_size, size): yield x[i:min(max_size, i+size)]\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(x, 28)));\n\n\n\n\nSecondly, we can use itertools.islice.\n\nfrom itertools import islice\n\nit = iter(x[:10])\nprint('First',  list(islice(it, 5)))\nprint('Second', list(islice(it, 5)))\nprint('Third',  list(islice(it, 5)))\n\nFirst [0.0, 0.0, 0.0, 0.0, 0.0]\nSecond [0.0, 0.0, 0.0, 0.0, 0.0]\nThird []\n\n\nSimply, islice(iterable, stop) will return a new iterator from the iterable (which can be another iterator), stop at stop. Paired with default start and step of 0 and 1 respectively, it means that islice(it, 5) will return an iterator containing the first 5 values of it. Now we realize that doing so will also exhaust these first 5 values of it, so the next call will call the next 5 values of it. Paired with a loop, it works exactly like chunks() defined above.\n\nit = iter(x)\nimg = list(iter(lambda: list(islice(it, 28)), []))\nplt.imshow(img);\n\n\n\n\nWork like a charm. A note: that empty list passed in after the lambda function is for the sentinel argument of iter(), instructing iter() to stop when it encounters the sentinel."
  }
]