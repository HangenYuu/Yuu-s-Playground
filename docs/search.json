[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Pham Nguyen Hung. I am a Deep Learning Practitioner. The stage I am at (student, master, etc.) is bound to change over the years, so you are encouraged to view the Now tab for my current position. I have a knack for numbers and complex concepts, and love learning them. Outside that, I have an interest in books of all types, and arts (music, drawing). The word that motivates me is “polymath”, and the figure is Nassim Nicholas Taleb."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "H’s notes",
    "section": "",
    "text": "My notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\n \n\n\n\n\nJan 22, 2023\n\n\nPham Nguyen Hung\n\n\n1 min\n\n\n\n\n\n\n\n\n\nMy notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\n \n\n\n\n\nJan 21, 2023\n\n\nPham Nguyen Hung\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnew\n\n\ncode\n\n\n \n\n\n\n\nOct 22, 2022\n\n\nPham Nguyen Hung\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "My major is chemical engineering, but I want to be a deep learning practitioner.\n\n\n\nI studied it on Coursera. Two courses were released at different times, so there was a mismatched in module used (TensorFlow vs PyTorch), but that means that I can learn from both worlds. I often encountered champions for PyTorch, such as Jeremy Howard, who praised the module over TensorFlow, but I feel like I need to experience both to judge.\n\n\n\nJust go to a blog post and see one of my entry.\n\n\n\nYou can see them at my Github repo. For the philosophy of project-based and just-in-time learning, check out this post or this post.\n\n\n\nIt is not an easy market that I come into, with the news about tech layoffs in Singapore this 2023. Still, I do what I can do and have to do. My deadline is 29/01/2023, when the career fair at my school starts."
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Code\nprint(\"Hello World!\")\n\n\nHello World!"
  },
  {
    "objectID": "posts/anw/index.html",
    "href": "posts/anw/index.html",
    "title": "Show me your portfolio",
    "section": "",
    "text": "Nassim Nicholas Taleb told me this when he talked about listening to pundits in The Black Swan (or was it Antifragile?). Derek Sivers shared the same idea about talking versus doing. The idea reminds me to pick my words carefully, when I give advice, and when I state my priority. It also reminds to tailor my daily schedule so that the priority is always at the top. (It is a nice idea, by the way, to speak and act on priority as singular rather than plural). The next time you say something, act on it to see if you really believe your words. And the next time you do something, stating clearly how you like doing that."
  },
  {
    "objectID": "posts/dlm/index.html",
    "href": "posts/dlm/index.html",
    "title": "Daily translation of Meditations to Vietnamese",
    "section": "",
    "text": "“Don’t trust in your reputation, money, or position, but in the strength that is yours—namely, your judgements about the things that you control and don’t control. For this alone is what makes us free and unfettered, that picks us up by the neck from the depths and lifts us eye to eye with the rich and powerful.” — EPICTETUS, DISCOURSES, 3.26.34–35\n\nĐừng tin vào danh vọng, tiền bạc, hay vị trí [của bản thân], mà hãy tin vào sức mạnh mà con nắm giữ—khả năng phân biệt giữa thứ mà con có thể kiểm soát và thứ mà con không thể. Bởi đây là thứ duy nhất giúp chúng ta được tự do và không bị ràng buộc, là thứ duy nhất kéo chúng ta thoát khỏi vực thẳm và đưa chúng ta ngang hàng với những kẻ giàu có và quyền lực.\nIt is something that is expected from Epictetus - the emphasis on the dichotomy of control. What surprises me is the last phrase. The ability to exercise the dichotomy of control and act accordingly gives us power and wealth. It is all based on the same principle: to have more, you can acquire more, or you can want less."
  },
  {
    "objectID": "posts/dlm/index.html#nov---2022-1",
    "href": "posts/dlm/index.html#nov---2022-1",
    "title": "Daily translation of Meditations to Vietnamese",
    "section": "08 - Nov - 2022",
    "text": "08 - Nov - 2022\n\n“Remember that you are an actor in a play, playing a character according to the will of the playwright—if a short play, then it’s short; if long, long. If he wishes you to play the beggar, play even that role well, just as you would if it were a cripple, a honcho, or an everyday person. For this is your duty, to perform well the character assigned you. That selection belongs to another.” — EPICTETUS, ENCHIRIDION, 17\n\nHãy nhớ rằng con là một diễn viên trong một vở kịch [mang tên cuộc đời], diễn một vai theo ý muốn của người viết kịch—nếu đó là một vở kịch ngắn, thì diễn ngắn; nếu nó dài, thì diễn dài. Nếu người viết kịch muốn con đóng vai một kẻ ăn mày, vậy thì hãy diễn vai đó thật tốt, cũng tương tự như vậy nếu con nhận được vai kẻ bị thọt, người đứng đầu, hay chỉ là một người bình thường. Bởi việc diễn thật tốt vai đã được giao chính là nghĩa vụ của con. Việc chọn vai nào [cho con] thuộc về những người khác.\nNow the last sentence raised my eyebrows. What? Let’s others choose for me? Ins’t that antithetical to “Essentialism”? That I need to prioritize my life instead of letting others prioritize it for me! But be calm - Epictetus did not talk about letting others choose. The others here referred to the higher being, chance, luck i.e. anything that we cannot control. It is probable that the course of our life is led to stranger lands because of some accidents. However, we are still in control: we can choose to get back on track after being thrown off-trackor not. That alone is important, that alone is enough."
  },
  {
    "objectID": "posts/ftf/index.html#data-analyst",
    "href": "posts/ftf/index.html#data-analyst",
    "title": "First thing (1)",
    "section": "Data Analyst:",
    "text": "Data Analyst:\n\nHere is a diagram from DataCamp that sums up the job descriptions well:\n\nFrom the job descriptions, you will need to know…\nBasic:\n\n…how to use a business intelligence platforms - Looker, Power BI, Tableau and the like. One quick look at DataCamp Data Analyst in Power BI track shows that Power BI is capable of step 2-4 in the diagram. You can prepare and transform data; you can explore data from basic (mean, median, etc.) to complex operations that you define yourself; you can do all kinds of visualizations; you can publish the findings to a report, an app (possibly dashboard), or slides to aid your presentation. And compared to frameworks in languages such as R or Python, these platforms look less daunting because they have a GUI (you still often need to “program stuff”). These are the main tools for your trade.\n…how to communicate (written and oral). At the end of anything at work, you will need to “give report to stakeholders”. Being a data analyst\n\nPreferred: These are specific TikTok’s preferences:\n\n…how to do statistical analysis. This is arguably “basic”, not “preferred”. The numbers you calculate in the platform, they all have statistical meanings, so it helps to know what you calculating. Understanding statistics also helps you to avoid"
  },
  {
    "objectID": "posts/ftf/index.html#data-engineer",
    "href": "posts/ftf/index.html#data-engineer",
    "title": "First thing (1)",
    "section": "Data Engineer:",
    "text": "Data Engineer:\nAs a LinkedIn post suggested, a data engineer’s job is “being in charge of the data pipeline”, making sure that people such as data scientists can have the data to do their jobs. I will admit that I know next to nothing about data engineer, so the following came from a certain “God-tier” roadmap that I picked up. (I am weak again anything “God-tier”, so please refrain from using that against me). I will guess that the job requirements will vary. The data you need to work on may just be stored in a single AWS S3, but there are cases where the data are distributed in 23 data centers around the world, and you need Vitess on top of MySQL to handle the requests successfully (totally not Youtube). In any case, a quick rundown of the “God-tier” roadmap reviews that you need:\n\nSQL (and NoSQL)\nDatabase system knowledge (data lake, data warehouse, etc.)\nLinux a.k.a working with command line interface (CLI)\nBig data framework (Hadoop, Spark, Kafka, depends on your company)\nCloud service knowledge (Google Cloud Product (GCP), Amazon Web Services (AWS), or Microsoft Azure - depends on your company)\nContainerization technology: Docker (and Kubernetes)\n\nYou may find the rest in the video and the attached document. Now onto the confusing topic…"
  },
  {
    "objectID": "posts/ftf/index.html#data-scientistmachine-learning-engineer",
    "href": "posts/ftf/index.html#data-scientistmachine-learning-engineer",
    "title": "First thing (1)",
    "section": "Data Scientist/Machine Learning Engineer:",
    "text": "Data Scientist/Machine Learning Engineer:\nFirst, let’s start with a diagram from DataCamp.\n\n\n\nSource: DataCamp\n\n\nThe things that confuse me"
  },
  {
    "objectID": "posts/ganlec/index.html",
    "href": "posts/ganlec/index.html",
    "title": "A Primer on Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "A brief history of GANs\nFor a fuller account, check out the MIT Technology Review article.\nBack in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from AlexNet to VGG. (Not to mention ResNet in 2015, an architecture with so interesting an idea that I had to make a project for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, Generative Adversarial Nets.\n\n\n\nThe image was not from the era, but was representative of what you got from the model at that time (and still now with GANs). Source\n\n\nNow I wanted to make two quick detours before going into the inside of GANs:\n\nAt its core sense, a function is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is trying to update its parameters such that the model will approximate the optimal function as closely as possible. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.\nAdvances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.\n\n\n\nThe GALN’s game:\nNote: I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.\nThe word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called Generator, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called Discriminator, (or Critic, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.\n\n\n\n\nGenerator\nDiscriminator\n\n\n\n\nInput\nRandom numbers\nImages (real & generated)\n\n\nOutput\nImages\nClass of image (binary)\n\n\nRole\nForger\nAppraiser\n\n\n\nOur loss function will be the good ol’ binary cross-entropy: \\[J(\\theta) = -\\frac{1}{m}*[y^{(i)}log(h(x^{(i)}, \\theta)) + (1 - y^{(i)})log(1 - (h(x^{(i)}, \\theta)))]\\]\nThat surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know \\(y^{(i)}\\) is the true label of the ith example (0 or 1), \\(h(x^{(i)}, \\theta)\\) is the predicted label for the ith example with input \\(x^{(i)}\\) and parameters \\(\\theta\\). With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the BCE function\ndef bce(y_true, y_pred):\n    return -1*(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n\n\n\n\nCode\ny_true = torch.zeros(50)\ny_pred = torch.linspace(0., 1., 50)\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 1: BCE loss when y = 0\n\n\n\n\n\n\nCode\ny_true = torch.ones(50)\ny_pred = torch.linspace(0., 1., 50)\ncriterion = torch.nn.BCELoss(reduction='none')\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 2: BCE loss when y = 1\n\n\n\n\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  },
  {
    "objectID": "posts/gan_p1/index.html",
    "href": "posts/gan_p1/index.html",
    "title": "A Primer on Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "A brief history of GANs\nFor a fuller account, check out the MIT Technology Review article.\nBack in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from AlexNet to VGG. (Not to mention ResNet in 2015, an architecture with so interesting an idea that I had to make a project for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, Generative Adversarial Nets.\n\n\n\nThe image was not from the era, but was representative of what you got from the model at that time (and still now with GANs). Source\n\n\nNow I wanted to make two quick detours before going into the inside of GANs:\n\nAt its core sense, a function is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is trying to update its parameters such that the model will approximate the optimal function as closely as possible. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.\nAdvances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.\n\n\n\nThe GANs game:\nNote: I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.\nThe word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called Generator, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called Discriminator, (or Critic, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.\n\n\n\n\nGenerator\nDiscriminator\n\n\n\n\nInput\nRandom numbers\nImages (real & generated)\n\n\nOutput\nImages\nClass of image (binary)\n\n\nRole\nForger\nAppraiser\n\n\n\nQuick detour: the GAN concept advances generative AI the same way backpropagation does so. The approach of trying to know the distribution of the image features was right, but the method was wrong a.k.a too complex and computationally expensive. With GAN, we have an elegant way to start with any random distribution while moving towards the optimal distribution incrementally. No need to know everything any more.\nOur loss function will be the good ol’ binary cross-entropy: \\[J(\\theta) = -\\frac{1}{m}*[y^{(i)}log(h(x^{(i)}, \\theta)) + (1 - y^{(i)})log(1 - (h(x^{(i)}, \\theta)))]\\]\nThat surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know \\(y^{(i)}\\) is the true label of the ith example (0 or 1), \\(h(x^{(i)}, \\theta)\\) is the predicted label for the ith example with input \\(x^{(i)}\\) and parameters \\(\\theta\\). With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the BCE function\ndef bce(y_true, y_pred):\n    return -1*(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n\n\n\n\nCode\ny_true = torch.zeros(50)\ny_pred = torch.linspace(0., 1., 50)\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 1: BCE loss when y = 0\n\n\n\n\n\n\nCode\ny_true = torch.ones(50)\ny_pred = torch.linspace(0., 1., 50)\ncriterion = torch.nn.BCELoss(reduction='none')\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 2: BCE loss when y = 1\n\n\n\n\nI mentioned that this is a conflict between Generator and Discriminator. For Discriminator, it wants to classify correctly i.e. catch the Generator every time while approve the value of the real images. In other words, it wants to minimize its loss function. For Generator, it wants the reverse i.e. pass a fake as a real to the Discriminator every single time. In other words, it wants to maximize the loss function (of the Discriminator). This leads to the ter minimax game that you may hear some people use to describe GAN.\nThe game can be considered complete when the Discriminator’s accuracy drops to 50% i.e. it can no longer discern, and essentially has to guess at random for each image. At this, our Generator will become potent enough to fool even us with its humans and cats.\n\n\nEnd of part 1:\nAs a primer this is far enough. I will continue on the subject, describing each model’s simplest architecture possible, the process of training one, as well as the difficulty in training GANs. (Training a model is hard enough, now we have two.)\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  },
  {
    "objectID": "posts/gan_p2/index.html",
    "href": "posts/gan_p2/index.html",
    "title": "Building a simple GANs",
    "section": "",
    "text": "Training GANs:\nLike any training process, the process GANs can be decomposed into feed-forward and backpropagation, though with distinct features from, say, training a classifier. The parameters updated after backpropagation can also be divided into two steps, for Discriminator and Generator. These are summed up in the images below:\n\n\n\nFirst, in the feed-forward, we pass some random noise (denoted by \\(\\xi\\)) into the Generator, which outputs some fake examples (denoted by \\(\\hat{X}\\)). The fake examples are then merged with a dataset of real examples (just \\(X\\)) and feed separately into the Discriminator, and we receive the outputs as a vector containing the possibilities of each example being real (between 0 and 1).\n\n\n\n\n\nSecond, for training the Discriminator. We will calculate the loss as binary cross-entropy (BCE) loss for two components: how closely to 0 the Discriminator predicted the fake examples, and how closely to 1 the Discriminator predicted the real examples. Here, we need to detach the Generator from the gradient flow as we want to update the Discriminator’s parameters only\n\n\n\n\n\nThird, for training the Generator. From the predictions for the fake examples, we calculate the BCE loss as how closely the Discriminator predicted them to 1. We then update the Generator’s parameters."
  }
]