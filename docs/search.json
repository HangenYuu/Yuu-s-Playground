[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Pham Nguyen Hung. I am a Deep Learning Practitioner. The stage I am at (student, master, etc.) is bound to change over the years, so you are encouraged to view the Now tab for my current position. I have a knack for numbers and complex concepts, and love learning them. Outside that, I have an interest in books of all types, and arts (music, drawing). The word that motivates me is “polymath”, and the figure is Nassim Nicholas Taleb."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "H’s notes",
    "section": "",
    "text": "My notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\n \n\n\n\n\nJan 24, 2023\n\n\nPham Nguyen Hung\n\n\n10 min\n\n\n\n\n\n\n\n\n\nMy notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\n \n\n\n\n\nJan 23, 2023\n\n\nPham Nguyen Hung\n\n\n13 min\n\n\n\n\n\n\n\n\n\nMy notes on taking the specialization by deeplearning.ai series\n\n\n\n\ncode\n\n\nGAN\n\n\n \n\n\n\n\nJan 21, 2023\n\n\nPham Nguyen Hung\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnew\n\n\ncode\n\n\n \n\n\n\n\nOct 22, 2022\n\n\nPham Nguyen Hung\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "My major is chemical engineering, but I want to be a deep learning practitioner.\n\n\n\nI studied it on Coursera. Two courses were released at different times, so there was a mismatched in module used (TensorFlow vs PyTorch), but that means that I can learn from both worlds. I often encountered champions for PyTorch, such as Jeremy Howard, who praised the module over TensorFlow, but I feel like I need to experience both to judge.\n\n\n\nJust go to a blog post and see one of my entry.\n\n\n\nYou can see them at my Github repo. For the philosophy of project-based and just-in-time learning, check out this post or this post.\n\n\n\nIt is not an easy market that I come into, with the news about tech layoffs in Singapore this 2023. Still, I do what I can do and have to do. My deadline is 29/01/2023, when the career fair at my school starts."
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Code\nprint(\"Hello World!\")\n\n\nHello World!"
  },
  {
    "objectID": "posts/anw/index.html",
    "href": "posts/anw/index.html",
    "title": "Show me your portfolio",
    "section": "",
    "text": "Nassim Nicholas Taleb told me this when he talked about listening to pundits in The Black Swan (or was it Antifragile?). Derek Sivers shared the same idea about talking versus doing. The idea reminds me to pick my words carefully, when I give advice, and when I state my priority. It also reminds to tailor my daily schedule so that the priority is always at the top. (It is a nice idea, by the way, to speak and act on priority as singular rather than plural). The next time you say something, act on it to see if you really believe your words. And the next time you do something, stating clearly how you like doing that."
  },
  {
    "objectID": "posts/dlm/index.html",
    "href": "posts/dlm/index.html",
    "title": "Daily translation of Meditations to Vietnamese",
    "section": "",
    "text": "“Don’t trust in your reputation, money, or position, but in the strength that is yours—namely, your judgements about the things that you control and don’t control. For this alone is what makes us free and unfettered, that picks us up by the neck from the depths and lifts us eye to eye with the rich and powerful.” — EPICTETUS, DISCOURSES, 3.26.34–35\n\nĐừng tin vào danh vọng, tiền bạc, hay vị trí [của bản thân], mà hãy tin vào sức mạnh mà con nắm giữ—khả năng phân biệt giữa thứ mà con có thể kiểm soát và thứ mà con không thể. Bởi đây là thứ duy nhất giúp chúng ta được tự do và không bị ràng buộc, là thứ duy nhất kéo chúng ta thoát khỏi vực thẳm và đưa chúng ta ngang hàng với những kẻ giàu có và quyền lực.\nIt is something that is expected from Epictetus - the emphasis on the dichotomy of control. What surprises me is the last phrase. The ability to exercise the dichotomy of control and act accordingly gives us power and wealth. It is all based on the same principle: to have more, you can acquire more, or you can want less."
  },
  {
    "objectID": "posts/dlm/index.html#nov---2022-1",
    "href": "posts/dlm/index.html#nov---2022-1",
    "title": "Daily translation of Meditations to Vietnamese",
    "section": "08 - Nov - 2022",
    "text": "08 - Nov - 2022\n\n“Remember that you are an actor in a play, playing a character according to the will of the playwright—if a short play, then it’s short; if long, long. If he wishes you to play the beggar, play even that role well, just as you would if it were a cripple, a honcho, or an everyday person. For this is your duty, to perform well the character assigned you. That selection belongs to another.” — EPICTETUS, ENCHIRIDION, 17\n\nHãy nhớ rằng con là một diễn viên trong một vở kịch [mang tên cuộc đời], diễn một vai theo ý muốn của người viết kịch—nếu đó là một vở kịch ngắn, thì diễn ngắn; nếu nó dài, thì diễn dài. Nếu người viết kịch muốn con đóng vai một kẻ ăn mày, vậy thì hãy diễn vai đó thật tốt, cũng tương tự như vậy nếu con nhận được vai kẻ bị thọt, người đứng đầu, hay chỉ là một người bình thường. Bởi việc diễn thật tốt vai đã được giao chính là nghĩa vụ của con. Việc chọn vai nào [cho con] thuộc về những người khác.\nNow the last sentence raised my eyebrows. What? Let’s others choose for me? Ins’t that antithetical to “Essentialism”? That I need to prioritize my life instead of letting others prioritize it for me! But be calm - Epictetus did not talk about letting others choose. The others here referred to the higher being, chance, luck i.e. anything that we cannot control. It is probable that the course of our life is led to stranger lands because of some accidents. However, we are still in control: we can choose to get back on track after being thrown off-trackor not. That alone is important, that alone is enough."
  },
  {
    "objectID": "posts/ftf/index.html#data-analyst",
    "href": "posts/ftf/index.html#data-analyst",
    "title": "First thing (1)",
    "section": "Data Analyst:",
    "text": "Data Analyst:\n\nHere is a diagram from DataCamp that sums up the job descriptions well:\n\nFrom the job descriptions, you will need to know…\nBasic:\n\n…how to use a business intelligence platforms - Looker, Power BI, Tableau and the like. One quick look at DataCamp Data Analyst in Power BI track shows that Power BI is capable of step 2-4 in the diagram. You can prepare and transform data; you can explore data from basic (mean, median, etc.) to complex operations that you define yourself; you can do all kinds of visualizations; you can publish the findings to a report, an app (possibly dashboard), or slides to aid your presentation. And compared to frameworks in languages such as R or Python, these platforms look less daunting because they have a GUI (you still often need to “program stuff”). These are the main tools for your trade.\n…how to communicate (written and oral). At the end of anything at work, you will need to “give report to stakeholders”. Being a data analyst\n\nPreferred: These are specific TikTok’s preferences:\n\n…how to do statistical analysis. This is arguably “basic”, not “preferred”. The numbers you calculate in the platform, they all have statistical meanings, so it helps to know what you calculating. Understanding statistics also helps you to avoid"
  },
  {
    "objectID": "posts/ftf/index.html#data-engineer",
    "href": "posts/ftf/index.html#data-engineer",
    "title": "First thing (1)",
    "section": "Data Engineer:",
    "text": "Data Engineer:\nAs a LinkedIn post suggested, a data engineer’s job is “being in charge of the data pipeline”, making sure that people such as data scientists can have the data to do their jobs. I will admit that I know next to nothing about data engineer, so the following came from a certain “God-tier” roadmap that I picked up. (I am weak again anything “God-tier”, so please refrain from using that against me). I will guess that the job requirements will vary. The data you need to work on may just be stored in a single AWS S3, but there are cases where the data are distributed in 23 data centers around the world, and you need Vitess on top of MySQL to handle the requests successfully (totally not Youtube). In any case, a quick rundown of the “God-tier” roadmap reviews that you need:\n\nSQL (and NoSQL)\nDatabase system knowledge (data lake, data warehouse, etc.)\nLinux a.k.a working with command line interface (CLI)\nBig data framework (Hadoop, Spark, Kafka, depends on your company)\nCloud service knowledge (Google Cloud Product (GCP), Amazon Web Services (AWS), or Microsoft Azure - depends on your company)\nContainerization technology: Docker (and Kubernetes)\n\nYou may find the rest in the video and the attached document. Now onto the confusing topic…"
  },
  {
    "objectID": "posts/ftf/index.html#data-scientistmachine-learning-engineer",
    "href": "posts/ftf/index.html#data-scientistmachine-learning-engineer",
    "title": "First thing (1)",
    "section": "Data Scientist/Machine Learning Engineer:",
    "text": "Data Scientist/Machine Learning Engineer:\nFirst, let’s start with a diagram from DataCamp.\n\n\n\nSource: DataCamp\n\n\nThe things that confuse me"
  },
  {
    "objectID": "posts/ganlec/index.html",
    "href": "posts/ganlec/index.html",
    "title": "A Primer on Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "A brief history of GANs\nFor a fuller account, check out the MIT Technology Review article.\nBack in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from AlexNet to VGG. (Not to mention ResNet in 2015, an architecture with so interesting an idea that I had to make a project for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, Generative Adversarial Nets.\n\n\n\nThe image was not from the era, but was representative of what you got from the model at that time (and still now with GANs). Source\n\n\nNow I wanted to make two quick detours before going into the inside of GANs:\n\nAt its core sense, a function is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is trying to update its parameters such that the model will approximate the optimal function as closely as possible. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.\nAdvances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.\n\n\n\nThe GALN’s game:\nNote: I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.\nThe word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called Generator, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called Discriminator, (or Critic, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.\n\n\n\n\nGenerator\nDiscriminator\n\n\n\n\nInput\nRandom numbers\nImages (real & generated)\n\n\nOutput\nImages\nClass of image (binary)\n\n\nRole\nForger\nAppraiser\n\n\n\nOur loss function will be the good ol’ binary cross-entropy: \\[J(\\theta) = -\\frac{1}{m}*[y^{(i)}log(h(x^{(i)}, \\theta)) + (1 - y^{(i)})log(1 - (h(x^{(i)}, \\theta)))]\\]\nThat surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know \\(y^{(i)}\\) is the true label of the ith example (0 or 1), \\(h(x^{(i)}, \\theta)\\) is the predicted label for the ith example with input \\(x^{(i)}\\) and parameters \\(\\theta\\). With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the BCE function\ndef bce(y_true, y_pred):\n    return -1*(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n\n\n\n\nCode\ny_true = torch.zeros(50)\ny_pred = torch.linspace(0., 1., 50)\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 1: BCE loss when y = 0\n\n\n\n\n\n\nCode\ny_true = torch.ones(50)\ny_pred = torch.linspace(0., 1., 50)\ncriterion = torch.nn.BCELoss(reduction='none')\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 2: BCE loss when y = 1\n\n\n\n\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  },
  {
    "objectID": "posts/gan_p1/index.html",
    "href": "posts/gan_p1/index.html",
    "title": "A Primer on Generative Adversarial Networks (GANs)",
    "section": "",
    "text": "A brief history of GANs\nFor a fuller account, check out the MIT Technology Review article.\nBack in 2014, computer vision had witnessed the power of deep learning. One must not look no further than the entries for the ImageNet challenge, with the introduction of very deep models from AlexNet to VGG. (Not to mention ResNet in 2015, an architecture with so interesting an idea that I had to make a project for it.) However, as discerning and mature as these models could get in classification, they were nascent in generating data: results were blurry images or with weird artifact. “Complex statistical analysis of the elements that make up a photograph” was proposed but would not work. We needed a simpler, more elegant way of solving the problem. And the solution arrived in Goodfellow’s seminal paper, Generative Adversarial Nets.\n\n\n\nThe image was not from the era, but was representative of what you got from the model at that time (and still now with GANs, if your model was trained poorly or prematurely). Source\n\n\nNow I wanted to make two quick detours before going into the inside of GANs:\n\nAt its core sense, a function is a set of rule(s) to describe how inputs should be matched with outputs. For each problem that we pose to a neural network, there exists an optimal function to do so. The easiest example is image classification, with the optimal function being a human being (with many constraints behind, but yes, humans still surpass machines in this). If we think this way, then training a model is trying to update its parameters such that the model will approximate the optimal function as closely as possible. Neural networks thus are powerful because they can approximate very complex functions, such as the neural configuration in our head that map these pixels of a tree to the word “tree”.\nAdvances in neural network came about with backpropagation. Neural network could be powerful, yes, but at that time (before the 90s) there existed no easy and computationally cheap way to update the parameters. Then there was backpropagation. The parameters could now be initialized randomly at the beginning, and then be updated incrementally with the gradient of the loss value with it. In other words, now we can start from (almost) anywhere because we have this amazing magical map that can tell the next direction that we need to take.\n\n\n\nThe GANs game:\nNote: I will use digital images as examples to describe GANs. A quick introduction: images are stored as matrices of numbers inside computer’s memory. It we are talking about a black and white image, its content could be divided into discrete squares called pixels; each pixel stores a number describing the intensity of light at the pixel, with 0 for black and 255 for white, and numbers in-between for the shades of gray. If we are referring to colored images, we will have three matrices for intensity of red, green, and blue. This means that to generate images, we just need to generate one matrix or three matrices of certain dimensions, which could be displayed to the screen later.\nThe word “adversarial” in GAN means “involving or characterized by conflict or opposition” according to Oxford Dictionary. Simply put, a GANs’ system consists of, instead of one, two neural networks pitted against each other. The first one is called Generator, its inputs will be some random numbers, and its output will be the matrix or matrices described above. The second one is called Discriminator, (or Critic, which we will meet later), its inputs will be some real images and the generated images, its output will classification of the image it sees (ideally 0 for real images and 1 for generated images). An analogy is we have the real images as a pile of real antique paintings, the Generator as a forger, the generated images as a pile of forged paintings, and the Discriminator as an appraiser trying to discern the two piles.\n\n\n\n\nGenerator\nDiscriminator\n\n\n\n\nInput\nRandom numbers\nImages (real & generated)\n\n\nOutput\nImages\nClass of image (binary)\n\n\nRole\nForger\nAppraiser\n\n\n\nQuick detour: the GAN concept advances generative AI the same way backpropagation does so. The approach of trying to know the distribution of the image features was right, but the method was wrong a.k.a too complex and computationally expensive. With GAN, we have an elegant way to start with any random distribution while moving towards the optimal distribution incrementally. No need to know everything any more.\nOur loss function will be the good ol’ binary cross-entropy: \\[J(\\theta) = -\\frac{1}{m}*[y^{(i)}log(h(x^{(i)}, \\theta)) + (1 - y^{(i)})log(1 - (h(x^{(i)}, \\theta)))]\\]\nThat surely looks very intimidating, but one must not let symbols and numbers get in the way of intuition. One just need to know \\(y^{(i)}\\) is the true label of the ith example (0 or 1), \\(h(x^{(i)}, \\theta)\\) is the predicted label for the ith example with input \\(x^{(i)}\\) and parameters \\(\\theta\\). With this information, it is easy to realize that the loss will be 0 if the predicted label is the true label and infinitely large otherwise.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport torch\n\n# Define the BCE function\ndef bce(y_true, y_pred):\n    return -1*(y_true*torch.log(y_pred) + (1-y_true)*torch.log(1-y_pred))\n\n\n\n\nCode\ny_true = torch.zeros(50)\ny_pred = torch.linspace(0., 1., 50)\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 1: BCE loss when y = 0\n\n\n\n\n\n\nCode\ny_true = torch.ones(50)\ny_pred = torch.linspace(0., 1., 50)\ncriterion = torch.nn.BCELoss(reduction='none')\n\nplt.figure()\nplt.plot(y_pred, bce(y_true, y_pred), \"o--\")\nplt.xlabel(\"prediction\", fontsize=23)\nplt.ylabel(\"loss\", fontsize=23)\nplt.grid()\nplt.show()\n\n\n\n\n\nFigure 2: BCE loss when y = 1\n\n\n\n\nI mentioned that this is a conflict between Generator and Discriminator. For Discriminator, it wants to classify correctly i.e. catch the Generator every time while approve the value of the real images. In other words, it wants to minimize its loss function. For Generator, it wants the reverse i.e. pass a fake as a real to the Discriminator every single time. In other words, it wants to maximize the loss function (of the Discriminator). This leads to the ter minimax game that you may hear some people use to describe GAN.\nThe game can be considered complete when the Discriminator’s accuracy drops to 50% i.e. it can no longer discern, and essentially has to guess at random for each image. At this, our Generator will become potent enough to fool even us with its humans and cats.\n\n\nEnd of part 1:\nAs a primer this is far enough. I will continue on the subject, describing each model’s simplest architecture possible, the process of training one, as well as the difficulty in training GANs. (Training a model is hard enough, now we have two.)\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  },
  {
    "objectID": "posts/gan_p2/index.html",
    "href": "posts/gan_p2/index.html",
    "title": "Building a simple GAN",
    "section": "",
    "text": "In the last post, I briefed you the history and concept of Generative Adversarial Networks (GANs). In this post, we will get more technical, going into the training process as well as trying our hand creating a simple GAN model on my favourite dataset, KMNIST."
  },
  {
    "objectID": "posts/gan_p2/index.html#the-dataset",
    "href": "posts/gan_p2/index.html#the-dataset",
    "title": "Building a simple GAN",
    "section": "The dataset:",
    "text": "The dataset:\nFirst rule: always, always look at the data first. Now, KMNIST is a dataset inspired by the MNIST dataset of 10 hand-written digits. Here, we have 10 hand-written Kanji characters. Look at the provided examples, the handwriting surely looks messy, some almost unrecognizable from the modern version.\n\n\n\nThe 10 classes of Kuzushiji-MNIST, with the first column showing each character’s modern hiragana counterpart. Source\n\n\nSimilar to MNIST, a KMNIST image has only one channel. Let’s visualize one.\n\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n\n\n# Function learnt from GAN's Specialization Course 1 Week 1\ndef tensor_show(image_tensor, num_images=25, size=(1, 28, 28)):\n    # The original image tensor could be stored on GPU and \n    # have been flattened out for training, so we restore it\n    # first.\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    # torch uses (color channel, height, width) while \n    # matplotlib used (height, width, color channel)\n    # so we fix it here\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n\n\n# Download needs to be set to True the first time you run it.\nbatch_size = 32\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=False, transform=transforms.ToTensor()),\n    batch_size=batch_size,\n    shuffle=True)\n\n\nimage_tensor = next(iter(dataloader))[0]\ntensor_show(image_tensor)"
  },
  {
    "objectID": "posts/gan_p2/index.html#the-discriminator",
    "href": "posts/gan_p2/index.html#the-discriminator",
    "title": "Building a simple GAN",
    "section": "The Discriminator:",
    "text": "The Discriminator:\n\nThe architecture for each block of Discriminator and Generator follows the suggestions from the Deep Convolutional GAN paper.\n\nThe Discriminator is essentially a classifier, so we can define as with a normal classifier. It means that we can start with the good ol’ linear model, but I will skip a bit to the year 2015, when DCGAN was introduced and construct my GANs with a 3-layered convolutional architecture. To conveniently construct each layer, I will also define a general function to create a layer of arbitrary sizes. A non-last layer will have a convolution followed by batch normalization and LeakyReLU (batch normalization is there to stabilize GAN’s training. We will touch upon tricks to stabilize GAN’s training in the next post).\n\nclass Discriminator(nn.Module):\n    def __init__(self, image_channel=1, hidden_dim=56):\n        super().__init__()\n        self.disc = nn.Sequential(\n            self.make_disc_block(image_channel, hidden_dim),\n            self.make_disc_block(hidden_dim, hidden_dim),\n            self.make_disc_block(hidden_dim, 1, final_layer=True),\n        )\n\n    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                    nn.BatchNorm2d(output_channels),\n                    nn.LeakyReLU(negative_slope=0.25)\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                    nn.Conv2d(input_channels, output_channels, kernel_size, stride)\n            )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.disc(x)\n        # The input can be a tensor of multiple images\n        # We want to return a tensor with the possibility\n        # of real/fake for each image.\n        return x.view(len(x), -1)"
  },
  {
    "objectID": "posts/gan_p2/index.html#the-generator",
    "href": "posts/gan_p2/index.html#the-generator",
    "title": "Building a simple GAN",
    "section": "The Generator:",
    "text": "The Generator:\nA point to note: convolution (or convolution/pooling) will reduce the dimensions of your data, essentially distilling the information to the output (the possibility of a class in a classifier). Meanwhile, Generator will make use of the transposed convolution operation, which increases the dimensions of data, essentially magnifying the noises into an image. (I will create a blog post about convolution in the future, in the meantime, check out this notebook as my draft.)\nFirst, we need a function to generate noise. Basically, we need some tensor containing random numbers, and we can conveniently return a tensor filled with random numbers from a normal distribution with torch.randn(). For the dimensions, we define argument z_dim as the dimension of the noise input, and n_samples as the number of samples we need.\n\ndef generate_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn((n_samples, z_dim), device=device)\n\nFor the Generator class, I will also create a function to construct each layer. A non-last layer will have a transposed convolution, followed by batch normalization and ReLU activation. The final layer does not have batch normalization but will have Tanh activation to squish the pixels in range.\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim=14, image_channel=1, hidden_dim=56):\n        # z_dim is the dimension of the input noise vector\n        self.z_dim = z_dim\n        super().__init__()\n        self.gen = nn.Sequential(\n            self.make_gen_block(z_dim, hidden_dim * 4),\n            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n            self.make_gen_block(hidden_dim * 2, hidden_dim),\n            self.make_gen_block(hidden_dim, image_channel, kernel_size=4, final_layer=True),\n        )\n    \n    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.ReLU()\n            )\n        else: # Final Layer\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh()\n            )\n    # Recall torch expect an image to be in the form (color channel, height, width).\n    # In a batch, torch expects it to be (no. of images in batch, color channel, height, width)\n    # So we need to transform the noise, originally in (no. of images in batch, input dimension)\n    # to (no. of images in batch, input dimension, 1, 1)\n    # See more here:\n    # https://pytorch.org/vision/stable/transforms.html#transforming-and-augmenting-images\n    def unsqueeze_noise(self, noise):\n        return noise.view(len(noise), self.z_dim, 1, 1)\n\n    def forward(self, noise):\n        x = self.unsqueeze_noise(noise)\n        return self.gen(x)"
  },
  {
    "objectID": "posts/gan_p2/index.html#optimizers-and-criterion",
    "href": "posts/gan_p2/index.html#optimizers-and-criterion",
    "title": "Building a simple GAN",
    "section": "Optimizers and Criterion",
    "text": "Optimizers and Criterion\nNext, we want to define our optimizers (one for each model) and our criterion.\n\n# We do not have activation at the output for Discriminator, so the outputs\n# are raw (logits).\ncriterion = nn.BCEWithLogitsLoss()\nz_dim = 64\ndisplay_step = 500\nbatch_size = 1000\n# Learning rate of 0.0002 and beta_1 (momentum term for Adam optimizer) of \n# 0.5 works well for DCGAN, according to the paper (yes, I seriously searched\n# for keyword \"learning rate\" in the paper)\nlr = 0.0002\nbeta_1 = 0.5 \nbeta_2 = 0.999\n# Device-agnostic code\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# You can tranform the image values to be between -1 and 1 (the range of the Tanh activation)\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n])\n\ndataloader = DataLoader(\n    datasets.KMNIST('data', download=False, transform=transform),\n    batch_size=batch_size,\n    shuffle=True)\n\n\ngen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\ndisc = Discriminator().to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n\n# You initialize the weights to the normal distribution\n# with mean 0 and standard deviation 0.02\n# (Yes, the paper said so.)\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\n# Apply recursively weights_init() according to the docs:\n# https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init)"
  },
  {
    "objectID": "posts/gan_p2/index.html#training",
    "href": "posts/gan_p2/index.html#training",
    "title": "Building a simple GAN",
    "section": "Training",
    "text": "Training\nOkay, now onto training!\nn_epochs = 100\ncur_step = 1 # For visualization purpose\nmean_generator_loss = 0\nmean_discriminator_loss = 0\nfor epoch in range(n_epochs):\n    for real, _ in tqdm(dataloader):\n        cur_batch_size = len(real)\n        real = real.to(device)\n\n        ## Update Discriminator\n\n        # Empty the optimizer\n        disc_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise = generate_noise(cur_batch_size, z_dim, device=device)\n        fake = gen(fake_noise)\n        disc_fake_pred = disc(fake.detach())\n\n        # Calculate loss\n        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n\n        # Same for real examples\n        disc_real_pred = disc(real)\n        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n\n        # The Discriminator's loss is the average of the two\n        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n\n        # Keep track of the average Discriminator loss\n        mean_discriminator_loss += disc_loss.item() / display_step\n\n        # Update Discriminator's gradients a.k.a backpropagation\n        # Normally don't set retain_graph=True, but we do so for GAN\n        # as we need to propagate through the graph a second time\n        # when updating the Generator.\n        disc_loss.backward(retain_graph=True)\n\n        # Update Discriminator's optimizer\n        disc_opt.step()\n\n        ## Update Generator\n\n        # Empty the optimizer\n        gen_opt.zero_grad()\n\n        # Generate noise and pass through Discriminator for fake examples\n        fake_noise_2 = generate_noise(cur_batch_size, z_dim, device=device)\n        fake_2 = gen(fake_noise_2)\n        disc_fake_pred = disc(fake_2)\n\n        # Calculate loss\n        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n\n        # Backpropagation for Generator's loss\n        gen_loss.backward()\n\n        # Update Generator's optimizer\n        gen_opt.step()\n\n        # Keep track of the average Generator loss\n        mean_generator_loss += gen_loss.item() / display_step\n\n        ## Visualization code\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n            tensor_show(fake)\n            tensor_show(real)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1"
  },
  {
    "objectID": "posts/gan-p3/index.html",
    "href": "posts/gan-p3/index.html",
    "title": "That Unstable GAN",
    "section": "",
    "text": "In the last two posts, I have gone over the concept and process of training GANs. I made it seem so straightforward, but reality is harsher. Recall that for GANs, we are training two neural networks at once, and these two compete with each other to mutually improve. Much like real life, if one competitor is too good, the other will not learn anything, if the at point before the end the Generator or the Discriminator becomes too good (or the other becomes too bad), then training breaks down. Thus, training GANs is highly unstable, and any way to ensure smoother training is encouraged."
  },
  {
    "objectID": "posts/gan-p3/index.html#activation-function",
    "href": "posts/gan-p3/index.html#activation-function",
    "title": "That Unstable GAN",
    "section": "Activation function",
    "text": "Activation function\nActivation function is a requirement for neural networks’ ability to approximate complex function. Without it, a neural network will become just another linear function.\n\n\nCode\nimport torch\nimport torch.nn.functional as F\n\nimport numpy as np\nimport seaborn as sb\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(17)\ntorch.manual_seed(17)\ndef linear(a, b, x):\n    return a*x + b\n\n\n\n\nCode\nx = torch.randn(50)\n\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, linear(.5, 4, x) + linear(3.56, -5.32, x) + linear(-1.86, 3.74, x), 'o--')\nax2.plot(x, torch.relu(0.5*x) + torch.relu(3.56*x) + torch.relu(-1.86*x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n\n\n\n\n\nFigure 1: Stacking linear functions on top of each other is just a linear function. Meanwhile, stacking ReLU functions on top of each other create a piecemeal linear function that approximates a curve.\n\n\n\n\nWe all starts with the sigmoid function in a binary cross-entropy problem. However, sigmoid, together with tanh, leads to the “vanishing gradient” problem. When the output value of gets close to 0 or 1 for sigmoid (or -1 or 1 for tanh), the gradient gets close to 0, so the weights either are updated very slowly or stop learning altogether. That was when ReLU came into play: the function has a clear, positive gradient when output value is greater than 0, while the bend makes sure that ReLU stacking on each other can produce a curve.\n\n\n\nEach neural network had three hidden layers with three units in each one. The only difference was the activation function. Learning rate: 0.03, regularization: L2. Source\n\n\nHowever, the joy ReLU brought came to halt when “dying ReLU” problem was reported. Suppose we have an output smaller or equal 0, then our derivative will be 0. The 0 derivative on the node means that it will not get updated, and that’s the end for it. Worse, the previous components connected to the node are affected as well, so the whole structure of our neural network will be “dead”. To fix, we have the variation: LeakyReLU. For LeakyReLU, the output value below 0 is not set at 0, but is multiplied by a constant (such as 0.2). Gradient for such value will still be non-zero, provide information to update the weights.\nAnother, more advanced variation is GeLU, where the output is multiplied with i.e. weighted by its percentile. Sounds too complicated? Look at the formula: \\[GELU(x)=x*P(X<x)=x*\\Phi(x)\\] for \\(X\\) ~ \\(\\mathcal{N}(0, 1)\\)\nGELU has been successfully applied in Transformer models such as BERT, GPT-3, and especially in CNN such as ConvNeXts. (Yeah, look at ConvNeXts - it started with a ResNet, the great ConvNet architecture, then the authors slowly introduced all the modern training tricks, until the result surpassed the Swin Transformer in the cheer of CNN-backer/Transformer-haters. Okay, that was eaxaggerating, but still…)\n\n\nCode\nfig = plt.figure(figsize=(9,3))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(x, F.leaky_relu(x, negative_slope=0.1), 'o--')\nax2.plot(x, F.gelu(x), 'o--')\n\nax1.grid()\nax2.grid()\nplt.show()\n\n\n\n\n\nFigure 2: LeakyReLU and GELU\n\n\n\n\nNow let’s move on to the second general trick that we have already done: batch normalization."
  },
  {
    "objectID": "posts/gan-p3/index.html#batch-normalization",
    "href": "posts/gan-p3/index.html#batch-normalization",
    "title": "That Unstable GAN",
    "section": "Batch normalization",
    "text": "Batch normalization\nWe all know that neural netowrk is trying to appromixate a certain way of mapping inputs i.e. data to outputs. The parameters of a neural network therefore depend on the data we receive, characteristically the distribution of the data. Here I have this example of an HDR image, which captures a farther range of color and exposure than a compressed format such as JPG or PNG. I found the original image from the Internet here\n\n\n\nThe curve at the bottom that may remind you of a bell curve is the curve for the distribution of pixel values a.k.a colors\n\n\nNow, we train a neural network on data having similar color distribution such as this image, possibly for the task of recognizing grass. The model was trained well. Alas, the testing image contains one such as this\n\n\n\nThis was the exact same image, but compressed at a differen color distribution (shifted to the right)\n\n\nHere we say that the data distribution has shifted between training data and testing data. This generally will cause model problems (decrease accuracy, etc.). Data distribution shift (or covariate shift) can also happen between batches of training data, leading to slow convergence (imagine the model has to take a zig-zag path instead of a straight one). This can be dealt with by normalization, where make sure that the distributions of the training set and the testing set are similar e.g. centered around a mean of 0 and a standard deviation of 1. This could be done by taking the mean and standard deviation for each training batch of image and normalize the inputs of each training batch, then take the accumulated statistics to normalize the testing set during testing. This will smooth out the cost function and increases model performance (you might not need to do this if your training set and testing set are already similar to each other).\nHowever, model is susceptible to internal covariate shift as well, where the activation output distributions shift between each layer. This can happen due to the change in the weights of each layer. Batch normalization came into play here by normalizing the inputs to each layer (“batch” means that we do so for each batch of image). For example, supposed are at nueron \\(i\\) of non-last layer \\(l\\), with activated output from the last layer to this neuron being \\(a_{i}^{[l-1]}\\). The logit out of this neuron will be \\[z_{i}^{[l]}=\\Sigma W_{i}^{[l]}a_{i}^{[l-1]}\\]\nWithout batch normalization, the logit will be passed into activation to output \\(a_{i}^{[l]}\\). But here, we will perform batch normalization:\n\nWe get the statistics mean \\(\\mu _{z_{i}^{[l]}}\\) and variance \\(\\sigma _{z_{i}^{[l]}} ^{2}\\) for the batch.\nWe use them in the formula \\[\\hat{z}_{i}^{[l]}=\\frac{z_{i}^{[l]}-\\mu _{z_{i}^{[l]}}}{\\sqrt{\\sigma _{z_{i}^{[l]}} ^{2} + \\epsilon}}\\] Nothing too fancy - it’s just the normalization formula that you encounter in any statistics course/textbook: substract the value by the mean, then divide it by the square root of variance a.k.a the standard deviation. The \\(\\epsilon\\) term is a positive constant there to make sure that the denominator is always positive.\nWe map the normalized value \\(\\hat{z}_{i}^{[l]}\\) to a new distribution with the formula \\[y_{i}^{[l]}=\\gamma*\\hat{z}_{i}^{[l]} + \\beta\\] where \\(\\gamma\\) is scale factor and \\(\\beta\\) the shift factor. These two are learnable inputs in the batch normalization layer, and will be tuned to figure out the best distribution for the task at hand.\nWe pass \\(y_{i}^{[l]}\\) through the activation function to the output \\(a_{i}^{[l]}\\).\n\nThe batch normalization layer seems complicated, but we usually does not need to all the things. As backpropagation is reduced to just calling loss.backward in PyTorch, the nn.BatchNorm2d() (for images) will take care of this during training.\nThere is another normalization method called layer normalization. I will not go into details here, though I very much want to because it was used in the training of ConvNeXts as well (seriously, I want to make a blog post just about the tricks used in pushing this CNN to surpass Swin). Here is a post about the two normalizations that also have great images. In PyTorch, this is implemented in nn.LayerNorm()."
  },
  {
    "objectID": "posts/gan-p3/index.html#wgan",
    "href": "posts/gan-p3/index.html#wgan",
    "title": "That Unstable GAN",
    "section": "WGAN:",
    "text": "WGAN:\nFirst, we need to talk about mode collapse. Now, a mode in statistical term is the value that we are most likely to get from a distribution (not too correct for continuous distribution, but still great for understanding). This will be represented by a peak in the data distribution, such as the mean in a normal distribution. A distribution can have just one mode, like the normal distribution, or multiple modes like below.\n\n\nCode\nsample1 = np.random.normal(loc=20, scale=5, size=300)\nsample2 = np.random.normal(loc=40, scale=5, size=700)\n# Concatenating the two sample along the second axis\nsample = np.hstack((sample1, sample2))\n\nsb.kdeplot(sample)\nplt.show()\n\n\n\n\n\nFigure 3: A bimodal distribution created by merging two normal distributions\n\n\n\n\nThe outputs have their modes alright. For example, in our KMNIST dataset, there are 10 modes for the output, corresponding to 10 characters. So we have a new way to think about training: we are trying to make the model learn to shift the distribution of the outputs to approximate the one we want. For illustration, suppose initially our model is outputing each value for each pixel randomly, leading to an output distribution like this.\n\n\n\nInitial output distribution\n\n\nWe want to change the output to this kind of distribution\n\n\n\nWe want to shift from the circle to just 10 peaks i.e. just outputting 1 from 10 classes at a time\n\n\nIn the ideal scenario, our model will be guided by the loss function to make the right shift. However, notice a lack in the BCE loss: it only promotes the model generating images close to real images, regardless of the class of the image. This means that there exists a quick n’ dirty way for the Generator to reduce the loss by only generating images from 1 class that the Discriminator is most fooled by. So there exists a case where we end up with a Generator that outputs very realistic kanji character, only that it generates kanji character, say, only “tsu”. That’s boring.\nMy last paragraph gives hint to the source of the problem: our loss function. BCE loss works to push the Generator forward, but it cannot capture the information of class within the image. We need something else. And that something else is Wasserstein GAN, introducing a new kind of loss function: the Wasserstein Loss (no surprise) a.k.a the Earth Mover’s distance, or EM distance. It measures the difference between two distributions, and can be informally defined as the least amount of energy required to move and mold a earth pile in the shape of one distribution to the shape of another distribution (hence earth mover).\nIn mathematical form, if we have the noise vector \\(z\\), the fake image data \\(x\\), the Generator model \\(g()\\), the Discriminator who becomes the Critic \\(c()\\), then the Wasserstein Loss is the difference between the expected value i.e. the mean of the Critic outputs for real images and the mean of the Critic outputs for generated images. \\[WLoss=E(c(x))-E(c(g(z)))\\]\nWe still have a minimax game here, with the Generator’s goal being minimize the above difference and the Critic’s goal being maximize the above difference. Notice also that the Critic now is no longer a classifier: it can give any real value possible e.g. higher score for real(istic) examples. This means that the Generator will get useful feedback for all classes of examples, and is less prone to mode collapse. Getting rid of the classifier i.e. the sigmoid function in the output also means that vanishing gradient is also less likely. Two birds, one stone.\nW-Loss has one condition: the function of the Critic should be 1-Lipschitz Continuity. That looks intimidating but it just means that the norm of the gradient (the value in 2D math, the \\(\\sqrt{x^{2}+y^{2}+...}\\) as example in higher dimensions) for the Critic can be at most 1 at any point. In other words, the output of the Critic cannot increase/decrease more than linearly at any point. To achieve this, the first proposed (and terrible way, according to the original author) was weight clipping - forcing the weights to a fixed interval, say, [0,1]. Any negative value will be set to 0, and any value more than 1 will be set to 1. This was terrible (I have to say it again) because it limits the potential of the Critic. Another less strict way is gradient penalty (the first dead ar5iv link), where you add a regularization term in the loss function to promote the Critic to be 1-Lipschitz Continuity, as oppposed to forcing it. Formula is \\[WLoss=E(c(x))-E(c(g(z))) + \\lambda * pen\\] with \\(pen=E((|| \\nabla c(\\hat{x})||_{2}-1)^{2})\\), \\(\\hat{x}=\\epsilon x + (1-\\epsilon)g(z)\\)\nFor completeness, the Critic gradient needs checking at every point in the feature space, which is impractical. What we do is sampling some points from real examples, and then some points from generated examples with weights for each, and then we calculate the penalty for the interpolated examples. An example for the code of WGAN can be found here (nothing for now). For a more technical review of WGAN, check out this paper, also available as a blog post.\nNext in line: Conditional GANs.\nAll the quoted paper from ArXiv was embedded with its corresponding Ar5iv link for the HTML version instead to support the tool. To change to the abstract page, follow this example: https://ar5iv.labs.arxiv.org/html/1409.1556 → https://arxiv.org/abs/1409.1556."
  }
]